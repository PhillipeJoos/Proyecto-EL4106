{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69090f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from pytorch_tcn import TCN\n",
    "\n",
    "# Metrics and visualization\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import seaborn as sns\n",
    "\n",
    "# Models and feature extractor\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "# Others\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D  # Necesario para crear la leyenda a medida\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "default_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\n",
    "architectures = [\"GRU\", \"LSTM\", \"RNN\", \"CNN\", \"TCNN\", \"Transformer\"]\n",
    "color_map = {\n",
    "    arch: default_colors[i % len(default_colors)]\n",
    "    for i, arch in enumerate(architectures)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20f275",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c354a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpeechCommands(Dataset):\n",
    "    def __init__(self, root, files_list, download=True, target_len=16000, mode=\"mfcc\", cnn_model=None):\n",
    "        \"\"\"\n",
    "        mode: 'mfcc', 'mfcc_delta', 'mfcc_delta_delta', 'cnn', 'wav2vec2'\n",
    "        cnn_model: modelo CNN preentrenado o personalizado para extracción\n",
    "        \"\"\"\n",
    "        self.target_len = target_len\n",
    "        self.mode = mode\n",
    "        self.cnn_model = cnn_model\n",
    "        self.dataset = torchaudio.datasets.SPEECHCOMMANDS(root=root, download=download)\n",
    "        self.indices = None\n",
    "        self.splitter(files_list, root)\n",
    "\n",
    "    def splitter(self, files_list, root):\n",
    "        with open(files_list, 'r') as f:\n",
    "            self.file_paths = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        self.all_paths = []\n",
    "        for item in tqdm(self.dataset._walker, desc=f\"Splitting {files_list}\"):\n",
    "            relative_path = os.path.relpath(\n",
    "                item,\n",
    "                start=os.path.join(root, \"SpeechCommands\", \"speech_commands_v0.02\")\n",
    "            ).replace(\"\\\\\", \"/\")\n",
    "            self.all_paths.append(relative_path)\n",
    "\n",
    "        self.indices = [i for i, path in enumerate(self.all_paths) if path in self.file_paths]\n",
    "        print(f\"Archivos encontrados: {len(self.indices)} / {len(self.file_paths)}\")\n",
    "\n",
    "    def pad_waveform(self, waveform):\n",
    "        length = waveform.shape[-1]\n",
    "        if length < self.target_len:\n",
    "            waveform = F.pad(waveform, (0, self.target_len - length))\n",
    "        elif length > self.target_len:\n",
    "            waveform = waveform[:, :self.target_len]\n",
    "        return waveform\n",
    "\n",
    "    def extract_feature_single(self, waveform, sample_rate, feature_extractor=None, processor=None, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Extrae features de UNA muestra según el modo configurado.\n",
    "        \"\"\"\n",
    "        waveform = self.pad_waveform(waveform).to(device)\n",
    "\n",
    "        if feature_extractor is not None:\n",
    "            feature_extractor = feature_extractor.to(device)\n",
    "\n",
    "        # --- MFCC ---\n",
    "        if self.mode == \"mfcc\":\n",
    "            feat = feature_extractor(waveform).squeeze(0).cpu().transpose(0, 1)\n",
    "\n",
    "        # --- MFCC + Delta ---\n",
    "        elif self.mode == \"mfcc_delta\":\n",
    "            base = feature_extractor(waveform)\n",
    "            delta = torchaudio.functional.compute_deltas(base)\n",
    "            feat = torch.cat([base, delta], dim=1).squeeze(0).cpu().transpose(0, 1)\n",
    "\n",
    "        # --- MFCC + Delta + Delta-Delta ---\n",
    "        elif self.mode == \"mfcc_delta_delta\":\n",
    "            base = feature_extractor(waveform)\n",
    "            delta = torchaudio.functional.compute_deltas(base)\n",
    "            delta2 = torchaudio.functional.compute_deltas(delta)\n",
    "            feat = torch.cat([base, delta, delta2], dim=1).squeeze(0).cpu().transpose(0, 1)\n",
    "\n",
    "        # --- CNN ---\n",
    "        elif self.mode == \"cnn\":\n",
    "            spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate).to(device)\n",
    "            spec = spec_transform(waveform).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                embedding = self.cnn_model(spec.to(device)).cpu().squeeze()\n",
    "            feat = embedding\n",
    "\n",
    "        # --- Wav2Vec2 ---\n",
    "        elif self.mode == \"wav2vec2\":\n",
    "            waveform = waveform.squeeze(0)\n",
    "            inputs = processor(\n",
    "                waveform,\n",
    "                sampling_rate=sample_rate,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = feature_extractor(**inputs)\n",
    "            feat = outputs.last_hidden_state.squeeze(0).cpu()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Modo de extracción '{self.mode}' no soportado.\")\n",
    "\n",
    "        return feat\n",
    "\n",
    "    def extract_features(self, feature_extractor=None, processor=None, device=\"cuda\"):\n",
    "        features, labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx in tqdm(self.indices, desc=f\"Extrayendo features ({self.mode})\"):\n",
    "                waveform, sample_rate, label, _, _ = self.dataset[idx]\n",
    "                feat = self.extract_feature_single(\n",
    "                    waveform, sample_rate, feature_extractor, processor, device\n",
    "                )\n",
    "                features.append(feat)\n",
    "                labels.append(label)\n",
    "\n",
    "        features = torch.stack(features)\n",
    "        print(f\"Features tensor: {features.shape}\")\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "    def save_features(self, feature_extractor=None, save_path=None, processor=None, device=\"cuda\"):\n",
    "        print(f\"Guardando features ({self.mode}) en {save_path}\")\n",
    "        try:\n",
    "            features, labels = self.extract_features(feature_extractor, processor, device)\n",
    "            torch.save({\"features\": features, \"labels\": labels}, save_path)\n",
    "            print(f\"Features guardadas correctamente en {save_path}\")\n",
    "            print(f\"Clases finales: {set(labels)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al guardar features en {save_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = self.indices[idx]\n",
    "        waveform, sample_rate, label, speaker_id, utterance_number = self.dataset[original_idx]\n",
    "        waveform = self.pad_waveform(waveform)\n",
    "        return waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "class FeaturesDataset(Dataset):\n",
    "    def __init__(self, features_path):\n",
    "        \"\"\"\n",
    "        Carga un archivo .pt con 'features' y 'labels' previamente guardados.\n",
    "\n",
    "        features_path: ruta al archivo .pt (por ejemplo 'data/train.pt')\n",
    "        \"\"\"\n",
    "        data = torch.load(features_path)\n",
    "        self.features = data[\"features\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(sorted(set(self.labels)))}\n",
    "        self.idx_to_label = {v: k for k, v in self.label_to_idx.items()}\n",
    "        self.numeric_labels = torch.tensor([self.label_to_idx[l] for l in self.labels])\n",
    "\n",
    "        print(f\"Dataset cargado desde {features_path}\")\n",
    "        print(f\" - {len(self.features)} ejemplos\")\n",
    "        print(f\" - {len(self.label_to_idx)} clases\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.numeric_labels[idx]\n",
    "        return feature, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e834f86",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32de13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type,\n",
    "        n_input_channels,\n",
    "        hidd_size=256,\n",
    "        out_features = 35,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Para utilizar una vanilla RNN entregue rnn_type=\"RNN\"\n",
    "        Para utilizar una LSTM entregue rnn_type=\"LSTM\"\n",
    "        Para utilizar una GRU entregue rnn_type=\"GRU\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "        if rnn_type == \"GRU\":\n",
    "            self.rnn_layer = nn.GRU(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers)\n",
    "\n",
    "        elif rnn_type == \"LSTM\":\n",
    "            self.rnn_layer = nn.LSTM(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers)\n",
    "\n",
    "        elif rnn_type == \"RNN\":\n",
    "            self.rnn_layer = nn.RNN(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers, bidirectional=True)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"rnn_type {rnn_type} not supported.\")\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidd_size, out_features),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.rnn_type == \"GRU\":\n",
    "            out, h = self.rnn_layer(x)\n",
    "\n",
    "        elif self.rnn_type == \"LSTM\":\n",
    "            out, (h, c) = self.rnn_layer(x)\n",
    "\n",
    "        elif self.rnn_type == \"RNN\":\n",
    "            out, h = self.rnn_layer(x)\n",
    "\n",
    "        out = h[-1]\n",
    "\n",
    "        return self.net(out)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, n_input_channels, hidd_size=64, out_features=35):\n",
    "        \"\"\"\n",
    "        Modelo CNN (Convolutional Neural Network)\n",
    "\n",
    "        Args:\n",
    "            n_input_channels (int): Canales de entrada (e.g., 13 para MFCC)\n",
    "            hidd_size (int): Número base de canales en las capas convolucionales\n",
    "            out_features (int): Número de clases de salida (e.g., 35)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Bloques Convolucionales ---\n",
    "        # nn.Conv1d espera la entrada como (Batch, Channels, SeqLen)\n",
    "        \n",
    "        # (B, 13, T) -> (B, 64, T/2)\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv1d(n_input_channels, hidd_size, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidd_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        # (B, 64, T/2) -> (B, 128, T/4)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(hidd_size, hidd_size * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidd_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        # (B, 128, T/4) -> (B, 256, T/4)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv1d(hidd_size * 2, hidd_size * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidd_size * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # --- Pooling Global y Clasificación ---\n",
    "        \n",
    "        # Colapsa la dimensión de secuencia (T/4) a 1\n",
    "        # (B, 256, T/4) -> (B, 256, 1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # (B, 256) -> (B, 35)\n",
    "        self.fc = nn.Linear(hidd_size * 4, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1) \n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Clasificar\n",
    "        return self.fc(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa el Positional Encoding para añadir información de posición.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Matriz de Positional Encoding\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [BatchSize, SeqLen, EmbeddingDim]\n",
    "        \"\"\"\n",
    "        # x.shape[1] es la longitud de la secuencia (SeqLen)\n",
    "        x = x + self.pe[:x.size(1)].transpose(0, 1) # Transpose para hacer Broadcasting [1, SeqLen, EmbDim]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input_features: int,  # e.g., 13 for MFCCs\n",
    "        n_output_classes: int = 35,\n",
    "        d_model: int = 128,      # Dimensión de la representación del Transformer\n",
    "        nhead: int = 8,          # Número de cabezas de atención\n",
    "        d_hid: int = 256,        # Dimensión de la capa FeedForward (FNN)\n",
    "        n_layers: int = 6,       # Número de bloques Codificadores\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 1. Proyección de entrada: de n_input_features a d_model\n",
    "        self.input_projection = nn.Linear(n_input_features, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # 3. Bloques Codificadores del Transformer\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=d_hid, \n",
    "            dropout=dropout,\n",
    "            batch_first=True # Importante para que el input sea [Batch, Seq, Feature]\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        # 4. Capa de Clasificación Final\n",
    "        # La estrategia es tomar la representación del PRIMER token (similar al [CLS] de BERT,\n",
    "        # pero aquí usamos el primer frame de audio como vector de secuencia).\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, n_output_classes)\n",
    "        )\n",
    "        \n",
    "        # Inicialización de pesos (buena práctica para Transformers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_projection.bias.data.zero_()\n",
    "        self.input_projection.weight.data.uniform_(-initrange, initrange)\n",
    "        self.classifier[0].bias.data.zero_()\n",
    "        self.classifier[0].weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [BatchSize, SeqLen, n_input_features]\n",
    "        \"\"\"\n",
    "        # 1. Proyección de la entrada\n",
    "        x = self.input_projection(x) * np.sqrt(self.d_model) # Factor de escalamiento\n",
    "        \n",
    "        # 2. Agregar Positional Encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # 3. Pasar por los Codificadores del Transformer\n",
    "        # torch.Size([BatchSize, SeqLen, d_model])\n",
    "        output = self.transformer_encoder(x) \n",
    "        \n",
    "        # 4. Clasificación: Tomar la salida del primer frame (SeqLen=0) como \n",
    "        # la representación agregada de toda la secuencia.\n",
    "        final_representation = output[:, 0, :] # [BatchSize, d_model]\n",
    "        \n",
    "        # 5. Capa Lineal Final\n",
    "        return self.classifier(final_representation)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidd_size=256,\n",
    "        in_channels = 13,\n",
    "        out_channels = 64,\n",
    "        N_conv_blocks = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        rnn_in = 0\n",
    "        if N_conv_blocks == 1:\n",
    "            self.conv_blocks = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2)     \n",
    "            )\n",
    "            rnn_in = out_channels\n",
    "        elif N_conv_blocks == 2:\n",
    "            self.conv_blocks = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2)\n",
    "            )\n",
    "            rnn_in = out_channels\n",
    "        elif N_conv_blocks == 3:\n",
    "            self.conv_blocks = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2)\n",
    "            )\n",
    "            rnn_in = out_channels\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Choose valid number (1-3)')\n",
    "\n",
    "        self.rnn_layer = RNNModel(\n",
    "            n_input_channels=rnn_in,\n",
    "            rnn_type=\"RNN\",\n",
    "            hidd_size=hidd_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        perm_x = torch.permute(x, (0, 2, 1))\n",
    "        conv_out = self.conv_blocks(perm_x)\n",
    "        deperm_x = torch.permute(conv_out, (0, 2, 1))\n",
    "        return self.rnn_layer(deperm_x)\n",
    "\n",
    "class MejorCNN1DModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidd_size=256,\n",
    "        in_channels=13,   # Tus 13 MFCCs\n",
    "        num_classes=35    # Clases de SpeechCommands\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Bloques CNN ---\n",
    "        # Aumentamos canales, usamos BatchNorm, y kernels más grandes\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Bloque 1\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=64, \n",
    "                kernel_size=7,  \n",
    "                padding='same'\n",
    "            ),\n",
    "            nn.BatchNorm1d(64), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),    \n",
    "            nn.Dropout(0.2),    \n",
    "\n",
    "            # Bloque 2\n",
    "            nn.Conv1d(\n",
    "                in_channels=64, \n",
    "                out_channels=128,\n",
    "                kernel_size=5, \n",
    "                padding='same'\n",
    "            ),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),    \n",
    "            nn.Dropout(0.2),    \n",
    "\n",
    "            # Bloque 3\n",
    "            nn.Conv1d(\n",
    "                in_channels=128, \n",
    "                out_channels=256,\n",
    "                kernel_size=3, \n",
    "                padding='same'\n",
    "            ),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)     \n",
    "            # La secuencia de salida será L // 8\n",
    "        )\n",
    "        \n",
    "        # --- Capa RNN ---\n",
    "        # El input para la RNN ahora tiene 256 canales\n",
    "        # (El tamaño del feature de la CNN)\n",
    "        rnn_in_features = 256 \n",
    "        \n",
    "        self.rnn_layer = RNNModel(\n",
    "            n_input_channels=rnn_in_features,\n",
    "            rnn_type=\"GRU\",       # <-- RECOMENDADO: Usa GRU o LSTM, no \"RNN\"\n",
    "            hidd_size=hidd_size,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        perm_x = x.permute(0, 2, 1)\n",
    "        conv_out = self.conv_blocks(perm_x)\n",
    "        deperm_x = conv_out.permute(0, 2, 1)\n",
    "        \n",
    "        return self.rnn_layer(deperm_x)\n",
    "\n",
    "class SpeechCommandTCN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_classes, tcn_params):\n",
    "        super().__init__()\n",
    "        # Instanciamos la TCN base\n",
    "        self.tcn = TCN(num_inputs=num_inputs, **tcn_params)\n",
    "\n",
    "        # Calculamos la dimensión de salida de la TCN (el último canal)\n",
    "        # Si tcn_params['num_channels'] es [64, 64, 128, 128], output_dim es 128\n",
    "        output_dim = tcn_params['num_channels'][-1]\n",
    "\n",
    "        # Capa lineal para convertir features en probabilidades de clases\n",
    "        self.classifier = nn.Linear(output_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, Time, Features) from FeaturesDataset\n",
    "        # TCN expects (Batch, Channels, Time) when input_shape='NCL'\n",
    "        x = x.permute(0, 2, 1) # Convert (Batch, Time, Features) to (Batch, Features, Time)\n",
    "\n",
    "        # 1. Feature Extraction (TCN)\n",
    "        out = self.tcn(x)  # Salida: (Batch, Out_Channels, Time)\n",
    "\n",
    "        # 2. Global Average Pooling\n",
    "        # Promediamos sobre la dimensión del tiempo (dim=2) para tener un vector por audio\n",
    "        out = out.mean(dim=2) # Output: (Batch, Out_Channels)\n",
    "\n",
    "        # 3. Clasificación\n",
    "        return self.classifier(out)\n",
    "def save_model(model, path, config=None):\n",
    "    \"\"\"\n",
    "    Guarda un modelo PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "    - model: instancia de nn.Module\n",
    "    - path: destino del archivo .pt\n",
    "    - config: diccionario con parámetros del modelo\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"config\": config if config is not None else {}\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "def load_trained_model(model_class, checkpoint_path, device=\"cuda\", config=None, **override_kwargs):\n",
    "    \"\"\"\n",
    "    Carga un modelo guardado, permitiendo entregar un config externo.\n",
    "\n",
    "    Prioridades de config:\n",
    "        1. config explícito pasado a la función\n",
    "        2. config guardado en el checkpoint\n",
    "        3. override_kwargs (pisan todo lo anterior)\n",
    "\n",
    "    Parameters:\n",
    "    - model_class: clase del modelo\n",
    "    - checkpoint_path: ruta al archivo .pt\n",
    "    - device: \"cpu\" o \"cuda\"\n",
    "    - config: diccionario completo externo para construir el modelo\n",
    "    - override_kwargs: parámetros puntuales que quieras reemplazar\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # 1) checkpoint config\n",
    "    if checkpoint.get(\"config\", {}) is not None:\n",
    "        final_config = checkpoint.get(\"config\", {}).copy()\n",
    "\n",
    "    # 2) si el usuario entrega un config, reemplaza completamente al del checkpoint\n",
    "    if config is not None:\n",
    "        final_config = config.copy()\n",
    "\n",
    "    # 3) override puntual de valores individuales\n",
    "    final_config.update(override_kwargs)\n",
    "\n",
    "    # construir modelo\n",
    "    model = model_class(**final_config)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Modelo cargado desde {checkpoint_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8e3a4",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu):\n",
    "    # Predicción\n",
    "    y_predicted = model(x_batch)\n",
    "\n",
    "    # Cálculo de loss\n",
    "    loss = criterion(y_predicted, y_batch)\n",
    "\n",
    "    # Actualización de parámetros\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return y_predicted, loss\n",
    "\n",
    "\n",
    "def evaluate(val_loader, model, criterion, use_gpu):\n",
    "    cumulative_loss = 0\n",
    "    cumulative_predictions = 0\n",
    "    data_count = 0\n",
    "\n",
    "    for x_val, y_val in val_loader:\n",
    "        if use_gpu:\n",
    "            x_val = x_val.cuda()\n",
    "            y_val = y_val.cuda()\n",
    "\n",
    "        y_predicted = model(x_val)\n",
    "\n",
    "        loss = criterion(y_predicted, y_val)\n",
    "\n",
    "        class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "\n",
    "        cumulative_predictions += (y_val == class_prediction).sum().item()\n",
    "        cumulative_loss += loss.item() * y_val.shape[0]\n",
    "        data_count += y_val.shape[0]\n",
    "\n",
    "    val_acc = cumulative_predictions / data_count\n",
    "    val_loss = cumulative_loss / data_count\n",
    "\n",
    "    return val_acc, val_loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    n_evaluations_per_epoch=6,\n",
    "    use_gpu=False,\n",
    "    patience=5,                 \n",
    "    model_config = None,\n",
    "    model_arch = \"ola\"\n",
    "):\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=use_gpu\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=use_gpu\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-9)\n",
    "\n",
    "    curves = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    iteration = 0\n",
    "    n_batches = len(train_loader)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    print(n_batches)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\rEpoch {epoch + 1}/{epochs}\")\n",
    "        cumulative_train_loss = 0\n",
    "        cumulative_train_corrects = 0\n",
    "        examples_count = 0\n",
    "\n",
    "        model.train()\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            if use_gpu:\n",
    "                x_batch = x_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "\n",
    "            y_predicted, loss = train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu)\n",
    "\n",
    "            cumulative_train_loss += loss.item() * x_batch.shape[0]\n",
    "            examples_count += y_batch.shape[0]\n",
    "\n",
    "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "            cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
    "\n",
    "            if (i % (n_batches // n_evaluations_per_epoch) == 0) and (i > 0):\n",
    "                train_loss = cumulative_train_loss / examples_count\n",
    "                train_acc = cumulative_train_corrects / examples_count\n",
    "                print(f\"Iteration {iteration} - Batch {i}/{len(train_loader)} - Train loss: {train_loss}, Train acc: {train_acc}\")\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_acc, val_loss = evaluate(val_loader, model, criterion, use_gpu)\n",
    "\n",
    "        print(f\"Val loss: {val_loss}, Val acc: {val_acc}\")\n",
    "\n",
    "        train_loss = cumulative_train_loss / examples_count\n",
    "        train_acc = cumulative_train_corrects / examples_count\n",
    "\n",
    "        curves[\"train_acc\"].append(train_acc)\n",
    "        curves[\"val_acc\"].append(val_acc)\n",
    "        curves[\"train_loss\"].append(train_loss)\n",
    "        curves[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"Sin mejora. Paciencia: {epochs_without_improvement}/{patience}\")\n",
    "\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"Early stopping activado!\")\n",
    "                break\n",
    "\n",
    "    total_time = time.perf_counter() - t0\n",
    "    print(f\"Tiempo total de entrenamiento: {total_time:.4f} [s]\")\n",
    "\n",
    "    if model_config is not None:\n",
    "        save_path = \"model_weights\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        save_model(model, os.path.join(save_path, f\"{model_arch}_{timestamp}.pt\"))\n",
    "    model.cpu()\n",
    "    return curves, total_time\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_models_metrics(models, dataloader, criterion, use_gpu=True, num_repeats=15):\n",
    "    \"\"\"\n",
    "    Evalúa múltiples modelos y calcula métricas promedio y desviación estándar.\n",
    "    Mide el tiempo de inferencia por batch, promediando 'num_repeats' ejecuciones.\n",
    "    \"\"\"\n",
    "\n",
    "    all_metrics = {\n",
    "        \"accuracy\": [],\n",
    "        \"recall\": [],\n",
    "        \"precision\": [],\n",
    "        \"f1\": [],\n",
    "        \"infer_time\": []    # Lista de listas, donde cada lista interior son los tiempos promedio por batch\n",
    "    }\n",
    "\n",
    "    all_times = []      # ← promedio de tiempos por modelo (un valor por modelo)\n",
    "    all_f1_list = []    # ← lista de f1 score por modelo (un valor por modelo)\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        if use_gpu:\n",
    "            model.cuda()\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        losses = []\n",
    "\n",
    "        # Lista de tiempos de inferencia promedio (promedio de las 'num_repeats' ejecuciones)\n",
    "        infer_times_per_batch = [] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                if use_gpu:\n",
    "                    X, y = X.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "                \n",
    "                # Para asegurar consistencia, sincronizar GPU antes de empezar\n",
    "                if use_gpu:\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                # ---- Medir tiempo (Inferencia Repetida) ----\n",
    "                batch_run_times = []\n",
    "                \n",
    "                # Repetir la inferencia 'num_repeats' veces\n",
    "                for r in range(num_repeats):\n",
    "                    start_time = time.time()\n",
    "                    outputs = model(X)\n",
    "                    \n",
    "                    if use_gpu:\n",
    "                        torch.cuda.synchronize() # Esperar que la GPU termine\n",
    "                    \n",
    "                    end_time = time.time()\n",
    "                    \n",
    "                    # Descartar la primera ejecución (r=0) para evitar el tiempo de 'warm-up'\n",
    "                    if r > 0: \n",
    "                        batch_run_times.append(end_time - start_time)\n",
    "\n",
    "                # Calcular el tiempo de inferencia promedio para este batch\n",
    "                # Si num_repeats=1, se añade 0 a batch_run_times. Hay que manejarlo.\n",
    "                if len(batch_run_times) > 0:\n",
    "                    avg_batch_time = np.mean(batch_run_times)\n",
    "                else:\n",
    "                    # Si solo se ejecutó una vez (num_repeats=1) o menos, usamos el primer tiempo.\n",
    "                    # Nota: El tiempo del warm-up no es ideal, pero es el único disponible.\n",
    "                    start_time = time.time()\n",
    "                    outputs = model(X)\n",
    "                    if use_gpu:\n",
    "                        torch.cuda.synchronize()\n",
    "                    avg_batch_time = time.time() - start_time\n",
    "                    \n",
    "                infer_times_per_batch.append(avg_batch_time)\n",
    "                # --------------------------------------------\n",
    "                \n",
    "                loss = criterion(outputs, y)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                # Calcular predicciones y métricas\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        # --- Métricas totales por modelo ---\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred, average='macro', zero_division='warn')\n",
    "        prec = precision_score(y_true, y_pred, average='macro', zero_division='warn')\n",
    "        f1 = f1_score(y_true, y_pred, average='macro', zero_division='warn')\n",
    "\n",
    "        all_metrics[\"accuracy\"].append(acc)\n",
    "        all_metrics[\"recall\"].append(rec)\n",
    "        all_metrics[\"precision\"].append(prec)\n",
    "        all_metrics[\"f1\"].append(f1)\n",
    "        \n",
    "        # Guardar los tiempos de inferencia promedio por batch para este modelo\n",
    "        all_metrics[\"infer_time\"].append(infer_times_per_batch)\n",
    "        all_times.append(np.mean(infer_times_per_batch))\n",
    "        all_f1_list.append(f1)\n",
    "\n",
    "    # --- Promedios Finales ---\n",
    "    # Nota: Aquí promediamos las métricas entre los modelos\n",
    "    metrics_mean = {k: np.mean(v) if k != \"infer_time\" else None for k, v in all_metrics.items()}\n",
    "    metrics_std  = {k: np.std(v)  if k != \"infer_time\" else None for k, v in all_metrics.items()}\n",
    "    \n",
    "    # El promedio y STD de \"infer_time\" se maneja directamente con all_times\n",
    "    metrics_mean[\"infer_time\"] = np.mean(all_times)\n",
    "    metrics_std[\"infer_time\"] = np.std(all_times)\n",
    "\n",
    "    return metrics_mean, metrics_std, all_metrics, all_times, all_f1_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c360d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b30a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(wf, sample_rate, label=\"\", figname=None):\n",
    "    \"\"\"\n",
    "    Muestra el waveform (izquierda) y los MFCCs (derecha) de una señal de audio.\n",
    "\n",
    "    Parámetros:\n",
    "        wf (Tensor): señal de audio [1, N] o [N]\n",
    "        sample_rate (int): frecuencia de muestreo (Hz)\n",
    "        label (str): etiqueta opcional para el título\n",
    "        figname (str): ruta para guardar la figura (si es None, solo muestra)\n",
    "    \"\"\"\n",
    "    if isinstance(wf, torch.Tensor):\n",
    "        wf = wf.squeeze().cpu()\n",
    "\n",
    "    # === Transformación MFCC ===\n",
    "    mfcc_transform = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=13,\n",
    "        melkwargs={\"n_fft\": 320, \"hop_length\": 160, \"n_mels\": 23},\n",
    "        log_mels=True\n",
    "    )\n",
    "    mfcc = mfcc_transform(wf.unsqueeze(0)).squeeze().cpu().numpy()  # [n_mfcc, time]\n",
    "\n",
    "    # === Crear figura con 2 subplots ===\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # --- Waveform ---\n",
    "    time = torch.arange(0, len(wf)) / sample_rate\n",
    "    axes[0].plot(time, wf.numpy(), color=\"steelblue\", linewidth=1.0)\n",
    "    axes[0].set_title(\"Waveform\", fontsize=12)\n",
    "    axes[0].set_xlabel(\"Tiempo [s]\")\n",
    "    axes[0].set_ylabel(\"Amplitud\")\n",
    "\n",
    "    # --- MFCC ---\n",
    "    sns.heatmap(mfcc, ax=axes[1], cmap=\"viridis\", cbar=True)\n",
    "    axes[1].set_title(\"MFCCs\", fontsize=12)\n",
    "    axes[1].set_xlabel(\"Tiempo (frames)\")\n",
    "    axes[1].set_ylabel(\"Coeficiente MFCC\")\n",
    "\n",
    "    fig.suptitle(f\"Audio: {label}\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === Guardar o mostrar ===\n",
    "    if figname:\n",
    "        name = os.path.join('img', f'{figname}.pdf')\n",
    "        plt.savefig(name, bbox_inches=\"tight\")\n",
    "        print(f\"Figura guardada en {name}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    " \n",
    "def plot_f1_vs_time_custom_legend(df, filename='f1_vs_inference_time_shapes.pdf'):\n",
    "    \"\"\"\n",
    "    Grafica F1 vs Tiempo.\n",
    "    - Color = Arquitectura\n",
    "    - Forma (Marker) = Feature\n",
    "    - Leyenda personalizada dividida en dos secciones.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(13, 8)) # Un poco más ancho para la leyenda\n",
    "    \n",
    "    # 1. Definir Mapeo de Formas (Markers) para cada Feature\n",
    "    # o: círculo, s: cuadrado, ^: triángulo, D: diamante, P: plus grueso\n",
    "    marker_map = {\n",
    "        'mfcc': 'o',              \n",
    "        'mfcc_delta': 's',       \n",
    "        'mfcc_delta_delta': '^', \n",
    "        'wav2vec2': 'D'          \n",
    "    }\n",
    "    \n",
    "    # Alias para que la leyenda se vea limpia\n",
    "    feat_legend_names = {\n",
    "        'mfcc': 'MFCC',\n",
    "        'mfcc_delta': 'MFCC Delta',\n",
    "        'mfcc_delta_delta': 'MFCC Delta²',\n",
    "        'wav2vec2': 'Wav2Vec2'\n",
    "    }\n",
    "\n",
    "    # 2. Definir Colores para Arquitecturas\n",
    "    unique_archs = df['Architecture'].unique()\n",
    "\n",
    "    # 3. Bucle de ploteo\n",
    "    for idx, row in df.iterrows():\n",
    "        arch_name = row['Architecture']\n",
    "        feat_name = row['Feature']\n",
    "        \n",
    "        c = color_map.get(arch_name, 'gray')\n",
    "        m = marker_map.get(feat_name, 'x') # 'x' por si sale una feature nueva desconocida\n",
    "\n",
    "        plt.errorbar(\n",
    "            x=row['Time (ms)'],\n",
    "            y=row['F1 Mean'],\n",
    "            xerr=row['Time std (ms)'],\n",
    "            yerr=row['F1 Std'],\n",
    "            fmt=m,               # <--- Aquí aplicamos la FORMA\n",
    "            color=c,             # <--- Aquí aplicamos el COLOR\n",
    "            ecolor=c,            # Color de la barra de error igual al punto\n",
    "            capsize=5,\n",
    "            markersize=10,       # Puntos un poco más grandes para ver la forma\n",
    "            alpha=0.8,\n",
    "            linestyle='None'     # Importante: que no intente unir líneas\n",
    "        )\n",
    "\n",
    "    # 4. Construcción de la Leyenda Personalizada (Manual)\n",
    "    legend_elements = []\n",
    "    \n",
    "    # -- Sección 1: Arquitecturas (Colores) --\n",
    "    legend_elements.append(Line2D([0], [0], color='w', label=r'$\\bf{Arquitectura}$')) # Título falso\n",
    "    for arch in unique_archs:\n",
    "        legend_elements.append(\n",
    "            Line2D([0], [0], marker='o', color='w', label=arch,\n",
    "                   markerfacecolor=color_map[arch], markersize=10)\n",
    "        )\n",
    "    \n",
    "    # Espacio en blanco en la leyenda\n",
    "    legend_elements.append(Line2D([0], [0], color='w', label=' ')) \n",
    "    \n",
    "    # -- Sección 2: Features (Formas) --\n",
    "    legend_elements.append(Line2D([0], [0], color='w', label=r'$\\bf{Features}$')) # Título falso\n",
    "    # Filtramos solo las features que realmente existen en el dataframe\n",
    "    present_features = df['Feature'].unique()\n",
    "    \n",
    "    for feat in present_features:\n",
    "        if feat in marker_map:\n",
    "            display_name = feat_legend_names.get(feat, feat)\n",
    "            legend_elements.append(\n",
    "                Line2D([0], [0], marker=marker_map[feat], color='w', label=display_name,\n",
    "                       markerfacecolor='black', markersize=10, markeredgecolor='black')\n",
    "                       # Usamos negro para indicar que la forma es lo importante, no el color\n",
    "            )\n",
    "\n",
    "    # Configuración final del gráfico\n",
    "    plt.xlabel(\"Tiempo de inferencia (ms)\", fontsize=12)\n",
    "    plt.ylabel(\"F1 Score Promedio (%)\", fontsize=12)\n",
    "    plt.title(\"Comparativa: Arquitectura vs Feature Input\", fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Colocar la leyenda fuera a la derecha para no tapar datos\n",
    "    plt.legend(handles=legend_elements, loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join('img', filename)\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Gráfico guardado en: {save_path}\")\n",
    "def show_curves(all_curves, suptitle='', filename = ''):\n",
    "    min_len = {k: min(len(c[k]) for c in all_curves) for k in all_curves[0].keys()}\n",
    "\n",
    "    trimmed = {\n",
    "        k: np.array([c[k][:min_len[k]] for c in all_curves])\n",
    "        for k in all_curves[0].keys()\n",
    "    }\n",
    "\n",
    "    final_curve_means = {k: trimmed[k].mean(axis=0) for k in trimmed}\n",
    "    final_curve_stds  = {k: trimmed[k].std(axis=0)  for k in trimmed}\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    fig.set_facecolor('white')\n",
    "\n",
    "    epochs = np.arange(len(final_curve_means[\"val_loss\"])) + 1\n",
    "\n",
    "    ax[0].plot(epochs, final_curve_means['val_loss'], label='validation')\n",
    "    ax[0].plot(epochs, final_curve_means['train_loss'], label='training')\n",
    "    ax[0].fill_between(epochs,\n",
    "                       y1=final_curve_means[\"val_loss\"] - final_curve_stds[\"val_loss\"],\n",
    "                       y2=final_curve_means[\"val_loss\"] + final_curve_stds[\"val_loss\"], alpha=.5)\n",
    "    ax[0].fill_between(epochs,\n",
    "                       y1=final_curve_means[\"train_loss\"] - final_curve_stds[\"train_loss\"],\n",
    "                       y2=final_curve_means[\"train_loss\"] + final_curve_stds[\"train_loss\"], alpha=.5)\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Loss evolution during training')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # ==== Plot de precisión ====\n",
    "    ax[1].plot(epochs, final_curve_means['val_acc'], label='validation')\n",
    "    ax[1].plot(epochs, final_curve_means['train_acc'], label='training')\n",
    "    ax[1].fill_between(epochs,\n",
    "                       y1=final_curve_means[\"val_acc\"] - final_curve_stds[\"val_acc\"],\n",
    "                       y2=final_curve_means[\"val_acc\"] + final_curve_stds[\"val_acc\"], alpha=.5)\n",
    "    ax[1].fill_between(epochs,\n",
    "                       y1=final_curve_means[\"train_acc\"] - final_curve_stds[\"train_acc\"],\n",
    "                       y2=final_curve_means[\"train_acc\"] + final_curve_stds[\"train_acc\"], alpha=.5)\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_title('Accuracy evolution during training')\n",
    "    ax[1].legend()\n",
    "\n",
    "    fig.suptitle(suptitle, fontsize=16, weight=\"bold\")\n",
    "\n",
    "    filepath = os.path.join('img', f'{filename}.pdf')\n",
    "    plt.savefig(filepath, bbox_inches='tight', format='pdf')\n",
    "    plt.close(fig)\n",
    "\n",
    "def get_metrics_and_confusion_matrix(models, dataset, name='', filename=''):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=min(32, len(dataset)))\n",
    "\n",
    "\n",
    "    y_true = torch.cat([y for _, y in dataloader])\n",
    "    total_classes = len(torch.unique(y_true))\n",
    "    \n",
    "    # Definir un ID seguro para 'others'\n",
    "    OTHERS_ID = total_classes \n",
    "\n",
    "    # --- Obtener nombres de clases del dataset ---\n",
    "    if hasattr(dataset, \"idx_to_label\"):\n",
    "        # Manejo robusto si es dict o lista\n",
    "        if isinstance(dataset.idx_to_label, dict):\n",
    "            class_names = [dataset.idx_to_label[i] for i in range(total_classes)]\n",
    "        else:\n",
    "            class_names = dataset.idx_to_label\n",
    "    elif hasattr(dataset, \"labels\"):\n",
    "        class_names = dataset.labels\n",
    "    elif hasattr(dataset, \"classes\"): # Común en ImageFolder\n",
    "        class_names = dataset.classes\n",
    "    else:\n",
    "        class_names = [str(i) for i in range(total_classes)]\n",
    "\n",
    "    # --- NORMALIZACIÓN DE STRINGS ---\n",
    "    # Creamos un diccionario insensible a mayúsculas/espacios para buscar\n",
    "    # { \"stop\": 3, \"go\": 5, ... }\n",
    "    name_to_idx = {name.lower().strip(): i for i, name in enumerate(class_names)}\n",
    "\n",
    "    target_major_names = [\n",
    "        \"off\", \"right\", \"stop\", \"go\", \"on\", \n",
    "        \"yes\", \"up\", \"down\", \"no\", \"left\"\n",
    "    ]\n",
    "    \n",
    "    # Las clases menores no necesitan listarse explícitamente para la lógica, \n",
    "    # se calculan como el complemento, pero si las usas para validar:\n",
    "    # target_minor_names = [...] \n",
    "\n",
    "    # 2. Convertir nombres a índices (top10) con validación\n",
    "    top10 = []\n",
    "    found_names = []\n",
    "    \n",
    "    print(f\"--- Validando clases para {name} ---\")\n",
    "    for name_query in target_major_names:\n",
    "        clean_name = name_query.lower().strip()\n",
    "        if clean_name in name_to_idx:\n",
    "            idx = name_to_idx[clean_name]\n",
    "            top10.append(idx)\n",
    "            found_names.append(class_names[idx]) # Guardamos el nombre original real\n",
    "        else:\n",
    "            # ESTE PRINT ES CLAVE: Te dirá qué clase está fallando\n",
    "            print(f\"ADVERTENCIA: La clase '{name_query}' no está en el dataset.\")\n",
    "            print(f\"   -> Clases disponibles (ejemplo): {class_names[:5]}...\")\n",
    "    \n",
    "    top10_set = set(top10)\n",
    "\n",
    "    # ==========================================\n",
    "    # === FIN DE MODIFICACIÓN HARDCODED ===\n",
    "    # ==========================================\n",
    "\n",
    "    # --- Preparar Grupo A ---\n",
    "    y_true_A = y_true.clone()\n",
    "    for cls in range(total_classes):\n",
    "        if cls not in top10_set:\n",
    "            y_true_A[y_true_A == cls] = OTHERS_ID \n",
    "\n",
    "    # Usamos los nombres REALES encontrados, más 'others'\n",
    "    ids_A = top10 + [OTHERS_ID] \n",
    "    label_names_A = found_names + [\"others\"]\n",
    "\n",
    "    # --- Preparar Grupo B (Resto) ---\n",
    "    # CORRECCIÓN: Había un error de sintaxis \"not n\" -> \"not in\"\n",
    "    mask_B = torch.tensor([c.item() not in top10_set for c in y_true], dtype=torch.bool)\n",
    "    \n",
    "    y_true_B = y_true[mask_B]\n",
    "    ids_B = sorted(torch.unique(y_true_B).tolist())\n",
    "    label_names_B = [class_names[c] for c in ids_B]\n",
    "\n",
    "    # ... (resto de tu función map_groupA, compute_group, etc.)\n",
    "    def map_groupA(pred):\n",
    "        predA = pred.clone()\n",
    "        for cls in range(total_classes):\n",
    "            if cls not in top10_set:\n",
    "                predA[predA == cls] = OTHERS_ID \n",
    "        return predA\n",
    "\n",
    "    # --- Compute Group ---\n",
    "    def compute_group(models, dataloader, y_true_group, target_ids, mask=None, map_func=None, compute_f1_total=False):\n",
    "        cms, f1_scores = [], []\n",
    "        f1_total_scores = [] \n",
    "\n",
    "        for model in models:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            preds_raw = [] \n",
    "            preds_group = [] \n",
    "            \n",
    "            with torch.no_grad(): \n",
    "                for x, _ in dataloader:\n",
    "                    x = x.to(device)\n",
    "                    p = model(x).argmax(dim=1)\n",
    "                    \n",
    "                    preds_raw.append(p)\n",
    "\n",
    "                    p_group = map_func(p) if map_func else p\n",
    "                    preds_group.append(p_group)\n",
    "\n",
    "            preds_raw = torch.cat(preds_raw)\n",
    "            preds_group = torch.cat(preds_group)\n",
    "\n",
    "            if mask is not None:\n",
    "                preds_group = preds_group[mask]\n",
    "                preds_raw = preds_raw[mask] \n",
    "\n",
    "            preds_cpu = preds_group.cpu()\n",
    "            \n",
    "            cm = confusion_matrix(\n",
    "                y_true_group,\n",
    "                preds_cpu,\n",
    "                labels=target_ids, \n",
    "                normalize=\"true\",\n",
    "            )\n",
    "            cms.append(cm)\n",
    "            f1_scores.append(f1_score(y_true_group, preds_cpu, average='macro'))\n",
    "\n",
    "            if compute_f1_total:\n",
    "                f1_total = f1_score(y_true.cpu(), preds_raw.cpu(), average='macro')\n",
    "                f1_total_scores.append(f1_total)\n",
    "\n",
    "        if compute_f1_total:\n",
    "            return (\n",
    "                np.mean(cms, axis=0),\n",
    "                np.std(cms, axis=0),\n",
    "                np.mean(f1_scores) * 100,\n",
    "                np.std(f1_scores) * 100,\n",
    "                np.mean(f1_total_scores) * 100,\n",
    "                np.std(f1_total_scores) * 100,\n",
    "            )\n",
    "        \n",
    "        return (\n",
    "            np.mean(cms, axis=0),\n",
    "            np.std(cms, axis=0),\n",
    "            np.mean(f1_scores) * 100,\n",
    "            np.std(f1_scores) * 100,\n",
    "        )\n",
    "\n",
    "    # Llamadas a compute_group\n",
    "    cmA_mean, cmA_std, accA_mean, accA_std, f1_total_mean, f1_total_std = compute_group(\n",
    "        models, dataloader, y_true_A, ids_A, map_func=map_groupA, compute_f1_total=True\n",
    "    )\n",
    "    \n",
    "    cmB_mean, cmB_std, accB_mean, accB_std = compute_group(\n",
    "        models, dataloader, y_true_B, ids_B, mask=mask_B, compute_f1_total=False \n",
    "    )\n",
    "\n",
    "    os.makedirs(\"img\", exist_ok=True)\n",
    "\n",
    "    def plot_both_cms(cmA_mean, cmA_std, labelsA, accA_mean, accA_std,\n",
    "                            cmB_mean, cmB_std, labelsB, accB_mean, accB_std,\n",
    "                            filename, f1_total_mean, f1_total_std, main_title=''): \n",
    "\n",
    "        vmin = min(cmA_mean.min(), cmB_mean.min())\n",
    "        vmax = max(cmA_mean.max(), cmB_mean.max())\n",
    "        \n",
    "        if main_title:\n",
    "             main_title = f\"{main_title}\\nOverall F1 (macro) = {f1_total_mean:.2f} ± {f1_total_std:.2f}%\"\n",
    "        else:\n",
    "             main_title = f\"Overall F1 (macro) = {f1_total_mean:.2f} ± {f1_total_std:.2f}%\"\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        fig.suptitle(main_title, fontsize=16, fontweight='bold')\n",
    "        \n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7]) \n",
    "\n",
    "        # --- Plot A ---\n",
    "        ax = axs[0]\n",
    "        imA = ax.imshow(cmA_mean, cmap=plt.cm.Blues, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f\"Main Classes + Others\\nF1 (macro): {accA_mean:.2f} ± {accA_std:.2f}%\") \n",
    "\n",
    "        ax.set_xticks(np.arange(len(labelsA)))\n",
    "        ax.set_yticks(np.arange(len(labelsA)))\n",
    "        ax.set_xticklabels(labelsA, rotation=90, ha=\"center\")\n",
    "        ax.set_yticklabels(labelsA)\n",
    "        \n",
    "        norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "        for i in range(len(labelsA)):\n",
    "            for j in range(len(labelsA)):\n",
    "                rgba = imA.cmap(norm(cmA_mean[i, j]))\n",
    "                luminance = 0.299*rgba[0] + 0.587*rgba[1] + 0.114*rgba[2]\n",
    "                text_color = \"white\" if luminance < 0.5 else \"black\"\n",
    "                ax.text(j, i, f\"{cmA_mean[i,j]:.2f}\\n±{cmA_std[i,j]:.2f}\",\n",
    "                        ha=\"center\", va=\"center\", fontsize=8, color=text_color)\n",
    "\n",
    "        ax.set_xlabel(\"Predicted label\")\n",
    "        ax.set_ylabel(\"True label\")\n",
    "\n",
    "        # --- Plot B ---\n",
    "        ax = axs[1]\n",
    "        imB = ax.imshow(cmB_mean, cmap=plt.cm.Blues, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f\"Minor Classes (Others breakdown)\\nF1 (macro): {accB_mean:.2f} ± {accB_std:.2f}%\") \n",
    "\n",
    "        ax.set_xticks(np.arange(len(labelsB)))\n",
    "        ax.set_yticks(np.arange(len(labelsB)))\n",
    "        ax.set_xticklabels(labelsB, rotation=90, ha=\"center\")\n",
    "        ax.set_yticklabels(labelsB)\n",
    "\n",
    "        for i in range(len(labelsB)):\n",
    "            for j in range(len(labelsB)):\n",
    "                rgba = imB.cmap(norm(cmB_mean[i, j]))\n",
    "                luminance = 0.299*rgba[0] + 0.587*rgba[1] + 0.114*rgba[2]\n",
    "                text_color = \"white\" if luminance < 0.5 else \"black\"\n",
    "                ax.text(j, i, f\"{cmB_mean[i,j]:.2f}\",\n",
    "                        ha=\"center\", va=\"center\", fontsize=6, color=text_color)\n",
    "\n",
    "        ax.set_xlabel(\"Predicted label\")\n",
    "        ax.set_ylabel(\"True label\")\n",
    "\n",
    "        cbar = fig.colorbar(imA, cax=cbar_ax) \n",
    "        cbar.set_label(\"Normalized frequency\", rotation=270, labelpad=15)\n",
    "        plt.subplots_adjust(right=0.9, top=0.85) \n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    plot_both_cms(\n",
    "        cmA_mean, cmA_std, label_names_A, accA_mean, accA_std,\n",
    "        cmB_mean, cmB_std, label_names_B, accB_mean, accB_std,\n",
    "        filename=f'{filename}.pdf',\n",
    "        f1_total_mean=f1_total_mean,\n",
    "        f1_total_std=f1_total_std,\n",
    "        main_title=f\"Results for model {'s' if len(models)>1 else ''} on {name}\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"f1_total_mean\": f1_total_mean,\n",
    "        \"f1_total_std\": f1_total_std,\n",
    "        \"cmA_mean\": cmA_mean,\n",
    "        \"cmB_mean\": cmB_mean,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89760d",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47cce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/features/train_mfcc.pt ya existe, saltando...\n",
      "data/features/train_mfcc_delta.pt ya existe, saltando...\n",
      "data/features/train_mfcc_delta_delta.pt ya existe, saltando...\n",
      "data/features/train_wav2vec2.pt ya existe, saltando...\n",
      "data/features/val_mfcc.pt ya existe, saltando...\n",
      "data/features/val_mfcc_delta.pt ya existe, saltando...\n",
      "data/features/val_mfcc_delta_delta.pt ya existe, saltando...\n",
      "data/features/val_wav2vec2.pt ya existe, saltando...\n",
      "data/features/test_mfcc.pt ya existe, saltando...\n",
      "data/features/test_mfcc_delta.pt ya existe, saltando...\n",
      "data/features/test_mfcc_delta_delta.pt ya existe, saltando...\n",
      "data/features/test_wav2vec2.pt ya existe, saltando...\n",
      "\n",
      "Extracción de features completada.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuración base ---\n",
    "ROOT_DIR = \"data\"\n",
    "SAVE_DIR = os.path.join(ROOT_DIR, \"features\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Parámetros comunes ---\n",
    "mfcc = torchaudio.transforms.MFCC(\n",
    "    sample_rate=16000,\n",
    "    n_mfcc=13,\n",
    "    melkwargs={\"n_fft\": 320, \"hop_length\": 160, \"n_mels\": 23}\n",
    ")\n",
    "\n",
    "# --- Inicializar Wav2Vec2 una sola vez ---\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
    "\n",
    "# --- Configuración de modos y extractores ---\n",
    "modes = {\n",
    "    \"mfcc\": mfcc,\n",
    "    \"mfcc_delta\": mfcc,\n",
    "    \"mfcc_delta_delta\": mfcc,\n",
    "    \"wav2vec2\": wav2vec2,\n",
    "}\n",
    "\n",
    "# --- Procesar para train y val ---\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    list_path = os.path.join(ROOT_DIR, f\"{split}_list.txt\")\n",
    "\n",
    "    for mode, extractor in modes.items():\n",
    "        save_path = os.path.join(SAVE_DIR, f\"{split}_{mode}.pt\")\n",
    "\n",
    "        if os.path.isfile(save_path):\n",
    "            print(f\"{save_path} ya existe, saltando...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nExtrayendo {mode} para {split}...\")\n",
    "\n",
    "        dataset = CustomSpeechCommands(ROOT_DIR, list_path, mode=mode)\n",
    "        if mode == \"wav2vec2\":\n",
    "            dataset.save_features(\n",
    "                feature_extractor=extractor,\n",
    "                processor=processor,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "        else:\n",
    "            dataset.save_features(\n",
    "                feature_extractor=extractor,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "\n",
    "print(\"\\nExtracción de features completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac49807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.26G/2.26G [10:01<00:00, 4.03MB/s] \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'diff_features/train_list.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExtrayendo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m dataset = \u001b[43mCustomSpeechCommands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmfcc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mwav2vec2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     21\u001b[39m     dataset.save_features(\n\u001b[32m     22\u001b[39m         feature_extractor=extractor,\n\u001b[32m     23\u001b[39m         processor=processor,\n\u001b[32m     24\u001b[39m         device=device,\n\u001b[32m     25\u001b[39m         save_path=save_path,\n\u001b[32m     26\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mCustomSpeechCommands.__init__\u001b[39m\u001b[34m(self, root, files_list, download, target_len, mode, cnn_model)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.dataset = torchaudio.datasets.SPEECHCOMMANDS(root=root, download=download)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.indices = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mCustomSpeechCommands.splitter\u001b[39m\u001b[34m(self, files_list, root)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplitter\u001b[39m(\u001b[38;5;28mself\u001b[39m, files_list, root):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiles_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     16\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_paths = [line.strip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f.readlines()]\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mself\u001b[39m.all_paths = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'diff_features/train_list.txt'"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = os.path.join(ROOT_DIR, 'petes')\n",
    "# --- Procesar para train y val ---\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    list_path = os.path.join(ROOT_DIR, f\"{split}_list.txt\")\n",
    "        \n",
    "    for hl in [800]:\n",
    "        mode = torchaudio.transforms.MFCC(\n",
    "            sample_rate=16000,\n",
    "            n_mfcc=13,\n",
    "            log_mels = True,\n",
    "            melkwargs={\"n_fft\": 1600, \"hop_length\": hl, \"n_mels\": 23}\n",
    "        )\n",
    "        save_path = os.path.join(SAVE_DIR, f\"{split}_{hl}_mfcc.pt\")\n",
    "        if os.path.isfile(save_path):\n",
    "            print(f\"{save_path} ya existe, saltando...\")\n",
    "            continue\n",
    "        print(f\"\\nExtrayendo {mode} para {split}...\")\n",
    "\n",
    "        dataset = CustomSpeechCommands(ROOT_DIR, list_path, mode='mfcc')\n",
    "        if mode == \"wav2vec2\":\n",
    "            dataset.save_features(\n",
    "                feature_extractor=extractor,\n",
    "                processor=processor,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "        else:\n",
    "            dataset.save_features(\n",
    "                feature_extractor=mode,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "\n",
    "print(\"\\nExtracción de features completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab517a3b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e1f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=======================\n",
      "=== PROBANDO FEATURE: mfcc ===\n",
      "=======================\n",
      "\n",
      "Dataset cargado desde data/features/train_mfcc.pt\n",
      " - 32453 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/features/val_mfcc.pt\n",
      " - 3875 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/features/test_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Dimensiones detectadas para mfcc: seq_len=101, features=13\n",
      "\n",
      "======= Entrenando arquitectura GRU con mfcc =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_1.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_2.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_3.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_4.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_5.pt\n",
      "--- Validando clases para GRU usando mfcc ---\n",
      "\n",
      "======= Entrenando arquitectura LSTM con mfcc =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_1.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_2.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_3.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_4.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_5.pt\n",
      "--- Validando clases para LSTM usando mfcc ---\n",
      "\n",
      "======= Entrenando arquitectura RNN con mfcc =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_1.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_2.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_3.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_4.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para RNN usando mfcc ---\n",
      "\n",
      "======= Entrenando arquitectura CNN con mfcc =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_1.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_2.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_3.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_4.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_5.pt\n",
      "--- Validando clases para CNN usando mfcc ---\n",
      "\n",
      "======= Entrenando arquitectura TCNN con mfcc =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_1.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_2.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_3.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_4.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_5.pt\n",
      "--- Validando clases para TCNN usando mfcc ---\n",
      "\n",
      "======= Entrenando arquitectura Transformer con mfcc =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_1.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_2.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_3.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_4.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_5.pt\n",
      "--- Validando clases para Transformer usando mfcc ---\n",
      "\n",
      "\n",
      "=======================\n",
      "=== PROBANDO FEATURE: mfcc_delta ===\n",
      "=======================\n",
      "\n",
      "Dataset cargado desde data/features/train_mfcc_delta.pt\n",
      " - 32453 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/features/val_mfcc_delta.pt\n",
      " - 3875 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/features/test_mfcc_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Dimensiones detectadas para mfcc_delta: seq_len=101, features=26\n",
      "\n",
      "======= Entrenando arquitectura GRU con mfcc_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_delta_5.pt\n",
      "--- Validando clases para GRU usando mfcc_delta ---\n",
      "\n",
      "======= Entrenando arquitectura LSTM con mfcc_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_delta_5.pt\n",
      "--- Validando clases para LSTM usando mfcc_delta ---\n",
      "\n",
      "======= Entrenando arquitectura RNN con mfcc_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_delta_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para RNN usando mfcc_delta ---\n",
      "\n",
      "======= Entrenando arquitectura CNN con mfcc_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_delta_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para CNN usando mfcc_delta ---\n",
      "\n",
      "======= Entrenando arquitectura TCNN con mfcc_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_delta_5.pt\n",
      "--- Validando clases para TCNN usando mfcc_delta ---\n",
      "\n",
      "======= Entrenando arquitectura Transformer con mfcc_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_delta_5.pt\n",
      "--- Validando clases para Transformer usando mfcc_delta ---\n",
      "\n",
      "\n",
      "=======================\n",
      "=== PROBANDO FEATURE: mfcc_delta_delta ===\n",
      "=======================\n",
      "\n",
      "Dataset cargado desde data/features/train_mfcc_delta_delta.pt\n",
      " - 32453 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/features/val_mfcc_delta_delta.pt\n",
      " - 3875 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/features/test_mfcc_delta_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Dimensiones detectadas para mfcc_delta_delta: seq_len=101, features=39\n",
      "\n",
      "======= Entrenando arquitectura GRU con mfcc_delta_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_delta_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_delta_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_delta_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_delta_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_mfcc_delta_delta_5.pt\n",
      "--- Validando clases para GRU usando mfcc_delta_delta ---\n",
      "\n",
      "======= Entrenando arquitectura LSTM con mfcc_delta_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_delta_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_delta_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_delta_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_delta_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_mfcc_delta_delta_5.pt\n",
      "--- Validando clases para LSTM usando mfcc_delta_delta ---\n",
      "\n",
      "======= Entrenando arquitectura RNN con mfcc_delta_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_delta_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_delta_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_delta_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_delta_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_mfcc_delta_delta_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para RNN usando mfcc_delta_delta ---\n",
      "\n",
      "======= Entrenando arquitectura CNN con mfcc_delta_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_delta_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_delta_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_delta_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_delta_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_mfcc_delta_delta_5.pt\n",
      "--- Validando clases para CNN usando mfcc_delta_delta ---\n",
      "\n",
      "======= Entrenando arquitectura TCNN con mfcc_delta_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_delta_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_delta_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_delta_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_delta_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_mfcc_delta_delta_5.pt\n",
      "--- Validando clases para TCNN usando mfcc_delta_delta ---\n",
      "\n",
      "======= Entrenando arquitectura Transformer con mfcc_delta_delta =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_delta_delta_1.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_delta_delta_2.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_delta_delta_3.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_delta_delta_4.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_mfcc_delta_delta_5.pt\n",
      "--- Validando clases para Transformer usando mfcc_delta_delta ---\n",
      "\n",
      "\n",
      "=======================\n",
      "=== PROBANDO FEATURE: wav2vec2 ===\n",
      "=======================\n",
      "\n",
      "Dataset cargado desde data/features/train_wav2vec2.pt\n",
      " - 32453 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/features/val_wav2vec2.pt\n",
      " - 3875 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/features/test_wav2vec2.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Dimensiones detectadas para wav2vec2: seq_len=49, features=768\n",
      "\n",
      "======= Entrenando arquitectura GRU con wav2vec2 =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: GRU_wav2vec2_1.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_wav2vec2_2.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_wav2vec2_3.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_wav2vec2_4.pt\n",
      "[SKIP] Cargando pesos existentes: GRU_wav2vec2_5.pt\n",
      "--- Validando clases para GRU usando wav2vec2 ---\n",
      "\n",
      "======= Entrenando arquitectura LSTM con wav2vec2 =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: LSTM_wav2vec2_1.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_wav2vec2_2.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_wav2vec2_3.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_wav2vec2_4.pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_wav2vec2_5.pt\n",
      "--- Validando clases para LSTM usando wav2vec2 ---\n",
      "\n",
      "======= Entrenando arquitectura RNN con wav2vec2 =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: RNN_wav2vec2_1.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_wav2vec2_2.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_wav2vec2_3.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_wav2vec2_4.pt\n",
      "[SKIP] Cargando pesos existentes: RNN_wav2vec2_5.pt\n",
      "--- Validando clases para RNN usando wav2vec2 ---\n",
      "\n",
      "======= Entrenando arquitectura CNN con wav2vec2 =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: CNN_wav2vec2_1.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_wav2vec2_2.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_wav2vec2_3.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_wav2vec2_4.pt\n",
      "[SKIP] Cargando pesos existentes: CNN_wav2vec2_5.pt\n",
      "--- Validando clases para CNN usando wav2vec2 ---\n",
      "\n",
      "======= Entrenando arquitectura TCNN con wav2vec2 =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: TCNN_wav2vec2_1.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_wav2vec2_2.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_wav2vec2_3.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_wav2vec2_4.pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_wav2vec2_5.pt\n",
      "--- Validando clases para TCNN usando wav2vec2 ---\n",
      "\n",
      "======= Entrenando arquitectura Transformer con wav2vec2 =======\n",
      "\n",
      "[SKIP] Cargando pesos existentes: Transformer_wav2vec2_1.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_wav2vec2_2.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_wav2vec2_3.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_wav2vec2_4.pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_wav2vec2_5.pt\n",
      "--- Validando clases para Transformer usando wav2vec2 ---\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.path.join(\"data\",\"features\")\n",
    "SAVE_DIR = ROOT_DIR\n",
    "device = \"cuda\"\n",
    "MODEL_WEIGHT_PATH = 'diff_features'\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "tcn_config = {\n",
    "    'num_channels': [64, 64, 128, 128],\n",
    "    'kernel_size': 3,\n",
    "    'dilations': [1, 2, 4, 8],\n",
    "    'dropout': 0,\n",
    "    'causal': False,             # False es mejor para clasificación offline\n",
    "    'input_shape': 'NCL',        # Changed to NCL as input is permuted to (Batch, Features, Time)\n",
    "    'use_norm': 'batch_norm',            # Set to None as per instruction\n",
    "    'use_skip_connections': True # Keep skip connections enabled\n",
    "}\n",
    "# === FEATURES A USAR ===\n",
    "feature_sets = [\n",
    "    \"mfcc\",\n",
    "    \"mfcc_delta\",\n",
    "    \"mfcc_delta_delta\",\n",
    "    \"wav2vec2\"\n",
    "]\n",
    "\n",
    "architectures = [\"GRU\", \"LSTM\", \"RNN\", \"CNN\", \"TCNN\", \"Transformer\"]\n",
    "color_map = {\n",
    "    arch: default_colors[i % len(default_colors)]\n",
    "    for i, arch in enumerate(architectures)\n",
    "}\n",
    "\n",
    "n_trains = 5\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "lr = 5e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_gpu = True\n",
    "\n",
    "f1_scores = {arch: {} for arch in architectures}\n",
    "f1_stds   = {arch: {} for arch in architectures}\n",
    "\n",
    "for feat in feature_sets:\n",
    "    print(f\"\\n\\n=======================\")\n",
    "    print(f\"=== PROBANDO FEATURE: {feat} ===\")\n",
    "    print(f\"=======================\\n\")\n",
    "\n",
    "    # Rutas según feature\n",
    "    train_path = os.path.join(SAVE_DIR, f\"train_{feat}.pt\")\n",
    "    val_path   = os.path.join(SAVE_DIR, f\"val_{feat}.pt\")\n",
    "    test_path  = os.path.join(SAVE_DIR, f\"test_{feat}.pt\")\n",
    "\n",
    "    # Cargar datasets\n",
    "    train_dataset = FeaturesDataset(train_path)\n",
    "    val_dataset   = FeaturesDataset(val_path)\n",
    "    test_dataset  = FeaturesDataset(test_path)\n",
    "\n",
    "    # Determinar dims usando un batch\n",
    "    sample = train_dataset[0][0]\n",
    "    input_features = sample.shape[-1]  # canales reales del feature\n",
    "    seq_len_input = sample.shape[0]\n",
    "\n",
    "    print(f\"Dimensiones detectadas para {feat}: seq_len={seq_len_input}, features={input_features}\")\n",
    "\n",
    "    # Crear dataloader test\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    for arch in architectures:\n",
    "        print(f\"\\n======= Entrenando arquitectura {arch} con {feat} =======\\n\")\n",
    "\n",
    "        models = []\n",
    "        curves = []\n",
    "\n",
    "        for k in range(n_trains):\n",
    "\n",
    "            # -------------------------\n",
    "            # CREAR MODELO SEGÚN TIPO\n",
    "            # -------------------------\n",
    "            if arch in [\"GRU\", \"LSTM\", \"RNN\"]:\n",
    "                config = {\n",
    "                    'rnn_type': arch,\n",
    "                    'n_input_channels': input_features,\n",
    "                    'hidd_size': 256,\n",
    "                    'out_features': 35,\n",
    "                    'num_layers': 1\n",
    "                }\n",
    "                model = RNNModel(**config)\n",
    "\n",
    "            elif arch == \"CNN\":\n",
    "                config = {\n",
    "                    'n_input_channels': input_features,\n",
    "                    'hidd_size': 64,\n",
    "                    'out_features': 35\n",
    "                }\n",
    "                model = CNNModel(**config)\n",
    "\n",
    "            elif arch == \"TCNN\":\n",
    "                tcn_config['num_channels'][0] = 64  # opcional\n",
    "                config = {\n",
    "                    'num_inputs': input_features,\n",
    "                    'num_classes': 35,\n",
    "                    'tcn_params': tcn_config\n",
    "                }\n",
    "                model = SpeechCommandTCN(**config)\n",
    "\n",
    "            elif arch == \"Transformer\":\n",
    "                config = {\n",
    "                    'n_input_features': input_features,\n",
    "                    'n_output_classes': 35,\n",
    "                    'd_model': 128,\n",
    "                    'nhead': 8,\n",
    "                    'd_hid': 512,\n",
    "                    'n_layers': 4,\n",
    "                    'dropout': 0.0\n",
    "                }\n",
    "                model = TransformerModel(**config)\n",
    "\n",
    "            # -------------------------\n",
    "            # CARGA O ENTRENAMIENTO\n",
    "            # -------------------------\n",
    "            filename = f\"{arch}_{feat}_{k+1}.pt\"\n",
    "            filepath = os.path.join(MODEL_WEIGHT_PATH, arch, feat, filename)\n",
    "\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"[SKIP] Cargando pesos existentes: {filename}\")\n",
    "                model.load_state_dict(torch.load(filepath)[\"state_dict\"])\n",
    "\n",
    "            else:\n",
    "                print(f\"[TRAIN] Entrenando modelo {arch} ({k+1}/5) con {feat}\")\n",
    "                curve, _ = train_model(\n",
    "                    model,\n",
    "                    train_dataset,\n",
    "                    val_dataset,\n",
    "                    epochs,\n",
    "                    criterion,\n",
    "                    batch_size,\n",
    "                    lr,\n",
    "                    n_evaluations_per_epoch=3,\n",
    "                    use_gpu=use_gpu,\n",
    "                    patience=10,\n",
    "                    model_config=config,\n",
    "                    model_arch=arch\n",
    "                )\n",
    "                curves.append(curve)\n",
    "\n",
    "                os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "                save_model(model, filepath, config)\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        # -------------------------\n",
    "        # EVALUAR ESTA ARQUITECTURA\n",
    "        # -------------------------\n",
    "        metrics_mean, metrics_std, *_ = evaluate_models_metrics(\n",
    "            models, test_loader, criterion, use_gpu=use_gpu\n",
    "        )\n",
    "\n",
    "        f1_scores[arch][feat] = metrics_mean[\"f1\"]\n",
    "        f1_stds[arch][feat]   = metrics_std[\"f1\"]\n",
    "\n",
    "        # MATRIZ DE CONFUSIÓN\n",
    "        get_metrics_and_confusion_matrix(\n",
    "            models,\n",
    "            test_dataset,\n",
    "            name=f\"{arch} usando {feat}\",\n",
    "            filename=os.path.join(\"img\", arch, f\"conf_mat_{arch}_{feat}\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a560b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model for mfcc: TCNN -> f1 = 80.35% ± 0.67%\n",
      "Best model for mfcc_delta: GRU -> f1 = 84.00% ± 1.25%\n",
      "Best model for mfcc_delta_delta: TCNN -> f1 = 84.73% ± 1.27%\n",
      "Best model for wav2vec2: Transformer -> f1 = 88.98% ± 1.75%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for feat in feature_sets:\n",
    "    max_feat_perf = 0.0\n",
    "    best_arch = ''\n",
    "    for arch in architectures:\n",
    "        f1 = f1_scores[arch][feat]\n",
    "        if max_feat_perf < f1:\n",
    "            max_feat_perf = f1\n",
    "            best_arch = arch\n",
    "    print(f'Best model for {feat}: {best_arch} -> f1 = {max_feat_perf*100:.2f}% ± {f1_stds[best_arch][feat]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8443d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(feature_sets))\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "for arch in architectures:\n",
    "    # Obtener arrays de mean y std en el mismo orden de features\n",
    "    means = np.array([f1_scores[arch][feat] for feat in feature_sets])\n",
    "    stds  = np.array([f1_stds[arch][feat]   for feat in feature_sets])\n",
    "\n",
    "    plt.errorbar(x, means, yerr=stds,label=arch, marker='o', capsize=4, alpha=0.8, color = color_map[arch])\n",
    "\n",
    "FIG_DIR = 'img'\n",
    "plt.xticks(x, feature_sets, rotation=20)\n",
    "plt.xlabel(\"Feature usado\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"F1 vs Feature para cada arquitectura (con desviación estándar)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'f1_vs_feats.pdf'))\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a75fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(feature_sets))\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "for arch in architectures:\n",
    "    if arch == 'RNN':\n",
    "        continue\n",
    "    # Obtener arrays de mean y std en el mismo orden de features\n",
    "    means = np.array([f1_scores[arch][feat] for feat in feature_sets])\n",
    "    stds  = np.array([f1_stds[arch][feat]   for feat in feature_sets])\n",
    "\n",
    "    plt.errorbar(x, means, yerr=stds,label=arch, marker='o', capsize=4, alpha=0.8, color = color_map[arch])\n",
    "\n",
    "FIG_DIR = 'img'\n",
    "plt.xticks(x, feature_sets, rotation=20)\n",
    "plt.xlabel(\"Feature usado\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"F1 vs Feature para cada arquitectura (con desviación estándar)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'f1_vs_feats_skip_rnn.pdf'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo de secuencia de entrada: 51\n",
      "Dataset cargado desde data/petes/train_320_mfcc.pt\n",
      " - 32453 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/val_320_mfcc.pt\n",
      " - 3875 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/test_320_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "\n",
      "======= Entrenando modelos tipo GRU (Seq Len: 51) =======\n",
      "[SKIP] Cargando pesos existentes: GRU_1_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_2_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_3_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_4_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_5_train_seq_len_[51].pt\n",
      "--- Validando clases para GRU con largo de secuencia de 51 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo LSTM (Seq Len: 51) =======\n",
      "[SKIP] Cargando pesos existentes: LSTM_1_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_2_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_3_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_4_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_5_train_seq_len_[51].pt\n",
      "--- Validando clases para LSTM con largo de secuencia de 51 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo RNN (Seq Len: 51) =======\n",
      "[SKIP] Cargando pesos existentes: RNN_1_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_2_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_3_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_4_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_5_train_seq_len_[51].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para RNN con largo de secuencia de 51 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo CNN (Seq Len: 51) =======\n",
      "[SKIP] Cargando pesos existentes: CNN_1_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_2_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_3_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_4_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_5_train_seq_len_[51].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para CNN con largo de secuencia de 51 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo TCNN (Seq Len: 51) =======\n",
      "[SKIP] Cargando pesos existentes: TCNN_1_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_2_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_3_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_4_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_5_train_seq_len_[51].pt\n",
      "--- Validando clases para TCNN con largo de secuencia de 51 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo Transformer (Seq Len: 51) =======\n",
      "[SKIP] Cargando pesos existentes: Transformer_1_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_2_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_3_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_4_train_seq_len_[51].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_5_train_seq_len_[51].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para Transformer con largo de secuencia de 51 puntos ---\n",
      "Largo de secuencia de entrada: 101\n",
      "Dataset cargado desde data/petes/train_160_mfcc.pt\n",
      " - 32453 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/val_160_mfcc.pt\n",
      " - 3875 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/test_160_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "\n",
      "======= Entrenando modelos tipo GRU (Seq Len: 101) =======\n",
      "[SKIP] Cargando pesos existentes: GRU_1_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_2_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_3_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_4_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_5_train_seq_len_[101].pt\n",
      "--- Validando clases para GRU con largo de secuencia de 101 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo LSTM (Seq Len: 101) =======\n",
      "[SKIP] Cargando pesos existentes: LSTM_1_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_2_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_3_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_4_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_5_train_seq_len_[101].pt\n",
      "--- Validando clases para LSTM con largo de secuencia de 101 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo RNN (Seq Len: 101) =======\n",
      "[SKIP] Cargando pesos existentes: RNN_1_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_2_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_3_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_4_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_5_train_seq_len_[101].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para RNN con largo de secuencia de 101 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo CNN (Seq Len: 101) =======\n",
      "[SKIP] Cargando pesos existentes: CNN_1_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_2_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_3_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_4_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_5_train_seq_len_[101].pt\n",
      "--- Validando clases para CNN con largo de secuencia de 101 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo TCNN (Seq Len: 101) =======\n",
      "[SKIP] Cargando pesos existentes: TCNN_1_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_2_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_3_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_4_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_5_train_seq_len_[101].pt\n",
      "--- Validando clases para TCNN con largo de secuencia de 101 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo Transformer (Seq Len: 101) =======\n",
      "[SKIP] Cargando pesos existentes: Transformer_1_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_2_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_3_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_4_train_seq_len_[101].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_5_train_seq_len_[101].pt\n",
      "--- Validando clases para Transformer con largo de secuencia de 101 puntos ---\n",
      "Largo de secuencia de entrada: 297\n",
      "Dataset cargado desde data/petes/train_54_mfcc.pt\n",
      " - 32453 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/val_54_mfcc.pt\n",
      " - 3875 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/test_54_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "\n",
      "======= Entrenando modelos tipo GRU (Seq Len: 297) =======\n",
      "[SKIP] Cargando pesos existentes: GRU_1_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_2_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_3_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_4_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_5_train_seq_len_[297].pt\n",
      "--- Validando clases para GRU con largo de secuencia de 297 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo LSTM (Seq Len: 297) =======\n",
      "[SKIP] Cargando pesos existentes: LSTM_1_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_2_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_3_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_4_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_5_train_seq_len_[297].pt\n",
      "--- Validando clases para LSTM con largo de secuencia de 297 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo RNN (Seq Len: 297) =======\n",
      "[SKIP] Cargando pesos existentes: RNN_1_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_2_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_3_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_4_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_5_train_seq_len_[297].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para RNN con largo de secuencia de 297 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo CNN (Seq Len: 297) =======\n",
      "[SKIP] Cargando pesos existentes: CNN_1_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_2_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_3_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_4_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_5_train_seq_len_[297].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para CNN con largo de secuencia de 297 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo TCNN (Seq Len: 297) =======\n",
      "[SKIP] Cargando pesos existentes: TCNN_1_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_2_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_3_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_4_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_5_train_seq_len_[297].pt\n",
      "--- Validando clases para TCNN con largo de secuencia de 297 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo Transformer (Seq Len: 297) =======\n",
      "[SKIP] Cargando pesos existentes: Transformer_1_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_2_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_3_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_4_train_seq_len_[297].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_5_train_seq_len_[297].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para Transformer con largo de secuencia de 297 puntos ---\n",
      "Largo de secuencia de entrada: 501\n",
      "Dataset cargado desde data/petes/train_32_mfcc.pt\n",
      " - 32453 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/val_32_mfcc.pt\n",
      " - 3875 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/test_32_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "\n",
      "======= Entrenando modelos tipo GRU (Seq Len: 501) =======\n",
      "[SKIP] Cargando pesos existentes: GRU_1_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_2_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_3_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_4_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_5_train_seq_len_[501].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para GRU con largo de secuencia de 501 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo LSTM (Seq Len: 501) =======\n",
      "[SKIP] Cargando pesos existentes: LSTM_1_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_2_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_3_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_4_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_5_train_seq_len_[501].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para LSTM con largo de secuencia de 501 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo RNN (Seq Len: 501) =======\n",
      "[SKIP] Cargando pesos existentes: RNN_1_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_2_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_3_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_4_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_5_train_seq_len_[501].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para RNN con largo de secuencia de 501 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo CNN (Seq Len: 501) =======\n",
      "[SKIP] Cargando pesos existentes: CNN_1_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_2_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_3_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_4_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_5_train_seq_len_[501].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para CNN con largo de secuencia de 501 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo TCNN (Seq Len: 501) =======\n",
      "[SKIP] Cargando pesos existentes: TCNN_1_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_2_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_3_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_4_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_5_train_seq_len_[501].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para TCNN con largo de secuencia de 501 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo Transformer (Seq Len: 501) =======\n",
      "[SKIP] Cargando pesos existentes: Transformer_1_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_2_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_3_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_4_train_seq_len_[501].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_5_train_seq_len_[501].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para Transformer con largo de secuencia de 501 puntos ---\n",
      "Largo de secuencia de entrada: 1001\n",
      "Dataset cargado desde data/petes/train_16_mfcc.pt\n",
      " - 32453 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/val_16_mfcc.pt\n",
      " - 3875 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/test_16_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "\n",
      "======= Entrenando modelos tipo GRU (Seq Len: 1001) =======\n",
      "[SKIP] Cargando pesos existentes: GRU_1_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_2_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_3_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_4_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: GRU_5_train_seq_len_[1001].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para GRU con largo de secuencia de 1001 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo LSTM (Seq Len: 1001) =======\n",
      "[SKIP] Cargando pesos existentes: LSTM_1_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_2_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_3_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_4_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: LSTM_5_train_seq_len_[1001].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para LSTM con largo de secuencia de 1001 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo RNN (Seq Len: 1001) =======\n",
      "[SKIP] Cargando pesos existentes: RNN_1_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_2_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_3_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_4_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: RNN_5_train_seq_len_[1001].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para RNN con largo de secuencia de 1001 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo CNN (Seq Len: 1001) =======\n",
      "[SKIP] Cargando pesos existentes: CNN_1_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_2_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_3_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_4_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: CNN_5_train_seq_len_[1001].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para CNN con largo de secuencia de 1001 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo TCNN (Seq Len: 1001) =======\n",
      "[SKIP] Cargando pesos existentes: TCNN_1_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_2_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_3_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_4_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: TCNN_5_train_seq_len_[1001].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para TCNN con largo de secuencia de 1001 puntos ---\n",
      "\n",
      "======= Entrenando modelos tipo Transformer (Seq Len: 1001) =======\n",
      "[SKIP] Cargando pesos existentes: Transformer_1_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_2_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_3_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_4_train_seq_len_[1001].pt\n",
      "[SKIP] Cargando pesos existentes: Transformer_5_train_seq_len_[1001].pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando clases para Transformer con largo de secuencia de 1001 puntos ---\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.path.join(\"data\",\"petes\")\n",
    "SAVE_DIR = ROOT_DIR\n",
    "MODEL_WEIGHT_PATH = 'seq_length_model_weights'\n",
    "device = \"cuda\"\n",
    "\n",
    "tcn_config = {\n",
    "    'num_channels': [64, 64, 128, 128],\n",
    "    'kernel_size': 3,\n",
    "    'dilations': [1, 2, 4, 8],\n",
    "    'dropout': 0,\n",
    "    'causal': False,             # False es mejor para clasificación offline\n",
    "    'input_shape': 'NCL',        # Changed to NCL as input is permuted to (Batch, Features, Time)\n",
    "    'use_norm': 'batch_norm',            # Set to None as per instruction\n",
    "    'use_skip_connections': True # Keep skip connections enabled\n",
    "}\n",
    "\n",
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_trains = 5\n",
    "epochs = 30\n",
    "use_gpu = True\n",
    "architectures = [\"GRU\", \"LSTM\", \"RNN\", \"CNN\", \"TCNN\", \"Transformer\"]\n",
    "\n",
    "# Inicializar diccionarios para guardar los scores por arquitectura\n",
    "f1_scores_seq_len = {arch: [] for arch in architectures}\n",
    "f1_stds_seq_len = {arch: [] for arch in architectures}\n",
    "\n",
    "hop_lengths = [800, 320, 160, 54, 32, 16]\n",
    "seq_len_seen = []\n",
    "\n",
    "# --- Inicio del Loop Principal ---\n",
    "for hop_length in hop_lengths:\n",
    "    seq_len_input = 1 + 16000 // hop_length\n",
    "    seq_len_seen.append(seq_len_input)\n",
    "    print(f\"Largo de secuencia de entrada: {seq_len_input}\")\n",
    "\n",
    "    # Cargar datasets (Asegúrate que estos archivos existan para cada hop_length)\n",
    "    train_dataset = FeaturesDataset(os.path.join(SAVE_DIR, f\"train_{hop_length}_mfcc.pt\"))\n",
    "    val_dataset   = FeaturesDataset(os.path.join(SAVE_DIR, f\"val_{hop_length}_mfcc.pt\"))\n",
    "    test_dataset  = FeaturesDataset(os.path.join(SAVE_DIR, f\"test_{hop_length}_mfcc.pt\"))\n",
    "\n",
    "    for arch in architectures:\n",
    "        print(f\"\\n======= Entrenando modelos tipo {arch} (Seq Len: {seq_len_input}) =======\")\n",
    "\n",
    "        models = []\n",
    "        curves = []\n",
    "\n",
    "        for k in range(n_trains):\n",
    "            # ... (Tu lógica de creación de modelo RNN, CNN, TCN, Transformer intacta) ...\n",
    "            # ... (Simplemente asegúrate de que 'model' esté definido aquí) ...j\n",
    "            # Ejemplo simplificado de tu bloque de creación:\n",
    "            if arch in [\"GRU\", \"LSTM\", \"RNN\"]:\n",
    "                config = {'rnn_type': arch, 'n_input_channels': 13, 'hidd_size': 256, 'out_features': 35, 'num_layers': 1}\n",
    "                model = RNNModel(**config)\n",
    "            elif arch == \"CNN\":\n",
    "                config = {'n_input_channels': 13, 'hidd_size': 64, 'out_features': 35}\n",
    "                model = CNNModel(**config)\n",
    "            elif arch == \"TCNN\":\n",
    "                config = {'num_inputs': 13, 'num_classes': 35, 'tcn_params': tcn_config}\n",
    "                model = SpeechCommandTCN(**config)\n",
    "            elif arch == 'Transformer':\n",
    "                config = {'n_input_features': 13, 'n_output_classes': 35, 'd_model': 128, 'nhead': 8, 'd_hid': 512, 'n_layers': 4, 'dropout': 0.0}\n",
    "                model = TransformerModel(**config)\n",
    "            # 2. DETERMINAR RUTA DEL ARCHIVO\n",
    "            # Usamos k+1 para que coincida con tus archivos (1, 2, 3, 4, 5)\n",
    "            filename = f'{arch}_{k+1}_train_seq_len_[{seq_len_input}].pt'\n",
    "            filepath = os.path.join(MODEL_WEIGHT_PATH, arch, filename)\n",
    "\n",
    "            # 3. LÓGICA DE CARGA INTELIGENTE\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"[SKIP] Cargando pesos existentes: {filename}\")\n",
    "                # Cargar pesos en el modelo inicializado\n",
    "                model.load_state_dict(torch.load(filepath)['state_dict'])\n",
    "                \n",
    "                # Si tienes guardadas las curvas en otro lado, podrías intentar cargarlas, \n",
    "                # pero si no, 'curves' quedará vacío para este modelo (no afecta la evaluación final de F1).\n",
    "            \n",
    "            else:\n",
    "                print(f\"   [TRAIN] Archivo no encontrado, entrenando: {filename}\")\n",
    "                \n",
    "                # Entrenamiento\n",
    "                curve, _ = train_model(\n",
    "                    model, train_dataset, val_dataset, epochs, criterion, batch_size, lr,\n",
    "                    n_evaluations_per_epoch=3, use_gpu=use_gpu, patience=10,\n",
    "                    model_config=config, model_arch=arch\n",
    "                )\n",
    "                curves.append(curve)\n",
    "                \n",
    "                # Guardar\n",
    "                os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "                save_model(model, filepath, config)\n",
    "\n",
    "                # Entrenamiento\n",
    "                curve, _ = train_model(\n",
    "                    model, train_dataset, val_dataset, epochs, criterion, batch_size, lr,\n",
    "                    n_evaluations_per_epoch=3, use_gpu=use_gpu, patience=10,\n",
    "                    model_config=config, model_arch=arch\n",
    "                )\n",
    "                curves.append(curve)\n",
    "                models.append(model)\n",
    "                save_model(\n",
    "                    model, \n",
    "                    os.path.join(\n",
    "                        MODEL_WEIGHT_PATH,\n",
    "                        f'{arch}',\n",
    "                        f'{arch}_{k+1}_train_seq_len_[{seq_len_input}].pt'\n",
    "                    ), \n",
    "                    config\n",
    "                )\n",
    "                # Mostrar curvas y matrices\n",
    "                show_curves(\n",
    "                    curves,\n",
    "                    suptitle=f\"{arch} con largo de secuencia de {seq_len_input} puntos\",\n",
    "                    filename=os.path.join(\n",
    "                        f'{arch}',\n",
    "                        f\"{arch}_seq_len{seq_len_input}\"\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "            models.append(model)\n",
    "        # Evaluar\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        metrics_mean, metrics_std, *_ = evaluate_models_metrics(models, test_loader, criterion, use_gpu=use_gpu)\n",
    "\n",
    "        # 2. GUARDAR MÉTRICAS EN EL DICCIONARIO\n",
    "        f1_scores_seq_len[arch].append(metrics_mean[\"f1\"])\n",
    "        f1_stds_seq_len[arch].append(metrics_std[\"f1\"])\n",
    "\n",
    "        get_metrics_and_confusion_matrix(\n",
    "            models,\n",
    "            test_dataset,\n",
    "            name=f\"{arch} con largo de secuencia de {seq_len_input} puntos\", \n",
    "            filename=os.path.join(\n",
    "                'img',\n",
    "                f'{arch}',\n",
    "                f\"conf_mat_{arch}_seq_len{seq_len_input}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "# --- Gráfico Final Corregido ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterar sobre cada arquitectura para dibujar su propia línea\n",
    "for arch in architectures:\n",
    "    plt.errorbar(\n",
    "        seq_len_seen, \n",
    "        f1_scores_seq_len[arch], \n",
    "        yerr=f1_stds_seq_len[arch], \n",
    "        marker=\"o\", \n",
    "        capsize=4, \n",
    "        label=arch,\n",
    "        alpha=0.8,\n",
    "        color = color_map[arch]\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Largo de secuencia de entrada (frames)\")\n",
    "plt.ylabel(\"F1-score promedio (± std)\")\n",
    "plt.title(\"Performance vs Largo de Secuencia por Arquitectura\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left') # Leyenda fuera del gráfico si hay muchas\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/f1_vs_seq_len_comparison.pdf\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c69634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GRU': [np.float64(0.011465190273256433), np.float64(0.01528085405203892), np.float64(0.0067723037122696846), np.float64(0.19573372853245366), np.float64(0.20072208756491683)], 'LSTM': [np.float64(0.009267545769890995), np.float64(0.012373248803635528), np.float64(0.011279809534352755), np.float64(0.24339189809734765), np.float64(0.26785214340432895)], 'RNN': [np.float64(0.009446390886852967), np.float64(0.011603061399435276), np.float64(0.005941037834251864), np.float64(0.0075785629906105914), np.float64(0.007512917288884602)], 'CNN': [np.float64(0.022793324136861023), np.float64(0.026086287140706294), np.float64(0.01464845924246241), np.float64(0.19656068181614586), np.float64(0.1809510818903086)], 'TCNN': [np.float64(0.019990001918966528), np.float64(0.030090794592362524), np.float64(0.024558128785449156), np.float64(0.20254890976924586), np.float64(0.20998706058146685)], 'Transformer': [np.float64(0.03165541732122716), np.float64(0.010965915014062926), np.float64(0.09682647787620985), np.float64(0.09406931094460634), np.float64(0.061522809895141084)]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Gráfico Final Corregido ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterar sobre cada arquitectura para dibujar su propia línea\n",
    "for arch in architectures:\n",
    "    plt.errorbar(\n",
    "        seq_len_seen, \n",
    "        f1_scores_seq_len[arch], \n",
    "        yerr=f1_stds_seq_len[arch], \n",
    "        marker=\"o\", \n",
    "        capsize=4, \n",
    "        label=arch,\n",
    "        alpha=0.8,\n",
    "        color = color_map[arch]\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Largo de secuencia de entrada (frames)\")\n",
    "plt.ylabel(\"F1-score promedio (± std)\")\n",
    "plt.title(\"Performance vs Largo de Secuencia por Arquitectura\")\n",
    "plt.legend(loc='best') # Leyenda fuera del gráfico si hay muchas\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/f1_vs_seq_len_comparison.pdf\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# --- Gráfico Final Corregido ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterar sobre cada arquitectura para dibujar su propia línea\n",
    "for arch in architectures:\n",
    "    if arch == 'RNN':\n",
    "        continue\n",
    "    plt.errorbar(\n",
    "        seq_len_seen, \n",
    "        f1_scores_seq_len[arch], \n",
    "        yerr=f1_stds_seq_len[arch], \n",
    "        marker=\"o\", \n",
    "        capsize=4, \n",
    "        label=arch,\n",
    "        alpha=0.8,\n",
    "        color = color_map[arch]\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Largo de secuencia de entrada (frames)\")\n",
    "plt.ylabel(\"F1-score promedio (± std)\")\n",
    "plt.title(\"Performance vs Largo de Secuencia por Arquitectura\")\n",
    "plt.legend(loc='best') # Leyenda fuera del gráfico si hay muchas\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/f1_vs_seq_len_comparison_rnn_skip.pdf\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# --- Gráfico Final Corregido ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterar sobre cada arquitectura para dibujar su propia línea\n",
    "for arch in architectures:\n",
    "    if arch != 'RNN':\n",
    "        continue\n",
    "    plt.errorbar(\n",
    "        seq_len_seen, \n",
    "        f1_scores_seq_len[arch], \n",
    "        yerr=f1_stds_seq_len[arch], \n",
    "        marker=\"o\", \n",
    "        capsize=4, \n",
    "        label=arch,\n",
    "        alpha=0.8,\n",
    "        color = color_map[arch]\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Largo de secuencia de entrada (frames)\")\n",
    "plt.ylabel(\"F1-score promedio (± std)\")\n",
    "plt.title(\"Performance vs Largo de Secuencia por Arquitectura\")\n",
    "plt.legend(loc='best') # Leyenda fuera del gráfico si hay muchas\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/f1_vs_seq_len_comparison_rnn_only.pdf\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f1_stds_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43c999ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqLen 0: GRU -> f1 = 81.38% ± 1.15%\n",
      "SeqLen 1: GRU -> f1 = 81.46% ± 1.53%\n",
      "SeqLen 2: GRU -> f1 = 81.15% ± 0.68%\n",
      "SeqLen 3: Transformer -> f1 = 72.01% ± 9.41%\n",
      "SeqLen 4: GRU -> f1 = 70.61% ± 20.07%\n"
     ]
    }
   ],
   "source": [
    "num_seq_lens = len(next(iter(f1_scores_seq_len.values())))\n",
    "\n",
    "for i in range(num_seq_lens):\n",
    "    max_f1 = -1\n",
    "    best_arch = None\n",
    "\n",
    "    for arch in architectures:\n",
    "        f1_val = f1_scores_seq_len[arch][i]       # f1 de esta arch para este seq_len\n",
    "        if f1_val > max_f1:\n",
    "            max_f1 = f1_val\n",
    "            best_arch = arch\n",
    "\n",
    "    std_val = f1_stds_seq_len[best_arch][i]\n",
    "\n",
    "    print(f'SeqLen {i}: {best_arch} -> f1 = {max_f1*100:.2f}% ± {std_val*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ab906b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Seq Len    |       GRU        |       LSTM       |       RNN        |       CNN        |       TCNN       |   Transformer   \n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "51         |   81.38±1.15*    |   79.96±0.93     |    2.20±0.94     |   73.21±2.28     |   80.19±2.00     |   73.45±3.17     | \n",
      "101        |   81.46±1.53*    |   80.10±1.24     |    2.86±1.16     |   70.93±2.61     |   81.35±3.01     |   74.99±1.10     | \n",
      "297        |   81.15±0.68*    |   79.07±1.13     |    2.41±0.59     |   63.45±1.46     |   79.64±2.46     |   71.39±9.68     | \n",
      "501        |   71.41±19.57    |   64.94±24.34    |    2.00±0.76     |   56.85±19.66    |   69.47±20.25    |   72.01±9.41*    | \n",
      "1001       |   70.61±20.07*   |   61.14±26.79    |    1.92±0.75     |   51.85±18.10    |   63.40±21.00    |   70.38±6.15     | \n"
     ]
    }
   ],
   "source": [
    "# Encabezado\n",
    "header = f\"{'Seq Len':<10} | \" + \" | \".join([f\"{arch:^16}\" for arch in architectures])\n",
    "print(\"-\" * len(header))\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for i in range(num_seq_lens):\n",
    "    current_seq_len = seq_len_seen[i] if 'seq_len_seen' in locals() else i\n",
    "    \n",
    "    # Calcular mejor score para marcarlo\n",
    "    scores_at_i = [f1_scores_seq_len[arch][i] for arch in architectures]\n",
    "    max_score = max(scores_at_i)\n",
    "    \n",
    "    row_str = f\"{str(current_seq_len):<10} | \"\n",
    "    \n",
    "    for arch in architectures:\n",
    "        mean = f1_scores_seq_len[arch][i] * 100\n",
    "        std = f1_stds_seq_len[arch][i] * 100\n",
    "        \n",
    "        # Si es el mejor, lo ponemos en negrita (código ANSI) o con un *\n",
    "        is_best = (f1_scores_seq_len[arch][i] == max_score)\n",
    "        marker = \"*\" if is_best else \" \"\n",
    "        \n",
    "        val_str = f\"{mean:.2f}±{std:.2f}{marker}\"\n",
    "        row_str += f\"{val_str:^16} | \"\n",
    "        \n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87130ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_DIR = os.path.join(\"data\",\"features\")\n",
    "# SAVE_DIR = ROOT_DIR\n",
    "# train_dataset = FeaturesDataset(os.path.join(SAVE_DIR, \"train_wav2vec2.pt\"))\n",
    "# val_dataset   = FeaturesDataset(os.path.join(SAVE_DIR, \"val_wav2vec2.pt\"))\n",
    "# test_dataset  = FeaturesDataset(os.path.join(SAVE_DIR, \"test_wav2vec2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6895b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "def infer_model_type(path):\n",
    "    name = path.lower()\n",
    "    if \"gru\" in name:\n",
    "        return \"GRU\"\n",
    "    if \"lstm\" in name:\n",
    "        return \"LSTM\"\n",
    "    if \"rnn\" in name and \"cnn\" not in name:\n",
    "        return \"RNN\"\n",
    "    if \"tcnn\" in name:\n",
    "        return \"TCNN\"\n",
    "    if \"transformer\" in name:\n",
    "        return \"TRANSFORMER\"\n",
    "    raise ValueError(f\"No pude inferir tipo de modelo desde: {path}\")\n",
    "\n",
    "MODEL_CLASS_BY_TYPE = {\n",
    "    \"GRU\": RNNModel,\n",
    "    \"LSTM\": RNNModel,\n",
    "    \"RNN\": RNNModel,\n",
    "    \"TCNN\": CNNModel,\n",
    "    \"TRANSFORMER\": TransformerModel,\n",
    "}\n",
    "\n",
    "def load_model_by_type(model_path, device=\"cuda\", config_override=None):\n",
    "    \"\"\"\n",
    "    Carga un modelo según su tipo (GRU, LSTM, RNN, TCNN, TRANSFORMER)\n",
    "    y permite pasar un config explícito.\n",
    "    \"\"\"\n",
    "    model_type = infer_model_type(model_path)\n",
    "\n",
    "    # si se entrega config explícito → úsalo\n",
    "    if config_override is not None:\n",
    "        return load_trained_model(\n",
    "            MODEL_CLASS_BY_TYPE[model_type],\n",
    "            model_path,\n",
    "            device=device,\n",
    "            config=config_override\n",
    "        )\n",
    "\n",
    "    # configs por defecto según tipo\n",
    "    if model_type in [\"GRU\", \"LSTM\", \"RNN\"]:\n",
    "        cfg = {\n",
    "            'rnn_type': model_type,\n",
    "            'n_input_channels': 768,\n",
    "            'hidd_size': 256,\n",
    "            'out_features': 35,\n",
    "            'num_layers': 1\n",
    "        }\n",
    "        return load_trained_model(RNNModel, model_path, device=device, config=cfg)\n",
    "\n",
    "    elif model_type == \"TCNN\":\n",
    "        cfg = {\n",
    "            'num_inputs': 768,\n",
    "            'out_features': 35\n",
    "        }\n",
    "        return load_trained_model(SpeechCommandTCN, model_path, device=device, config=cfg)\n",
    "\n",
    "    elif model_type == \"TRANSFORMER\":\n",
    "        return load_trained_model(TransformerModel, model_path, device=device)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(model_type)\n",
    "\n",
    "model_paths = sorted(glob.glob(\"old_model_weights/*.pt\"))\n",
    "\n",
    "# Skippear transformers antes de procesar\n",
    "model_paths = [p for p in model_paths if infer_model_type(p) != \"TRANSFORMER\"]\n",
    "\n",
    "# Diccionario para guardar modelos por arquitectura\n",
    "models_by_arch = {}\n",
    "paths_by_arch = {}\n",
    "\n",
    "for path in model_paths:\n",
    "    arch = infer_model_type(path)\n",
    "\n",
    "    model = load_model_by_type(path, device=\"cuda\")\n",
    "    models_by_arch.setdefault(arch, []).append(model)\n",
    "    paths_by_arch.setdefault(arch, []).append(path)\n",
    "\n",
    "print(\"Modelos cargados por arquitectura:\")\n",
    "for arch, lst in models_by_arch.items():\n",
    "    print(f\"{arch}: {len(lst)} modelos\")\n",
    "\n",
    "results_by_arch = {}\n",
    "times_by_arch = {}\n",
    "f1_dists_by_arch = {}\n",
    "\n",
    "\n",
    "# batch_size = 32\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# for arch, model_list in models_by_arch.items():\n",
    "#     print(f\"\\nEvaluando arquitectura: {arch}\")\n",
    "\n",
    "#     metrics_mean, metrics_std, all_metrics, mean_times, f1_dists = \\\n",
    "#         evaluate_models_metrics(model_list, test_loader, criterion)\n",
    "\n",
    "    \n",
    "\n",
    "#     results_by_arch[arch] = {\n",
    "#         \"mean\": metrics_mean,\n",
    "#         \"std\": metrics_std,\n",
    "#         \"all\": all_metrics,\n",
    "#     }\n",
    "#     print(results_by_arch[arch])\n",
    "\n",
    "#     # Guardamos tiempos y distribuciones f1\n",
    "#     times_by_arch[arch] = mean_times\n",
    "#     f1_dists_by_arch[arch] = f1_dists\n",
    "\n",
    "# # ============================================================\n",
    "# # 1) Calcular métricas agregadas por arquitectura\n",
    "# # ============================================================\n",
    "\n",
    "os.makedirs(\"img\", exist_ok=True)\n",
    "\n",
    "for arch, model_list in models_by_arch.items():\n",
    "    print(f\"\\n=== Arquitectura: {arch} ===\")\n",
    "\n",
    "    # Path base para guardar el PDF\n",
    "    outfile = os.path.join(\"img\", f\"conf_mat_{arch}.pdf\")\n",
    "\n",
    "    # Llamar directamente a tu función tal como existe\n",
    "    metrics = get_metrics_and_confusion_matrix(\n",
    "        model_list,\n",
    "        test_dataset,\n",
    "        name=outfile\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# arch_names = []\n",
    "# arch_f1_values = []\n",
    "# arch_f1_dists = []\n",
    "# arch_time_mean = []\n",
    "# arch_time_std = []\n",
    "\n",
    "# for arch in models_by_arch.keys():\n",
    "#     arch_names.append(arch)\n",
    "\n",
    "#     # Distribuciones F1 (1 valor por modelo)\n",
    "#     f1_vals = results_by_arch[arch][\"all\"][\"f1\"]\n",
    "#     arch_f1_values.append(np.mean(f1_vals))\n",
    "#     arch_f1_dists.append(f1_vals)\n",
    "\n",
    "#     tvals = times_by_arch[arch]     \n",
    "#     arch_time_mean.append(np.mean(tvals))\n",
    "#     arch_time_std.append(np.std(tvals))\n",
    "\n",
    "\n",
    "# arch_time_mean_ms = [t * 100000 for t in arch_time_mean]\n",
    "# arch_time_std_ms  = [s * 100000 for s in arch_time_std]\n",
    "\n",
    "# plot_f1_vs_inference_time_with_error_bars(\n",
    "#     arch_names,\n",
    "#     arch_time_mean_ms,\n",
    "#     arch_f1_dists,\n",
    "#     time_stds=arch_time_std_ms\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94fea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitecturas encontradas: ['LSTM', 'RNN', 'GRU', 'Transformer', 'TCNN', 'CNN']\n",
      "\n",
      ">>> Evaluando: LSTM | Feature: wav2vec2 | Input Dim: 768\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_wav2vec2.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/LSTM/wav2vec2/LSTM_wav2vec2_1.pt\n",
      "Modelo cargado desde diff_features/LSTM/wav2vec2/LSTM_wav2vec2_2.pt\n",
      "Modelo cargado desde diff_features/LSTM/wav2vec2/LSTM_wav2vec2_3.pt\n",
      "Modelo cargado desde diff_features/LSTM/wav2vec2/LSTM_wav2vec2_4.pt\n",
      "Modelo cargado desde diff_features/LSTM/wav2vec2/LSTM_wav2vec2_5.pt\n",
      "\n",
      ">>> Evaluando: LSTM | Feature: mfcc_delta | Input Dim: 26\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/LSTM/mfcc_delta/LSTM_mfcc_delta_1.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc_delta/LSTM_mfcc_delta_2.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc_delta/LSTM_mfcc_delta_3.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc_delta/LSTM_mfcc_delta_4.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc_delta/LSTM_mfcc_delta_5.pt\n",
      "\n",
      ">>> Evaluando: LSTM | Feature: mfcc | Input Dim: 13\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/LSTM/mfcc/LSTM_mfcc_1.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc/LSTM_mfcc_2.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc/LSTM_mfcc_3.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc/LSTM_mfcc_4.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc/LSTM_mfcc_5.pt\n",
      "\n",
      ">>> Evaluando: LSTM | Feature: mfcc_delta_delta | Input Dim: 39\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/LSTM/mfcc_delta_delta/LSTM_mfcc_delta_delta_1.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc_delta_delta/LSTM_mfcc_delta_delta_2.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc_delta_delta/LSTM_mfcc_delta_delta_3.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc_delta_delta/LSTM_mfcc_delta_delta_4.pt\n",
      "Modelo cargado desde diff_features/LSTM/mfcc_delta_delta/LSTM_mfcc_delta_delta_5.pt\n",
      "\n",
      ">>> Evaluando: RNN | Feature: wav2vec2 | Input Dim: 768\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_wav2vec2.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/RNN/wav2vec2/RNN_wav2vec2_1.pt\n",
      "Modelo cargado desde diff_features/RNN/wav2vec2/RNN_wav2vec2_2.pt\n",
      "Modelo cargado desde diff_features/RNN/wav2vec2/RNN_wav2vec2_3.pt\n",
      "Modelo cargado desde diff_features/RNN/wav2vec2/RNN_wav2vec2_4.pt\n",
      "Modelo cargado desde diff_features/RNN/wav2vec2/RNN_wav2vec2_5.pt\n",
      "\n",
      ">>> Evaluando: RNN | Feature: mfcc_delta | Input Dim: 26\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/RNN/mfcc_delta/RNN_mfcc_delta_1.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc_delta/RNN_mfcc_delta_2.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc_delta/RNN_mfcc_delta_3.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc_delta/RNN_mfcc_delta_4.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc_delta/RNN_mfcc_delta_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Evaluando: RNN | Feature: mfcc | Input Dim: 13\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/RNN/mfcc/RNN_mfcc_1.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc/RNN_mfcc_2.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc/RNN_mfcc_3.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc/RNN_mfcc_4.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc/RNN_mfcc_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Evaluando: RNN | Feature: mfcc_delta_delta | Input Dim: 39\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/RNN/mfcc_delta_delta/RNN_mfcc_delta_delta_1.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc_delta_delta/RNN_mfcc_delta_delta_2.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc_delta_delta/RNN_mfcc_delta_delta_3.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc_delta_delta/RNN_mfcc_delta_delta_4.pt\n",
      "Modelo cargado desde diff_features/RNN/mfcc_delta_delta/RNN_mfcc_delta_delta_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Evaluando: GRU | Feature: wav2vec2 | Input Dim: 768\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_wav2vec2.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/GRU/wav2vec2/GRU_wav2vec2_1.pt\n",
      "Modelo cargado desde diff_features/GRU/wav2vec2/GRU_wav2vec2_2.pt\n",
      "Modelo cargado desde diff_features/GRU/wav2vec2/GRU_wav2vec2_3.pt\n",
      "Modelo cargado desde diff_features/GRU/wav2vec2/GRU_wav2vec2_4.pt\n",
      "Modelo cargado desde diff_features/GRU/wav2vec2/GRU_wav2vec2_5.pt\n",
      "\n",
      ">>> Evaluando: GRU | Feature: mfcc_delta | Input Dim: 26\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/GRU/mfcc_delta/GRU_mfcc_delta_1.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc_delta/GRU_mfcc_delta_2.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc_delta/GRU_mfcc_delta_3.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc_delta/GRU_mfcc_delta_4.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc_delta/GRU_mfcc_delta_5.pt\n",
      "\n",
      ">>> Evaluando: GRU | Feature: mfcc | Input Dim: 13\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/GRU/mfcc/GRU_mfcc_1.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc/GRU_mfcc_2.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc/GRU_mfcc_3.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc/GRU_mfcc_4.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc/GRU_mfcc_5.pt\n",
      "\n",
      ">>> Evaluando: GRU | Feature: mfcc_delta_delta | Input Dim: 39\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/GRU/mfcc_delta_delta/GRU_mfcc_delta_delta_1.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc_delta_delta/GRU_mfcc_delta_delta_2.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc_delta_delta/GRU_mfcc_delta_delta_3.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc_delta_delta/GRU_mfcc_delta_delta_4.pt\n",
      "Modelo cargado desde diff_features/GRU/mfcc_delta_delta/GRU_mfcc_delta_delta_5.pt\n",
      "\n",
      ">>> Evaluando: Transformer | Feature: wav2vec2 | Input Dim: 768\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_wav2vec2.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/Transformer/wav2vec2/Transformer_wav2vec2_1.pt\n",
      "Modelo cargado desde diff_features/Transformer/wav2vec2/Transformer_wav2vec2_2.pt\n",
      "Modelo cargado desde diff_features/Transformer/wav2vec2/Transformer_wav2vec2_3.pt\n",
      "Modelo cargado desde diff_features/Transformer/wav2vec2/Transformer_wav2vec2_4.pt\n",
      "Modelo cargado desde diff_features/Transformer/wav2vec2/Transformer_wav2vec2_5.pt\n",
      "\n",
      ">>> Evaluando: Transformer | Feature: mfcc_delta | Input Dim: 26\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/Transformer/mfcc_delta/Transformer_mfcc_delta_1.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc_delta/Transformer_mfcc_delta_2.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc_delta/Transformer_mfcc_delta_3.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc_delta/Transformer_mfcc_delta_4.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc_delta/Transformer_mfcc_delta_5.pt\n",
      "\n",
      ">>> Evaluando: Transformer | Feature: mfcc | Input Dim: 13\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/Transformer/mfcc/Transformer_mfcc_1.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc/Transformer_mfcc_2.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc/Transformer_mfcc_3.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc/Transformer_mfcc_4.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc/Transformer_mfcc_5.pt\n",
      "\n",
      ">>> Evaluando: Transformer | Feature: mfcc_delta_delta | Input Dim: 39\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/Transformer/mfcc_delta_delta/Transformer_mfcc_delta_delta_1.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc_delta_delta/Transformer_mfcc_delta_delta_2.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc_delta_delta/Transformer_mfcc_delta_delta_3.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc_delta_delta/Transformer_mfcc_delta_delta_4.pt\n",
      "Modelo cargado desde diff_features/Transformer/mfcc_delta_delta/Transformer_mfcc_delta_delta_5.pt\n",
      "\n",
      ">>> Evaluando: TCNN | Feature: wav2vec2 | Input Dim: 768\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_wav2vec2.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/TCNN/wav2vec2/TCNN_wav2vec2_1.pt\n",
      "Modelo cargado desde diff_features/TCNN/wav2vec2/TCNN_wav2vec2_2.pt\n",
      "Modelo cargado desde diff_features/TCNN/wav2vec2/TCNN_wav2vec2_3.pt\n",
      "Modelo cargado desde diff_features/TCNN/wav2vec2/TCNN_wav2vec2_4.pt\n",
      "Modelo cargado desde diff_features/TCNN/wav2vec2/TCNN_wav2vec2_5.pt\n",
      "\n",
      ">>> Evaluando: TCNN | Feature: mfcc_delta | Input Dim: 26\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/TCNN/mfcc_delta/TCNN_mfcc_delta_1.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc_delta/TCNN_mfcc_delta_2.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc_delta/TCNN_mfcc_delta_3.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc_delta/TCNN_mfcc_delta_4.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc_delta/TCNN_mfcc_delta_5.pt\n",
      "\n",
      ">>> Evaluando: TCNN | Feature: mfcc | Input Dim: 13\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/TCNN/mfcc/TCNN_mfcc_1.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc/TCNN_mfcc_2.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc/TCNN_mfcc_3.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc/TCNN_mfcc_4.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc/TCNN_mfcc_5.pt\n",
      "\n",
      ">>> Evaluando: TCNN | Feature: mfcc_delta_delta | Input Dim: 39\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/TCNN/mfcc_delta_delta/TCNN_mfcc_delta_delta_1.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc_delta_delta/TCNN_mfcc_delta_delta_2.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc_delta_delta/TCNN_mfcc_delta_delta_3.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc_delta_delta/TCNN_mfcc_delta_delta_4.pt\n",
      "Modelo cargado desde diff_features/TCNN/mfcc_delta_delta/TCNN_mfcc_delta_delta_5.pt\n",
      "\n",
      ">>> Evaluando: CNN | Feature: wav2vec2 | Input Dim: 768\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_wav2vec2.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/CNN/wav2vec2/CNN_wav2vec2_1.pt\n",
      "Modelo cargado desde diff_features/CNN/wav2vec2/CNN_wav2vec2_2.pt\n",
      "Modelo cargado desde diff_features/CNN/wav2vec2/CNN_wav2vec2_3.pt\n",
      "Modelo cargado desde diff_features/CNN/wav2vec2/CNN_wav2vec2_4.pt\n",
      "Modelo cargado desde diff_features/CNN/wav2vec2/CNN_wav2vec2_5.pt\n",
      "\n",
      ">>> Evaluando: CNN | Feature: mfcc_delta | Input Dim: 26\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/CNN/mfcc_delta/CNN_mfcc_delta_1.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc_delta/CNN_mfcc_delta_2.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc_delta/CNN_mfcc_delta_3.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc_delta/CNN_mfcc_delta_4.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc_delta/CNN_mfcc_delta_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Evaluando: CNN | Feature: mfcc | Input Dim: 13\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/CNN/mfcc/CNN_mfcc_1.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc/CNN_mfcc_2.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc/CNN_mfcc_3.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc/CNN_mfcc_4.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc/CNN_mfcc_5.pt\n",
      "\n",
      ">>> Evaluando: CNN | Feature: mfcc_delta_delta | Input Dim: 39\n",
      "    Modelos encontrados: 5\n",
      "Dataset cargado desde data/features/test_mfcc_delta_delta.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Modelo cargado desde diff_features/CNN/mfcc_delta_delta/CNN_mfcc_delta_delta_1.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc_delta_delta/CNN_mfcc_delta_delta_2.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc_delta_delta/CNN_mfcc_delta_delta_3.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc_delta_delta/CNN_mfcc_delta_delta_4.pt\n",
      "Modelo cargado desde diff_features/CNN/mfcc_delta_delta/CNN_mfcc_delta_delta_5.pt\n",
      "\n",
      "=== TABLA RESUMEN DE RESULTADOS ===\n",
      "   Architecture           Feature      F1 Score  Time (ms)  Time std (ms)\n",
      "20          CNN          wav2vec2  80.51 ± 1.84   0.313844       0.001718\n",
      "23          CNN  mfcc_delta_delta  72.60 ± 1.59   0.198127       0.001056\n",
      "21          CNN        mfcc_delta  70.65 ± 2.80   0.199862       0.001991\n",
      "22          CNN              mfcc  69.14 ± 1.99   0.189575       0.001134\n",
      "8           GRU          wav2vec2  87.42 ± 1.26   0.716150       0.005848\n",
      "9           GRU        mfcc_delta  84.00 ± 1.25   0.982159       0.005683\n",
      "11          GRU  mfcc_delta_delta  83.61 ± 0.77   0.973274       0.003733\n",
      "10          GRU              mfcc  79.70 ± 0.39   0.961120       0.003678\n",
      "0          LSTM          wav2vec2  87.07 ± 0.98   0.830360       0.006842\n",
      "3          LSTM  mfcc_delta_delta  83.29 ± 0.59   1.083231       0.001810\n",
      "1          LSTM        mfcc_delta  82.16 ± 1.00   1.054647       0.002622\n",
      "2          LSTM              mfcc  78.57 ± 0.88   1.031524       0.001986\n",
      "4           RNN          wav2vec2  61.14 ± 4.73   0.604521       0.004062\n",
      "7           RNN  mfcc_delta_delta   4.23 ± 1.43   0.803110       0.003870\n",
      "6           RNN              mfcc   3.46 ± 1.00   0.783664       0.003877\n",
      "5           RNN        mfcc_delta   2.78 ± 0.60   0.798205       0.002688\n",
      "16         TCNN          wav2vec2  84.81 ± 1.14   0.781584       0.028213\n",
      "19         TCNN  mfcc_delta_delta  84.73 ± 1.27   0.844018       0.000918\n",
      "17         TCNN        mfcc_delta  83.76 ± 2.11   0.841667       0.004371\n",
      "18         TCNN              mfcc  80.35 ± 0.67   0.833706       0.002097\n",
      "12  Transformer          wav2vec2  88.98 ± 1.75   1.342522       0.003027\n",
      "13  Transformer        mfcc_delta  83.36 ± 0.90   3.413879       0.002491\n",
      "15  Transformer  mfcc_delta_delta  80.74 ± 2.56   3.414136       0.005350\n",
      "14  Transformer              mfcc  73.58 ± 2.32   3.415844       0.003213\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 0. CONFIGURACIÓN TCNN (Necesaria para recrear la arquitectura)\n",
    "# ==========================================\n",
    "tcn_config_params = {\n",
    "    'num_channels': [64, 64, 128, 128],\n",
    "    'kernel_size': 3,\n",
    "    'dilations': [1, 2, 4, 8],\n",
    "    'dropout': 0,\n",
    "    'causal': False,\n",
    "    'input_shape': 'NCL',\n",
    "    'use_norm': 'batch_norm',\n",
    "    'use_skip_connections': True\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURACIÓN Y MAPEOS\n",
    "# ==========================================\n",
    "ROOT_DIR = \"diff_features\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "FEATURE_DIMS = {\n",
    "    \"mfcc\": 13,\n",
    "    \"mfcc_delta\": 26,       \n",
    "    \"mfcc_delta_delta\": 39, \n",
    "    \"wav2vec2\": 768\n",
    "}\n",
    "\n",
    "MODEL_CLASS_BY_TYPE = {\n",
    "    \"GRU\": RNNModel,\n",
    "    \"LSTM\": RNNModel,\n",
    "    \"RNN\": RNNModel,\n",
    "    \"TCNN\": SpeechCommandTCN, \n",
    "    \"CNN\": CNNModel,\n",
    "    \"TRANSFORMER\": TransformerModel,\n",
    "}\n",
    "\n",
    "def infer_model_type(path):\n",
    "    name = path.lower()\n",
    "    if \"gru\" in name: return \"GRU\"\n",
    "    if \"lstm\" in name: return \"LSTM\"\n",
    "    if \"rnn\" in name: return \"RNN\"\n",
    "    if \"tcnn\" in name: return \"TCNN\"\n",
    "    if \"cnn\" in name: return \"CNN\"\n",
    "    if \"transformer\" in name: return \"TRANSFORMER\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def load_model_dynamic(model_path, input_dim, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Carga el modelo inyectando la dimensión de entrada correcta.\n",
    "    \"\"\"\n",
    "    model_type = infer_model_type(model_path)\n",
    "    model_class = MODEL_CLASS_BY_TYPE[model_type]\n",
    "    \n",
    "    config = {}\n",
    "    \n",
    "    # --- CONFIGURACIÓN ESPECÍFICA POR ARQUITECTURA ---\n",
    "    if model_type in [\"GRU\", \"LSTM\", \"RNN\"]:\n",
    "        config = {\n",
    "            'rnn_type': model_type,\n",
    "            'n_input_channels': input_dim, \n",
    "            'hidd_size': 256,\n",
    "            'out_features': 35,\n",
    "            'num_layers': 1\n",
    "        }\n",
    "    elif model_type == \"CNN\":\n",
    "        config = {\n",
    "            'n_input_channels': input_dim, \n",
    "            'hidd_size': 64,\n",
    "            'out_features': 35\n",
    "        }\n",
    "    \n",
    "    # CORRECCIÓN TCNN: Usar las llaves correctas para SpeechCommandTCN\n",
    "    elif model_type == \"TCNN\":\n",
    "        config = {\n",
    "            'num_inputs': input_dim,      # TCNN usa 'num_inputs', no 'n_input_channels'\n",
    "            'num_classes': 35,\n",
    "            'tcn_params': tcn_config_params\n",
    "        }\n",
    "    \n",
    "    elif model_type == \"TRANSFORMER\":\n",
    "        config = {\n",
    "            'n_input_features': input_dim,\n",
    "            'n_output_classes': 35, \n",
    "            'd_model': 128, \n",
    "            'nhead': 8, \n",
    "            'd_hid': 512, \n",
    "            'n_layers': 4\n",
    "        }\n",
    "\n",
    "    return load_trained_model(model_class, model_path, device=device, config=config)\n",
    "\n",
    "# ==========================================\n",
    "# 2. BUCLE PRINCIPAL\n",
    "# ==========================================\n",
    "\n",
    "architectures_inf = [d for d in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, d))]\n",
    "metric_results_inf = []\n",
    "\n",
    "print(f\"Arquitecturas encontradas: {architectures_inf}\")\n",
    "\n",
    "for arch in architectures_inf:\n",
    "    arch_path = os.path.join(ROOT_DIR, arch)\n",
    "    \n",
    "    features_inf = [d for d in os.listdir(arch_path) if os.path.isdir(os.path.join(arch_path, d))]\n",
    "    \n",
    "    for feat_inf in features_inf: # <--- Usamos feat_inf\n",
    "        \n",
    "        # CORRECCIÓN: Usar feat_inf para construir el path, NO 'feat'\n",
    "        feat_path = os.path.join(arch_path, feat_inf)\n",
    "        \n",
    "        # Obtener dimensión esperada\n",
    "        input_dim = FEATURE_DIMS.get(feat_inf)\n",
    "        \n",
    "        if input_dim is None:\n",
    "            print(f\"[SKIP] Dimensión desconocida para feature: {feat_inf}. Saltando...\")\n",
    "            continue\n",
    "\n",
    "        # 1. Buscar los 5 modelos .pt\n",
    "        model_files = glob.glob(os.path.join(feat_path, \"*.pt\"))\n",
    "        model_files.sort()\n",
    "        \n",
    "        if not model_files:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n>>> Evaluando: {arch} | Feature: {feat_inf} | Input Dim: {input_dim}\")\n",
    "        print(f\"    Modelos encontrados: {len(model_files)}\")\n",
    "\n",
    "        # 2. Cargar Dataset de Test CORRESPONDIENTE\n",
    "        # CORRECCIÓN: Usar feat_inf aquí también\n",
    "        test_dataset_path = os.path.join(\"data\", \"features\", f\"test_{feat_inf}.pt\") \n",
    "        \n",
    "        if not os.path.exists(test_dataset_path):\n",
    "            # Intento de fallback por si tus archivos se llaman distinto (ej: mfcc_delta -> mfcc)\n",
    "            print(f\"    [ALERTA] No existe {test_dataset_path}.\")\n",
    "            continue\n",
    "            \n",
    "        test_dataset_inf = FeaturesDataset(test_dataset_path)\n",
    "        test_loader_inf = torch.utils.data.DataLoader(test_dataset_inf, batch_size=32, shuffle=False)\n",
    "\n",
    "        # 3. Cargar Modelos\n",
    "        models_list = []\n",
    "        for m_path in model_files:\n",
    "            try:\n",
    "                m = load_model_dynamic(m_path, input_dim, device=device)\n",
    "                models_list.append(m)\n",
    "            except Exception as e:\n",
    "                print(f\"    [ERROR CRÍTICO] Falló carga de {os.path.basename(m_path)}: {e}\")\n",
    "\n",
    "        if not models_list:\n",
    "            print(\"    No se cargaron modelos válidos. Saltando evaluación.\")\n",
    "            continue\n",
    "\n",
    "        # 4. Evaluar\n",
    "        metrics_mean_inf, metrics_std_inf, _, _, _ = evaluate_models_metrics(\n",
    "            models_list, test_loader_inf, nn.CrossEntropyLoss(), use_gpu=(device==\"cuda\")\n",
    "        )\n",
    "        \n",
    "        # 5. Guardar Resultados\n",
    "        row = {\n",
    "            \"Architecture\": arch,\n",
    "            \"Feature\": feat_inf,\n",
    "            \"F1 Mean\": metrics_mean_inf[\"f1\"] * 100,\n",
    "            \"F1 Std\": metrics_std_inf[\"f1\"] * 100,\n",
    "            \"Acc Mean\": metrics_mean_inf[\"accuracy\"] * 100,\n",
    "            \"Time (ms)\": metrics_mean_inf[\"infer_time\"]*1000,\n",
    "            \"Time std (ms)\": metrics_std_inf['infer_time']*1000\n",
    "        }\n",
    "        metric_results_inf.append(row)\n",
    "        \n",
    "        # Matriz de confusión (Opcional)\n",
    "        img_name = f\"conf_mat_{arch}_{feat_inf}.pdf\"\n",
    "        save_path = os.path.join(\"img\", \"confusion_matrices\", img_name)\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        # Descomenta si quieres generar las imágenes\n",
    "        # get_metrics_and_confusion_matrix(models_list, test_dataset_inf, name=f\"{arch} - {feat_inf}\", filename=save_path)\n",
    "\n",
    "# ==========================================\n",
    "# 3. VISUALIZAR TABLA FINAL\n",
    "# ==========================================\n",
    "if metric_results_inf:\n",
    "    df_final = pd.DataFrame(metric_results_inf)\n",
    "    plot_f1_vs_time_from_df(df_final, filename='comparativa_modelos_f1_tiempo.pdf')\n",
    "    df_final[\"F1 Score\"] = df_final.apply(lambda x: f\"{x['F1 Mean']:.2f} ± {x['F1 Std']:.2f}\", axis=1)\n",
    "    df_final = df_final.sort_values([\"Architecture\", \"F1 Mean\"], ascending=[True, False])\n",
    "\n",
    "    print(\"\\n=== TABLA RESUMEN DE RESULTADOS ===\")\n",
    "    print(df_final[[\"Architecture\", \"Feature\", \"F1 Score\", \"Time (ms)\", \"Time std (ms)\"]])\n",
    "else:\n",
    "    print(\"No se generaron resultados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa6529e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gráfico guardado en: img/comparativa_modelos_f1_tiempo.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "plot_f1_vs_time_custom_legend(df_final, filename='comparativa_modelos_f1_tiempo.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
