{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69090f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# Metrics and visualization\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Models and feature extractor\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "# Others\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20f275",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c354a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpeechCommands(Dataset):\n",
    "    def __init__(self, root, files_list, download=True, target_len=16000, mode=\"mfcc\", cnn_model=None):\n",
    "        \"\"\"\n",
    "        mode: 'mfcc', 'mfcc_delta', 'mfcc_delta_delta', 'cnn', 'wav2vec2'\n",
    "        cnn_model: modelo CNN preentrenado o personalizado para extracción\n",
    "        \"\"\"\n",
    "        self.target_len = target_len\n",
    "        self.mode = mode\n",
    "        self.cnn_model = cnn_model\n",
    "        self.dataset = torchaudio.datasets.SPEECHCOMMANDS(root=root, download=download)\n",
    "        self.indices = None\n",
    "        self.splitter(files_list, root)\n",
    "\n",
    "    def splitter(self, files_list, root):\n",
    "        with open(files_list, 'r') as f:\n",
    "            self.file_paths = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        self.all_paths = []\n",
    "        for item in tqdm(self.dataset._walker, desc=f\"Splitting {files_list}\"):\n",
    "            relative_path = os.path.relpath(\n",
    "                item,\n",
    "                start=os.path.join(root, \"SpeechCommands\", \"speech_commands_v0.02\")\n",
    "            ).replace(\"\\\\\", \"/\")\n",
    "            self.all_paths.append(relative_path)\n",
    "\n",
    "        self.indices = [i for i, path in enumerate(self.all_paths) if path in self.file_paths]\n",
    "        print(f\"Archivos encontrados: {len(self.indices)} / {len(self.file_paths)}\")\n",
    "\n",
    "    def pad_waveform(self, waveform):\n",
    "        length = waveform.shape[-1]\n",
    "        if length < self.target_len:\n",
    "            waveform = F.pad(waveform, (0, self.target_len - length))\n",
    "        elif length > self.target_len:\n",
    "            waveform = waveform[:, :self.target_len]\n",
    "        return waveform\n",
    "\n",
    "    def extract_feature_single(self, waveform, sample_rate, feature_extractor=None, processor=None, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Extrae features de UNA muestra según el modo configurado.\n",
    "        \"\"\"\n",
    "        waveform = self.pad_waveform(waveform).to(device)\n",
    "\n",
    "        if feature_extractor is not None:\n",
    "            feature_extractor = feature_extractor.to(device)\n",
    "\n",
    "        # --- MFCC ---\n",
    "        if self.mode == \"mfcc\":\n",
    "            feat = feature_extractor(waveform).squeeze(0).cpu().transpose(0, 1)\n",
    "\n",
    "        # --- MFCC + Delta ---\n",
    "        elif self.mode == \"mfcc_delta\":\n",
    "            base = feature_extractor(waveform)\n",
    "            delta = torchaudio.functional.compute_deltas(base)\n",
    "            feat = torch.cat([base, delta], dim=1).squeeze(0).cpu().transpose(0, 1)\n",
    "\n",
    "        # --- MFCC + Delta + Delta-Delta ---\n",
    "        elif self.mode == \"mfcc_delta_delta\":\n",
    "            base = feature_extractor(waveform)\n",
    "            delta = torchaudio.functional.compute_deltas(base)\n",
    "            delta2 = torchaudio.functional.compute_deltas(delta)\n",
    "            feat = torch.cat([base, delta, delta2], dim=1).squeeze(0).cpu().transpose(0, 1)\n",
    "\n",
    "        # --- CNN ---\n",
    "        elif self.mode == \"cnn\":\n",
    "            spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate).to(device)\n",
    "            spec = spec_transform(waveform).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                embedding = self.cnn_model(spec.to(device)).cpu().squeeze()\n",
    "            feat = embedding\n",
    "\n",
    "        # --- Wav2Vec2 ---\n",
    "        elif self.mode == \"wav2vec2\":\n",
    "            waveform = waveform.squeeze(0)\n",
    "            inputs = processor(\n",
    "                waveform,\n",
    "                sampling_rate=sample_rate,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = feature_extractor(**inputs)\n",
    "            feat = outputs.last_hidden_state.squeeze(0).cpu()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Modo de extracción '{self.mode}' no soportado.\")\n",
    "\n",
    "        return feat\n",
    "\n",
    "    def extract_features(self, feature_extractor=None, processor=None, device=\"cuda\"):\n",
    "        features, labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx in tqdm(self.indices, desc=f\"Extrayendo features ({self.mode})\"):\n",
    "                waveform, sample_rate, label, _, _ = self.dataset[idx]\n",
    "                feat = self.extract_feature_single(\n",
    "                    waveform, sample_rate, feature_extractor, processor, device\n",
    "                )\n",
    "                features.append(feat)\n",
    "                labels.append(label)\n",
    "\n",
    "        features = torch.stack(features)\n",
    "        print(f\"Features tensor: {features.shape}\")\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "    def save_features(self, feature_extractor=None, save_path=None, processor=None, device=\"cuda\"):\n",
    "        print(f\"Guardando features ({self.mode}) en {save_path}\")\n",
    "        try:\n",
    "            features, labels = self.extract_features(feature_extractor, processor, device)\n",
    "            torch.save({\"features\": features, \"labels\": labels}, save_path)\n",
    "            print(f\"Features guardadas correctamente en {save_path}\")\n",
    "            print(f\"Clases finales: {set(labels)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al guardar features en {save_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = self.indices[idx]\n",
    "        waveform, sample_rate, label, speaker_id, utterance_number = self.dataset[original_idx]\n",
    "        waveform = self.pad_waveform(waveform)\n",
    "        return waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "class FeaturesDataset(Dataset):\n",
    "    def __init__(self, features_path):\n",
    "        \"\"\"\n",
    "        Carga un archivo .pt con 'features' y 'labels' previamente guardados.\n",
    "\n",
    "        features_path: ruta al archivo .pt (por ejemplo 'data/train.pt')\n",
    "        \"\"\"\n",
    "        data = torch.load(features_path)\n",
    "        self.features = data[\"features\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "\n",
    "        # Crear diccionario para pasar de string a índice (útil para entrenar)\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(sorted(set(self.labels)))}\n",
    "        self.idx_to_label = {v: k for k, v in self.label_to_idx.items()}\n",
    "        self.numeric_labels = torch.tensor([self.label_to_idx[l] for l in self.labels])\n",
    "\n",
    "        print(f\"Dataset cargado desde {features_path}\")\n",
    "        print(f\" - {len(self.features)} ejemplos\")\n",
    "        print(f\" - {len(self.label_to_idx)} clases\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.numeric_labels[idx]\n",
    "        return feature, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e834f86",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32de13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type,\n",
    "        n_input_channels,\n",
    "        hidd_size=256,\n",
    "        out_features = 35,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Para utilizar una vanilla RNN entregue rnn_type=\"RNN\"\n",
    "        Para utilizar una LSTM entregue rnn_type=\"LSTM\"\n",
    "        Para utilizar una GRU entregue rnn_type=\"GRU\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "        if rnn_type == \"GRU\":\n",
    "            self.rnn_layer = nn.GRU(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers)\n",
    "\n",
    "        elif rnn_type == \"LSTM\":\n",
    "            self.rnn_layer = nn.LSTM(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers)\n",
    "\n",
    "        elif rnn_type == \"RNN\":\n",
    "            self.rnn_layer = nn.RNN(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers, bidirectional=True)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"rnn_type {rnn_type} not supported.\")\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidd_size, out_features),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.rnn_type == \"GRU\":\n",
    "            out, h = self.rnn_layer(x)\n",
    "\n",
    "        elif self.rnn_type == \"LSTM\":\n",
    "            out, (h, c) = self.rnn_layer(x)\n",
    "\n",
    "        elif self.rnn_type == \"RNN\":\n",
    "            out, h = self.rnn_layer(x)\n",
    "\n",
    "        out = h[-1]\n",
    "\n",
    "        return self.net(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8e3a4",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b5b7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu):\n",
    "    # Predicción\n",
    "    y_predicted = model(x_batch)\n",
    "\n",
    "    # Cálculo de loss\n",
    "    loss = criterion(y_predicted, y_batch)\n",
    "\n",
    "    # Actualización de parámetros\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return y_predicted, loss\n",
    "\n",
    "\n",
    "def evaluate(val_loader, model, criterion, use_gpu):\n",
    "    cumulative_loss = 0\n",
    "    cumulative_predictions = 0\n",
    "    data_count = 0\n",
    "\n",
    "    for x_val, y_val in val_loader:\n",
    "        if use_gpu:\n",
    "            x_val = x_val.cuda()\n",
    "            y_val = y_val.cuda()\n",
    "\n",
    "        y_predicted = model(x_val)\n",
    "\n",
    "        loss = criterion(y_predicted, y_val)\n",
    "\n",
    "        class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "\n",
    "        cumulative_predictions += (y_val == class_prediction).sum().item()\n",
    "        cumulative_loss += loss.item() * y_val.shape[0]\n",
    "        data_count += y_val.shape[0]\n",
    "\n",
    "    val_acc = cumulative_predictions / data_count\n",
    "    val_loss = cumulative_loss / data_count\n",
    "\n",
    "    return val_acc, val_loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    n_evaluations_per_epoch=6,\n",
    "    use_gpu=False,\n",
    "):\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    # Definición de dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=use_gpu)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=use_gpu)\n",
    "\n",
    "    # Optimizador\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    # Listas para guardar curvas de entrenamiento\n",
    "    curves = {\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "    }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    n_batches = len(train_loader)\n",
    "    print(n_batches)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\rEpoch {epoch + 1}/{epochs}\")\n",
    "        cumulative_train_loss = 0\n",
    "        cumulative_train_corrects = 0\n",
    "        examples_count = 0\n",
    "\n",
    "        # Entrenamiento del modelo\n",
    "        model.train()\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            if use_gpu:\n",
    "                x_batch = x_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "\n",
    "            y_predicted, loss = train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu)\n",
    "\n",
    "            cumulative_train_loss += loss.item() * x_batch.shape[0]\n",
    "            examples_count += y_batch.shape[0]\n",
    "\n",
    "            # Calculamos número de aciertos\n",
    "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "            cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
    "\n",
    "            if (i % (n_batches // n_evaluations_per_epoch) == 0) and (i > 0):\n",
    "                train_loss = cumulative_train_loss / examples_count\n",
    "                train_acc = cumulative_train_corrects / examples_count\n",
    "\n",
    "                print(f\"Iteration {iteration} - Batch {i}/{len(train_loader)} - Train loss: {train_loss}, Train acc: {train_acc}\")\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc, val_loss = evaluate(val_loader, model, criterion, use_gpu)\n",
    "\n",
    "        print(f\"Val loss: {val_loss}, Val acc: {val_acc}\")\n",
    "\n",
    "        train_loss = cumulative_train_loss / examples_count\n",
    "        train_acc = cumulative_train_corrects / examples_count\n",
    "\n",
    "        curves[\"train_acc\"].append(train_acc)\n",
    "        curves[\"val_acc\"].append(val_acc)\n",
    "        curves[\"train_loss\"].append(train_loss)\n",
    "        curves[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    print()\n",
    "    total_time = time.perf_counter() - t0\n",
    "    print(f\"Tiempo total de entrenamiento: {total_time:.4f} [s]\")\n",
    "\n",
    "    model.cpu()\n",
    "\n",
    "    return curves, total_time\n",
    "\n",
    "def show_curves(all_curves, suptitle=''):\n",
    "    final_curve_means = {k: np.mean([c[k] for c in all_curves], axis=0) for k in all_curves[0].keys()}\n",
    "    final_curve_stds = {k: np.std([c[k] for c in all_curves], axis=0) for k in all_curves[0].keys()}\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    fig.set_facecolor('white')\n",
    "\n",
    "    epochs = np.arange(len(final_curve_means[\"val_loss\"])) + 1\n",
    "\n",
    "    # ==== Plot de pérdidas ====\n",
    "    ax[0].plot(epochs, final_curve_means['val_loss'], label='validation')\n",
    "    ax[0].plot(epochs, final_curve_means['train_loss'], label='training')\n",
    "    ax[0].fill_between(epochs, \n",
    "                       y1=final_curve_means[\"val_loss\"] - final_curve_stds[\"val_loss\"], \n",
    "                       y2=final_curve_means[\"val_loss\"] + final_curve_stds[\"val_loss\"], alpha=.5)\n",
    "    ax[0].fill_between(epochs, \n",
    "                       y1=final_curve_means[\"train_loss\"] - final_curve_stds[\"train_loss\"], \n",
    "                       y2=final_curve_means[\"train_loss\"] + final_curve_stds[\"train_loss\"], alpha=.5)\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Loss evolution during training')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # ==== Plot de precisión ====\n",
    "    ax[1].plot(epochs, final_curve_means['val_acc'], label='validation')\n",
    "    ax[1].plot(epochs, final_curve_means['train_acc'], label='training')\n",
    "    ax[1].fill_between(epochs, \n",
    "                       y1=final_curve_means[\"val_acc\"] - final_curve_stds[\"val_acc\"], \n",
    "                       y2=final_curve_means[\"val_acc\"] + final_curve_stds[\"val_acc\"], alpha=.5)\n",
    "    ax[1].fill_between(epochs, \n",
    "                       y1=final_curve_means[\"train_acc\"] - final_curve_stds[\"train_acc\"], \n",
    "                       y2=final_curve_means[\"train_acc\"] + final_curve_stds[\"train_acc\"], alpha=.5)\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_title('Accuracy evolution during training')\n",
    "    ax[1].legend()\n",
    "\n",
    "    fig.suptitle(suptitle, fontsize=16, weight=\"bold\")\n",
    "\n",
    "    # ==== Guardar y cerrar ====\n",
    "    filepath = os.path.join('img', f'{suptitle}.pdf')\n",
    "    plt.savefig(filepath, bbox_inches='tight', format='pdf')\n",
    "    plt.close(fig)  \n",
    "\n",
    "def get_metrics_and_confusion_matrix(models, dataset, name=''):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=min(16, len(dataset)))\n",
    "\n",
    "    # === Obtener etiquetas verdaderas ===\n",
    "    y_true = []\n",
    "    for _, y in dataloader:\n",
    "        y_true.append(y)\n",
    "    y_true = torch.cat(y_true)\n",
    "    n_classes = len(torch.unique(y_true))\n",
    "\n",
    "    # === Definir labels ===\n",
    "    if hasattr(dataset, 'idx_to_label'):\n",
    "        labels = [dataset.idx_to_label[i] for i in range(n_classes)]\n",
    "    elif hasattr(dataset, 'labels'):\n",
    "        labels = dataset.labels\n",
    "    else:\n",
    "        labels = [str(i) for i in range(n_classes)]\n",
    "\n",
    "    # === Calcular matrices de confusión ===\n",
    "    cms = []\n",
    "    for model in models:\n",
    "        model.cpu()\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        for x, _ in dataloader:\n",
    "            y_pred.append(model(x).argmax(dim=1))\n",
    "        y_pred = torch.cat(y_pred)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=range(n_classes), normalize='true')\n",
    "        cms.append(cm)\n",
    "\n",
    "    cms = np.stack(cms)\n",
    "    cm_mean = cms.mean(axis=0)\n",
    "    cm_std = cms.std(axis=0)\n",
    "\n",
    "    # === Accuracy promedio ===\n",
    "    accs = []\n",
    "    for model in models:\n",
    "        y_pred = []\n",
    "        for x, _ in dataloader:\n",
    "            y_pred.append(model(x).argmax(dim=1))\n",
    "        y_pred = torch.cat(y_pred)\n",
    "        accs.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "    acc_mean = np.mean(accs) * 100\n",
    "    acc_std = np.std(accs) * 100\n",
    "\n",
    "    # === Figura combinada ===\n",
    "    os.makedirs('img', exist_ok=True)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # --- Subplot 1: medias ---\n",
    "    im1 = axs[0].imshow(cm_mean, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    axs[0].set_title('Mean Confusion Matrix')\n",
    "    axs[0].set_xlabel('Predicted label')\n",
    "    axs[0].set_ylabel('True label')\n",
    "    axs[0].set_xticks(np.arange(n_classes))\n",
    "    axs[0].set_yticks(np.arange(n_classes))\n",
    "    axs[0].set_xticklabels(labels, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    axs[0].set_yticklabels(labels)\n",
    "    fig.colorbar(im1, ax=axs[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    # --- Subplot 2: desviaciones estándar ---\n",
    "    im2 = axs[1].imshow(cm_std, interpolation='nearest', cmap=plt.cm.Oranges)\n",
    "    axs[1].set_title('Standard Deviation')\n",
    "    axs[1].set_xlabel('Predicted label')\n",
    "    axs[1].set_ylabel('True label')\n",
    "    axs[1].set_xticks(np.arange(n_classes))\n",
    "    axs[1].set_yticks(np.arange(n_classes))\n",
    "    axs[1].set_xticklabels(labels, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    axs[1].set_yticklabels(labels)\n",
    "    fig.colorbar(im2, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    # --- Título general ---\n",
    "    fig.suptitle(rf'{name}, mean acc = {acc_mean:.2f} ± {acc_std:.2f}%', fontsize=12)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    filepath = os.path.join('img', f'conf_mat_{name}.pdf')\n",
    "    plt.savefig(filepath, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Combined confusion matrix (mean + std) saved to {filepath}\")\n",
    "\n",
    "def evaluate_with_std(model, dataloader, criterion, use_gpu=True):\n",
    "    # jaja std\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    all_losses = []\n",
    "    all_accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if use_gpu:\n",
    "                X, y = X.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            all_losses.append(loss.item())\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            acc = (preds == y).float().mean().item()\n",
    "            all_accuracies.append(acc)\n",
    "\n",
    "    mean_loss = np.mean(all_losses)\n",
    "    std_loss = np.std(all_losses)\n",
    "    mean_acc = np.mean(all_accuracies)\n",
    "    std_acc = np.std(all_accuracies)\n",
    "\n",
    "    model.cpu()\n",
    "\n",
    "    return mean_acc, std_acc, mean_loss, std_loss\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_models_metrics(models, dataloader, criterion, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Evalúa múltiples modelos y calcula métricas promedio y desviación estándar.\n",
    "    Retorna un diccionario con accuracy, recall, precision y f1\n",
    "    \"\"\"\n",
    "\n",
    "    # Diccionarios para guardar resultados\n",
    "    all_metrics = {\n",
    "        \"accuracy\": [],\n",
    "        \"recall\": [],\n",
    "        \"precision\": [],\n",
    "        \"f1\": [],\n",
    "    }\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        if use_gpu:\n",
    "            model.cuda()\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                if use_gpu:\n",
    "                    X, y = X.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        # Cálculo de métricas\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        loss_mean = np.mean(losses)\n",
    "\n",
    "        # Guardar métricas\n",
    "        all_metrics[\"accuracy\"].append(acc)\n",
    "        all_metrics[\"recall\"].append(rec)\n",
    "        all_metrics[\"precision\"].append(prec)\n",
    "        all_metrics[\"f1\"].append(f1)\n",
    "\n",
    "        if use_gpu:\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model.cpu()\n",
    "\n",
    "    # Calcular medias y desviaciones estándar\n",
    "    metrics_mean = {k: np.mean(v) for k, v in all_metrics.items()}\n",
    "    metrics_std = {k: np.std(v) for k, v in all_metrics.items()}\n",
    "\n",
    "    print(\"\\n=== Resultados promedio sobre modelos ===\")\n",
    "    for metric in all_metrics.keys():\n",
    "        print(f\"{metric.capitalize():<10}: {metrics_mean[metric]:.4f} +/- {metrics_std[metric]:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Detalles por modelo ===\")\n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"\\n=== Modelo {i + 1} ({model.rnn_type}) ===\")\n",
    "        for metric in all_metrics.keys():\n",
    "            print(f\"{metric.capitalize():<10}: {all_metrics[metric][i]:.4f} +/- {metrics_std[metric]:.4f}\")\n",
    "\n",
    "    # return metrics_mean, metrics_std, all_metrics\n",
    "    return\n",
    "\n",
    "def nfft_hop_length_exp(n_trains, feature_xtractor, batch_size, lr, epochs, criterion, use_gpu = True):\n",
    "    \n",
    "    # ======== Estructuras de resultados ========\n",
    "    results = {}  # {(nfft, hl): [accuracies]}\n",
    "    times_of_training = {}\n",
    "    models = {}\n",
    "\n",
    "    # ======== Obtener combinaciones de archivos ========\n",
    "    base_dir = os.path.join('data', 'petes')\n",
    "    files = os.listdir(base_dir)\n",
    "\n",
    "    # Extraer parámetros nfft y hop_length de los nombres\n",
    "    pattern = re.compile(r'n(\\d+)_hl(\\d+)')\n",
    "    pairs = sorted(list({pattern.search(f).groups() for f in files if pattern.search(f)}))\n",
    "\n",
    "    # ======== Loop sobre combinaciones ========\n",
    "    for nfft, hl in pairs:\n",
    "        nfft = int(nfft)\n",
    "        hl = int(hl)\n",
    "        print(f\"\\n=== Entrenando para nfft={nfft}, hop_length={hl} ===\")\n",
    "\n",
    "        # Cargar datasets\n",
    "        train_dataset = feature_xtractor(os.path.join(base_dir, f'train_n{nfft}_hl{hl}.pt'))\n",
    "        test_dataset = feature_xtractor(os.path.join(base_dir, f'test_n{nfft}_hl{hl}.pt'))\n",
    "        val_dataset = feature_xtractor(os.path.join(base_dir, f'val_n{nfft}_hl{hl}.pt'))\n",
    "\n",
    "        accs = []\n",
    "        train_times = []\n",
    "\n",
    "        for k in range(n_trains):\n",
    "            print(f'  Entrenando modelo {k+1}/{n_trains}')\n",
    "            model = RNNModel(rnn_type='RNN', n_input_channels=13, hidd_size=128)\n",
    "            all_curves, times = train_model(\n",
    "                model, train_dataset, val_dataset, epochs, criterion,\n",
    "                batch_size, lr, n_evaluations_per_epoch=3, use_gpu=use_gpu\n",
    "            )\n",
    "\n",
    "            val_acc = all_curves[\"val_acc\"][-1]  # o la métrica final que uses\n",
    "            accs.append(val_acc)\n",
    "            train_times.append(times)\n",
    "            models[(nfft, hl, k)] = model\n",
    "\n",
    "        results[(nfft, hl)] = accs\n",
    "        times_of_training[(nfft, hl)] = train_times\n",
    "\n",
    "    # ======== Graficar resultados ========\n",
    "    # === Procesar los resultados ===\n",
    "    nfft_vals = sorted(set(k[0] for k in results.keys()))\n",
    "    hl_vals   = sorted(set(k[1] for k in results.keys()))\n",
    "\n",
    "    # Crear matrices de promedio y desviación estándar\n",
    "    mean_matrix = np.zeros((len(hl_vals), len(nfft_vals)))\n",
    "    std_matrix  = np.zeros((len(hl_vals), len(nfft_vals)))\n",
    "\n",
    "    for i, hl in enumerate(hl_vals):\n",
    "        for j, nfft in enumerate(nfft_vals):\n",
    "            if (nfft, hl) in results:\n",
    "                vals = np.array(results[(nfft, hl)])\n",
    "                mean_matrix[i, j] = np.mean(vals)\n",
    "                std_matrix[i, j]  = np.std(vals)\n",
    "            else:\n",
    "                mean_matrix[i, j] = np.nan\n",
    "                std_matrix[i, j]  = np.nan\n",
    "\n",
    "    # === Graficar el mapa de calor ===\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    im = plt.imshow(mean_matrix, cmap='viridis', origin='lower', aspect='auto')\n",
    "\n",
    "    # Etiquetas\n",
    "    plt.xticks(range(len(nfft_vals)), nfft_vals)\n",
    "    plt.yticks(range(len(hl_vals)), hl_vals)\n",
    "    plt.xlabel('n_fft')\n",
    "    plt.ylabel('hop_length')\n",
    "    plt.title('Accuracy promedio ± desviación (5 entrenamientos por configuración)')\n",
    "\n",
    "    # Barra de color\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label('Accuracy promedio')\n",
    "\n",
    "    # Mostrar valores promedio ± std en cada celda\n",
    "    for i in range(len(hl_vals)):\n",
    "        for j in range(len(nfft_vals)):\n",
    "            mean_val = mean_matrix[i, j]\n",
    "            std_val  = std_matrix[i, j]\n",
    "            if not np.isnan(mean_val):\n",
    "                color = 'white' if mean_val < 0.7 else 'black'\n",
    "                plt.text(j, i, f\"+/-{std_val:.4f}\", \n",
    "                        ha='center', va='center', color=color, fontsize=6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c360d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b30a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(wf, sample_rate, label=\"\", figname=None):\n",
    "    \"\"\"\n",
    "    Muestra el waveform (izquierda) y los MFCCs (derecha) de una señal de audio.\n",
    "\n",
    "    Parámetros:\n",
    "        wf (Tensor): señal de audio [1, N] o [N]\n",
    "        sample_rate (int): frecuencia de muestreo (Hz)\n",
    "        label (str): etiqueta opcional para el título\n",
    "        figname (str): ruta para guardar la figura (si es None, solo muestra)\n",
    "    \"\"\"\n",
    "    if isinstance(wf, torch.Tensor):\n",
    "        wf = wf.squeeze().cpu()\n",
    "\n",
    "    # === Transformación MFCC ===\n",
    "    mfcc_transform = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=13,\n",
    "        melkwargs={\"n_fft\": 320, \"hop_length\": 160, \"n_mels\": 23},\n",
    "        log_mels=True\n",
    "    )\n",
    "    mfcc = mfcc_transform(wf.unsqueeze(0)).squeeze().cpu().numpy()  # [n_mfcc, time]\n",
    "\n",
    "    # === Crear figura con 2 subplots ===\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # --- Waveform ---\n",
    "    time = torch.arange(0, len(wf)) / sample_rate\n",
    "    axes[0].plot(time, wf.numpy(), color=\"steelblue\", linewidth=1.0)\n",
    "    axes[0].set_title(\"Waveform\", fontsize=12)\n",
    "    axes[0].set_xlabel(\"Tiempo [s]\")\n",
    "    axes[0].set_ylabel(\"Amplitud\")\n",
    "\n",
    "    # --- MFCC ---\n",
    "    sns.heatmap(mfcc, ax=axes[1], cmap=\"viridis\", cbar=True)\n",
    "    axes[1].set_title(\"MFCCs\", fontsize=12)\n",
    "    axes[1].set_xlabel(\"Tiempo (frames)\")\n",
    "    axes[1].set_ylabel(\"Coeficiente MFCC\")\n",
    "\n",
    "    fig.suptitle(f\"Audio: {label}\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === Guardar o mostrar ===\n",
    "    if figname:\n",
    "        name = os.path.join('img', f'{figname}.pdf')\n",
    "        plt.savefig(name, bbox_inches=\"tight\")\n",
    "        print(f\"Figura guardada en {name}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89760d",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f47cce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/features/train_mfcc.pt ya existe, saltando...\n",
      "data/features/train_mfcc_delta.pt ya existe, saltando...\n",
      "data/features/train_mfcc_delta_delta.pt ya existe, saltando...\n",
      "data/features/train_wav2vec2.pt ya existe, saltando...\n",
      "data/features/val_mfcc.pt ya existe, saltando...\n",
      "data/features/val_mfcc_delta.pt ya existe, saltando...\n",
      "data/features/val_mfcc_delta_delta.pt ya existe, saltando...\n",
      "data/features/val_wav2vec2.pt ya existe, saltando...\n",
      "data/features/test_mfcc.pt ya existe, saltando...\n",
      "data/features/test_mfcc_delta.pt ya existe, saltando...\n",
      "data/features/test_mfcc_delta_delta.pt ya existe, saltando...\n",
      "data/features/test_wav2vec2.pt ya existe, saltando...\n",
      "\n",
      "Extracción de features completada.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuración base ---\n",
    "ROOT_DIR = \"data\"\n",
    "SAVE_DIR = os.path.join(ROOT_DIR, \"features\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Parámetros comunes ---\n",
    "mfcc = torchaudio.transforms.MFCC(\n",
    "    sample_rate=16000,\n",
    "    n_mfcc=13,\n",
    "    melkwargs={\"n_fft\": 320, \"hop_length\": 160, \"n_mels\": 23}\n",
    ")\n",
    "\n",
    "# --- Inicializar Wav2Vec2 una sola vez ---\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
    "\n",
    "# --- Configuración de modos y extractores ---\n",
    "modes = {\n",
    "    \"mfcc\": mfcc,\n",
    "    \"mfcc_delta\": mfcc,\n",
    "    \"mfcc_delta_delta\": mfcc,\n",
    "    \"wav2vec2\": wav2vec2,\n",
    "}\n",
    "\n",
    "# --- Procesar para train y val ---\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    list_path = os.path.join(ROOT_DIR, f\"{split}_list.txt\")\n",
    "\n",
    "    for mode, extractor in modes.items():\n",
    "        save_path = os.path.join(SAVE_DIR, f\"{split}_{mode}.pt\")\n",
    "\n",
    "        if os.path.isfile(save_path):\n",
    "            print(f\"{save_path} ya existe, saltando...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nExtrayendo {mode} para {split}...\")\n",
    "\n",
    "        dataset = CustomSpeechCommands(ROOT_DIR, list_path, mode=mode)\n",
    "        if mode == \"wav2vec2\":\n",
    "            dataset.save_features(\n",
    "                feature_extractor=extractor,\n",
    "                processor=processor,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "        else:\n",
    "            dataset.save_features(\n",
    "                feature_extractor=extractor,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "\n",
    "print(\"\\nExtracción de features completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac49807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/train_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 201335.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 32453 / 32453\n",
      "Guardando features (mfcc) en data/petes/train_320_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 32453/32453 [00:56<00:00, 574.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([32453, 51, 13])\n",
      "Features guardadas correctamente en data/petes/train_320_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/train_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 206087.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 32453 / 32453\n",
      "Guardando features (mfcc) en data/petes/train_160_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 32453/32453 [00:56<00:00, 573.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([32453, 101, 13])\n",
      "Features guardadas correctamente en data/petes/train_160_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/train_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 215977.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 32453 / 32453\n",
      "Guardando features (mfcc) en data/petes/train_54_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 32453/32453 [00:56<00:00, 572.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([32453, 297, 13])\n",
      "Features guardadas correctamente en data/petes/train_54_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/train_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 203278.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 32453 / 32453\n",
      "Guardando features (mfcc) en data/petes/train_32_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 32453/32453 [00:57<00:00, 569.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([32453, 501, 13])\n",
      "Features guardadas correctamente en data/petes/train_32_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/train_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 207420.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 32453 / 32453\n",
      "Guardando features (mfcc) en data/petes/train_16_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 32453/32453 [00:57<00:00, 563.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([32453, 1001, 13])\n",
      "Features guardadas correctamente en data/petes/train_16_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/val_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 200207.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 3875 / 3875\n",
      "Guardando features (mfcc) en data/petes/val_320_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 3875/3875 [00:07<00:00, 513.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([3875, 51, 13])\n",
      "Features guardadas correctamente en data/petes/val_320_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/val_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 203206.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 3875 / 3875\n",
      "Guardando features (mfcc) en data/petes/val_160_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 3875/3875 [00:06<00:00, 568.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([3875, 101, 13])\n",
      "Features guardadas correctamente en data/petes/val_160_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/val_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 212493.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 3875 / 3875\n",
      "Guardando features (mfcc) en data/petes/val_54_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 3875/3875 [00:06<00:00, 566.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([3875, 297, 13])\n",
      "Features guardadas correctamente en data/petes/val_54_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/val_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 206657.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 3875 / 3875\n",
      "Guardando features (mfcc) en data/petes/val_32_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 3875/3875 [00:06<00:00, 564.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([3875, 501, 13])\n",
      "Features guardadas correctamente en data/petes/val_32_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/val_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 204971.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 3875 / 3875\n",
      "Guardando features (mfcc) en data/petes/val_16_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 3875/3875 [00:06<00:00, 564.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([3875, 1001, 13])\n",
      "Features guardadas correctamente en data/petes/val_16_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/test_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 214133.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 4381 / 4381\n",
      "Guardando features (mfcc) en data/petes/test_320_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 4381/4381 [00:08<00:00, 517.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([4381, 51, 13])\n",
      "Features guardadas correctamente en data/petes/test_320_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/test_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 208112.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 4381 / 4381\n",
      "Guardando features (mfcc) en data/petes/test_160_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 4381/4381 [00:07<00:00, 570.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([4381, 101, 13])\n",
      "Features guardadas correctamente en data/petes/test_160_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/test_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 213462.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 4381 / 4381\n",
      "Guardando features (mfcc) en data/petes/test_54_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 4381/4381 [00:07<00:00, 567.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([4381, 297, 13])\n",
      "Features guardadas correctamente en data/petes/test_54_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/test_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 212645.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 4381 / 4381\n",
      "Guardando features (mfcc) en data/petes/test_32_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 4381/4381 [00:07<00:00, 571.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([4381, 501, 13])\n",
      "Features guardadas correctamente en data/petes/test_32_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extrayendo MFCC(\n",
      "  (amplitude_to_DB): AmplitudeToDB()\n",
      "  (MelSpectrogram): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      ") para test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data/test_list.txt: 100%|██████████| 105829/105829 [00:00<00:00, 212084.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 4381 / 4381\n",
      "Guardando features (mfcc) en data/petes/test_16_mfcc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo features (mfcc): 100%|██████████| 4381/4381 [00:07<00:00, 567.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor: torch.Size([4381, 1001, 13])\n",
      "Features guardadas correctamente en data/petes/test_16_mfcc.pt\n",
      "Clases finales: {'backward', 'wow', 'marvin', 'off', 'bird', 'seven', 'happy', 'one', 'go', 'eight', 'tree', 'forward', 'three', 'six', 'five', 'visual', 'zero', 'stop', 'house', 'left', 'up', 'learn', 'right', 'sheila', 'four', 'down', 'nine', 'follow', 'on', 'yes', 'dog', 'cat', 'no', 'bed', 'two'}\n",
      "\n",
      "Extracción de features completada.\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = os.path.join(ROOT_DIR, 'petes')\n",
    "# --- Procesar para train y val ---\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    list_path = os.path.join(ROOT_DIR, f\"{split}_list.txt\")\n",
    "        \n",
    "    for hl in [320, 160, 54, 32, 16]:\n",
    "        mode = torchaudio.transforms.MFCC(\n",
    "            sample_rate=16000,\n",
    "            n_mfcc=13,\n",
    "            log_mels = True,\n",
    "            melkwargs={\"n_fft\": 320, \"hop_length\": hl, \"n_mels\": 23}\n",
    "        )\n",
    "        save_path = os.path.join(SAVE_DIR, f\"{split}_{hl}_mfcc.pt\")\n",
    "        if os.path.isfile(save_path):\n",
    "            print(f\"{save_path} ya existe, saltando...\")\n",
    "            continue\n",
    "        print(f\"\\nExtrayendo {mode} para {split}...\")\n",
    "\n",
    "        dataset = CustomSpeechCommands(ROOT_DIR, list_path, mode='mfcc')\n",
    "        if mode == \"wav2vec2\":\n",
    "            dataset.save_features(\n",
    "                feature_extractor=extractor,\n",
    "                processor=processor,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "        else:\n",
    "            dataset.save_features(\n",
    "                feature_extractor=mode,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "\n",
    "print(\"\\nExtracción de features completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab517a3b",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6895b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary testing\n",
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_trains = 5\n",
    "epochs = 20\n",
    "\n",
    "# for arch in ['GRU', 'LSTM', 'RNN']:\n",
    "#     print(f'Entrenando Modelo {arch}')\n",
    "#     times_of_training = []\n",
    "#     models = []\n",
    "#     curves = []\n",
    "#     for k in range(n_trains):\n",
    "#         print(f'Entrenando modelo {k}/{n_trains}')\n",
    "#         model = RNNModel(rnn_type = arch, n_input_channels=13) # puede ser que sea util estudiar el hidden size, o sea reducirlo hasta que comience a afectar el rendimiento del modelo en val\n",
    "#         all_curves, times = train_model(model, train_dataset, val_dataset, epochs, criterion, batch_size, lr, n_evaluations_per_epoch=3, use_gpu=True)\n",
    "#         curves.append(all_curves)\n",
    "#         times_of_training.append(times)\n",
    "#         models.append(model)\n",
    "#     show_curves(curves, arch)\n",
    "#     get_metrics_and_confusion_matrix(models, test_dataset, arch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2af0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
