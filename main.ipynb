{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69090f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# Metrics and visualization\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Models and feature extractor\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "# Others\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20f275",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c354a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpeechCommands(Dataset):\n",
    "    def __init__(self, root, files_list, download=True, target_len=16000, mode=\"mfcc\", cnn_model=None):\n",
    "        \"\"\"\n",
    "        mode: 'mfcc', 'mfcc_delta', 'mfcc_delta_delta', 'cnn', 'wav2vec2'\n",
    "        cnn_model: modelo CNN preentrenado o personalizado para extracción\n",
    "        \"\"\"\n",
    "        self.target_len = target_len\n",
    "        self.mode = mode\n",
    "        self.cnn_model = cnn_model\n",
    "        self.dataset = torchaudio.datasets.SPEECHCOMMANDS(root=root, download=download)\n",
    "        self.indices = None\n",
    "        self.splitter(files_list, root)\n",
    "\n",
    "    def splitter(self, files_list, root):\n",
    "        with open(files_list, 'r') as f:\n",
    "            self.file_paths = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        self.all_paths = []\n",
    "        for item in tqdm(self.dataset._walker, desc=f\"Splitting {files_list}\"):\n",
    "            relative_path = os.path.relpath(\n",
    "                item,\n",
    "                start=os.path.join(root, \"SpeechCommands\", \"speech_commands_v0.02\")\n",
    "            ).replace(\"\\\\\", \"/\")\n",
    "            self.all_paths.append(relative_path)\n",
    "\n",
    "        self.indices = [i for i, path in enumerate(self.all_paths) if path in self.file_paths]\n",
    "        print(f\"Archivos encontrados: {len(self.indices)} / {len(self.file_paths)}\")\n",
    "\n",
    "    def pad_waveform(self, waveform):\n",
    "        length = waveform.shape[-1]\n",
    "        if length < self.target_len:\n",
    "            waveform = F.pad(waveform, (0, self.target_len - length))\n",
    "        elif length > self.target_len:\n",
    "            waveform = waveform[:, :self.target_len]\n",
    "        return waveform\n",
    "\n",
    "    def extract_feature_single(self, waveform, sample_rate, feature_extractor=None, processor=None, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Extrae features de UNA muestra según el modo configurado.\n",
    "        \"\"\"\n",
    "        waveform = self.pad_waveform(waveform).to(device)\n",
    "\n",
    "        if feature_extractor is not None:\n",
    "            feature_extractor = feature_extractor.to(device)\n",
    "\n",
    "        # --- MFCC ---\n",
    "        if self.mode == \"mfcc\":\n",
    "            feat = feature_extractor(waveform).squeeze(0).cpu().transpose(0, 1)\n",
    "\n",
    "        # --- MFCC + Delta ---\n",
    "        elif self.mode == \"mfcc_delta\":\n",
    "            base = feature_extractor(waveform)\n",
    "            delta = torchaudio.functional.compute_deltas(base)\n",
    "            feat = torch.cat([base, delta], dim=1).squeeze(0).cpu().transpose(0, 1)\n",
    "\n",
    "        # --- MFCC + Delta + Delta-Delta ---\n",
    "        elif self.mode == \"mfcc_delta_delta\":\n",
    "            base = feature_extractor(waveform)\n",
    "            delta = torchaudio.functional.compute_deltas(base)\n",
    "            delta2 = torchaudio.functional.compute_deltas(delta)\n",
    "            feat = torch.cat([base, delta, delta2], dim=1).squeeze(0).cpu().transpose(0, 1)\n",
    "\n",
    "        # --- CNN ---\n",
    "        elif self.mode == \"cnn\":\n",
    "            spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate).to(device)\n",
    "            spec = spec_transform(waveform).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                embedding = self.cnn_model(spec.to(device)).cpu().squeeze()\n",
    "            feat = embedding\n",
    "\n",
    "        # --- Wav2Vec2 ---\n",
    "        elif self.mode == \"wav2vec2\":\n",
    "            waveform = waveform.squeeze(0)\n",
    "            inputs = processor(\n",
    "                waveform,\n",
    "                sampling_rate=sample_rate,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = feature_extractor(**inputs)\n",
    "            feat = outputs.last_hidden_state.squeeze(0).cpu()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Modo de extracción '{self.mode}' no soportado.\")\n",
    "\n",
    "        return feat\n",
    "\n",
    "    def extract_features(self, feature_extractor=None, processor=None, device=\"cuda\"):\n",
    "        features, labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx in tqdm(self.indices, desc=f\"Extrayendo features ({self.mode})\"):\n",
    "                waveform, sample_rate, label, _, _ = self.dataset[idx]\n",
    "                feat = self.extract_feature_single(\n",
    "                    waveform, sample_rate, feature_extractor, processor, device\n",
    "                )\n",
    "                features.append(feat)\n",
    "                labels.append(label)\n",
    "\n",
    "        features = torch.stack(features)\n",
    "        print(f\"Features tensor: {features.shape}\")\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "    def save_features(self, feature_extractor=None, save_path=None, processor=None, device=\"cuda\"):\n",
    "        print(f\"Guardando features ({self.mode}) en {save_path}\")\n",
    "        try:\n",
    "            features, labels = self.extract_features(feature_extractor, processor, device)\n",
    "            torch.save({\"features\": features, \"labels\": labels}, save_path)\n",
    "            print(f\"Features guardadas correctamente en {save_path}\")\n",
    "            print(f\"Clases finales: {set(labels)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al guardar features en {save_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = self.indices[idx]\n",
    "        waveform, sample_rate, label, speaker_id, utterance_number = self.dataset[original_idx]\n",
    "        waveform = self.pad_waveform(waveform)\n",
    "        return waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "class FeaturesDataset(Dataset):\n",
    "    def __init__(self, features_path):\n",
    "        \"\"\"\n",
    "        Carga un archivo .pt con 'features' y 'labels' previamente guardados.\n",
    "\n",
    "        features_path: ruta al archivo .pt (por ejemplo 'data/train.pt')\n",
    "        \"\"\"\n",
    "        data = torch.load(features_path)\n",
    "        self.features = data[\"features\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(sorted(set(self.labels)))}\n",
    "        self.idx_to_label = {v: k for k, v in self.label_to_idx.items()}\n",
    "        self.numeric_labels = torch.tensor([self.label_to_idx[l] for l in self.labels])\n",
    "\n",
    "        print(f\"Dataset cargado desde {features_path}\")\n",
    "        print(f\" - {len(self.features)} ejemplos\")\n",
    "        print(f\" - {len(self.label_to_idx)} clases\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.numeric_labels[idx]\n",
    "        return feature, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e834f86",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type,\n",
    "        n_input_channels,\n",
    "        hidd_size=256,\n",
    "        out_features = 35,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Para utilizar una vanilla RNN entregue rnn_type=\"RNN\"\n",
    "        Para utilizar una LSTM entregue rnn_type=\"LSTM\"\n",
    "        Para utilizar una GRU entregue rnn_type=\"GRU\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "        if rnn_type == \"GRU\":\n",
    "            self.rnn_layer = nn.GRU(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers)\n",
    "\n",
    "        elif rnn_type == \"LSTM\":\n",
    "            self.rnn_layer = nn.LSTM(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers)\n",
    "\n",
    "        elif rnn_type == \"RNN\":\n",
    "            self.rnn_layer = nn.RNN(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers, bidirectional=True)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"rnn_type {rnn_type} not supported.\")\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidd_size, out_features),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.rnn_type == \"GRU\":\n",
    "            out, h = self.rnn_layer(x)\n",
    "\n",
    "        elif self.rnn_type == \"LSTM\":\n",
    "            out, (h, c) = self.rnn_layer(x)\n",
    "\n",
    "        elif self.rnn_type == \"RNN\":\n",
    "            out, h = self.rnn_layer(x)\n",
    "\n",
    "        out = h[-1]\n",
    "\n",
    "        return self.net(out)\n",
    "\n",
    "class TCNNModel(nn.Module):\n",
    "    def __init__(self, n_input_channels, hidd_size=64, out_features=35):\n",
    "        \"\"\"\n",
    "        Modelo T-CNN (Temporal Convolutional Neural Network)\n",
    "\n",
    "        Args:\n",
    "            n_input_channels (int): Canales de entrada (e.g., 13 para MFCC)\n",
    "            hidd_size (int): Número base de canales en las capas convolucionales\n",
    "            out_features (int): Número de clases de salida (e.g., 35)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Bloques Convolucionales ---\n",
    "        # nn.Conv1d espera la entrada como (Batch, Channels, SeqLen)\n",
    "        \n",
    "        # (B, 13, T) -> (B, 64, T/2)\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv1d(n_input_channels, hidd_size, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(hidd_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        # (B, 64, T/2) -> (B, 128, T/4)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(hidd_size, hidd_size * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidd_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        # (B, 128, T/4) -> (B, 256, T/4)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv1d(hidd_size * 2, hidd_size * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidd_size * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # --- Pooling Global y Clasificación ---\n",
    "        \n",
    "        # Colapsa la dimensión de secuencia (T/4) a 1\n",
    "        # (B, 256, T/4) -> (B, 256, 1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # (B, 256) -> (B, 35)\n",
    "        self.fc = nn.Linear(hidd_size * 4, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1) \n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Clasificar\n",
    "        return self.fc(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa el Positional Encoding para añadir información de posición.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Matriz de Positional Encoding\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [BatchSize, SeqLen, EmbeddingDim]\n",
    "        \"\"\"\n",
    "        # x.shape[1] es la longitud de la secuencia (SeqLen)\n",
    "        x = x + self.pe[:x.size(1)].transpose(0, 1) # Transpose para hacer Broadcasting [1, SeqLen, EmbDim]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input_features: int,  # e.g., 13 for MFCCs\n",
    "        n_output_classes: int = 35,\n",
    "        d_model: int = 128,      # Dimensión de la representación del Transformer\n",
    "        nhead: int = 8,          # Número de cabezas de atención\n",
    "        d_hid: int = 256,        # Dimensión de la capa FeedForward (FNN)\n",
    "        n_layers: int = 6,       # Número de bloques Codificadores\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 1. Proyección de entrada: de n_input_features a d_model\n",
    "        self.input_projection = nn.Linear(n_input_features, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # 3. Bloques Codificadores del Transformer\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=d_hid, \n",
    "            dropout=dropout,\n",
    "            batch_first=True # Importante para que el input sea [Batch, Seq, Feature]\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        # 4. Capa de Clasificación Final\n",
    "        # La estrategia es tomar la representación del PRIMER token (similar al [CLS] de BERT,\n",
    "        # pero aquí usamos el primer frame de audio como vector de secuencia).\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, n_output_classes)\n",
    "        )\n",
    "        \n",
    "        # Inicialización de pesos (buena práctica para Transformers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_projection.bias.data.zero_()\n",
    "        self.input_projection.weight.data.uniform_(-initrange, initrange)\n",
    "        self.classifier[0].bias.data.zero_()\n",
    "        self.classifier[0].weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [BatchSize, SeqLen, n_input_features]\n",
    "        \"\"\"\n",
    "        # 1. Proyección de la entrada\n",
    "        x = self.input_projection(x) * np.sqrt(self.d_model) # Factor de escalamiento\n",
    "        \n",
    "        # 2. Agregar Positional Encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # 3. Pasar por los Codificadores del Transformer\n",
    "        # torch.Size([BatchSize, SeqLen, d_model])\n",
    "        output = self.transformer_encoder(x) \n",
    "        \n",
    "        # 4. Clasificación: Tomar la salida del primer frame (SeqLen=0) como \n",
    "        # la representación agregada de toda la secuencia.\n",
    "        final_representation = output[:, 0, :] # [BatchSize, d_model]\n",
    "        \n",
    "        # 5. Capa Lineal Final\n",
    "        return self.classifier(final_representation)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidd_size=256,\n",
    "        in_channels = 13,\n",
    "        out_channels = 64,\n",
    "        N_conv_blocks = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        rnn_in = 0\n",
    "        if N_conv_blocks == 1:\n",
    "            self.conv_blocks = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2)     \n",
    "            )\n",
    "            rnn_in = out_channels\n",
    "        elif N_conv_blocks == 2:\n",
    "            self.conv_blocks = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2)\n",
    "            )\n",
    "            rnn_in = out_channels\n",
    "        elif N_conv_blocks == 3:\n",
    "            self.conv_blocks = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size = 3, padding = 'same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2)\n",
    "            )\n",
    "            rnn_in = out_channels\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Choose valid number (1-3)')\n",
    "\n",
    "        self.rnn_layer = RNNModel(\n",
    "            n_input_channels=rnn_in,\n",
    "            rnn_type=\"RNN\",\n",
    "            hidd_size=hidd_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        perm_x = torch.permute(x, (0, 2, 1))\n",
    "        conv_out = self.conv_blocks(perm_x)\n",
    "        deperm_x = torch.permute(conv_out, (0, 2, 1))\n",
    "        return self.rnn_layer(deperm_x)\n",
    "\n",
    "class MejorCNN1DModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidd_size=256,\n",
    "        in_channels=13,   # Tus 13 MFCCs\n",
    "        num_classes=35    # Clases de SpeechCommands\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Bloques CNN ---\n",
    "        # Aumentamos canales, usamos BatchNorm, y kernels más grandes\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Bloque 1\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=64, \n",
    "                kernel_size=7,  \n",
    "                padding='same'\n",
    "            ),\n",
    "            nn.BatchNorm1d(64), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),    \n",
    "            nn.Dropout(0.2),    \n",
    "\n",
    "            # Bloque 2\n",
    "            nn.Conv1d(\n",
    "                in_channels=64, \n",
    "                out_channels=128,\n",
    "                kernel_size=5, \n",
    "                padding='same'\n",
    "            ),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),    \n",
    "            nn.Dropout(0.2),    \n",
    "\n",
    "            # Bloque 3\n",
    "            nn.Conv1d(\n",
    "                in_channels=128, \n",
    "                out_channels=256,\n",
    "                kernel_size=3, \n",
    "                padding='same'\n",
    "            ),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)     \n",
    "            # La secuencia de salida será L // 8\n",
    "        )\n",
    "        \n",
    "        # --- Capa RNN ---\n",
    "        # El input para la RNN ahora tiene 256 canales\n",
    "        # (El tamaño del feature de la CNN)\n",
    "        rnn_in_features = 256 \n",
    "        \n",
    "        self.rnn_layer = RNNModel(\n",
    "            n_input_channels=rnn_in_features,\n",
    "            rnn_type=\"GRU\",       # <-- RECOMENDADO: Usa GRU o LSTM, no \"RNN\"\n",
    "            hidd_size=hidd_size,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        perm_x = x.permute(0, 2, 1)\n",
    "        conv_out = self.conv_blocks(perm_x)\n",
    "        deperm_x = conv_out.permute(0, 2, 1)\n",
    "        \n",
    "        return self.rnn_layer(deperm_x)\n",
    "\n",
    "def save_model(model, path, config=None):\n",
    "    \"\"\"\n",
    "    Guarda un modelo PyTorch de forma genérica.\n",
    "\n",
    "    Parameters:\n",
    "    - model: instancia de cualquier nn.Module\n",
    "    - path: ruta donde guardar\n",
    "    - config: diccionario con los parámetros necesarios para reconstruir el modelo\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"config\": config\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "def load_trained_model(model_class, checkpoint_path, device=\"cpu\", **override_kwargs):\n",
    "    \"\"\"\n",
    "    Carga un modelo guardado con save_model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_class: la clase del modelo (RNNModel, TCNN, TransformerClassifier, etc.)\n",
    "    - checkpoint_path: ruta al archivo .pt\n",
    "    - device: \"cpu\" o \"cuda\"\n",
    "    - override_kwargs: si quieres reemplazar parámetros del config guardado.\n",
    "\n",
    "    Returns:\n",
    "    - instancia reconstruida del modelo listo para usar\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    config = checkpoint[\"config\"] or {}\n",
    "\n",
    "    config.update(override_kwargs)\n",
    "    model = model_class(**config)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Modelo cargado desde {checkpoint_path}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8e3a4",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu):\n",
    "    # Predicción\n",
    "    y_predicted = model(x_batch)\n",
    "\n",
    "    # Cálculo de loss\n",
    "    loss = criterion(y_predicted, y_batch)\n",
    "\n",
    "    # Actualización de parámetros\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return y_predicted, loss\n",
    "\n",
    "\n",
    "def evaluate(val_loader, model, criterion, use_gpu):\n",
    "    cumulative_loss = 0\n",
    "    cumulative_predictions = 0\n",
    "    data_count = 0\n",
    "\n",
    "    for x_val, y_val in val_loader:\n",
    "        if use_gpu:\n",
    "            x_val = x_val.cuda()\n",
    "            y_val = y_val.cuda()\n",
    "\n",
    "        y_predicted = model(x_val)\n",
    "\n",
    "        loss = criterion(y_predicted, y_val)\n",
    "\n",
    "        class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "\n",
    "        cumulative_predictions += (y_val == class_prediction).sum().item()\n",
    "        cumulative_loss += loss.item() * y_val.shape[0]\n",
    "        data_count += y_val.shape[0]\n",
    "\n",
    "    val_acc = cumulative_predictions / data_count\n",
    "    val_loss = cumulative_loss / data_count\n",
    "\n",
    "    return val_acc, val_loss\n",
    "\n",
    "def measure_inference_time(model, dataloader, n_iters=30):\n",
    "    model.eval()\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, _) in enumerate(dataloader):\n",
    "            if i >= n_iters: break\n",
    "            X = X.cuda()\n",
    "            _ = model(X)\n",
    "    return (time.perf_counter() - t0) / n_iters\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    n_evaluations_per_epoch=6,\n",
    "    use_gpu=False,\n",
    "    patience=5,                 \n",
    "    restore_best_weights=True,\n",
    "    save_model = True\n",
    "):\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=use_gpu\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=use_gpu\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    curves = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    iteration = 0\n",
    "    n_batches = len(train_loader)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "    best_weights = None\n",
    "\n",
    "    print(n_batches)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\rEpoch {epoch + 1}/{epochs}\")\n",
    "        cumulative_train_loss = 0\n",
    "        cumulative_train_corrects = 0\n",
    "        examples_count = 0\n",
    "\n",
    "        model.train()\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            if use_gpu:\n",
    "                x_batch = x_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "\n",
    "            y_predicted, loss = train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu)\n",
    "\n",
    "            cumulative_train_loss += loss.item() * x_batch.shape[0]\n",
    "            examples_count += y_batch.shape[0]\n",
    "\n",
    "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "            cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
    "\n",
    "            if (i % (n_batches // n_evaluations_per_epoch) == 0) and (i > 0):\n",
    "                train_loss = cumulative_train_loss / examples_count\n",
    "                train_acc = cumulative_train_corrects / examples_count\n",
    "                print(f\"Iteration {iteration} - Batch {i}/{len(train_loader)} - Train loss: {train_loss}, Train acc: {train_acc}\")\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_acc, val_loss = evaluate(val_loader, model, criterion, use_gpu)\n",
    "\n",
    "        print(f\"Val loss: {val_loss}, Val acc: {val_acc}\")\n",
    "\n",
    "        train_loss = cumulative_train_loss / examples_count\n",
    "        train_acc = cumulative_train_corrects / examples_count\n",
    "\n",
    "        curves[\"train_acc\"].append(train_acc)\n",
    "        curves[\"val_acc\"].append(val_acc)\n",
    "        curves[\"train_loss\"].append(train_loss)\n",
    "        curves[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "\n",
    "            if restore_best_weights:\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"Sin mejora. Paciencia: {epochs_without_improvement}/{patience}\")\n",
    "\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"Early stopping activado!\")\n",
    "                break\n",
    "\n",
    "    # Restaurar mejores pesos\n",
    "    if restore_best_weights and best_weights is not None:\n",
    "        print(\"Restaurando mejores pesos del modelo…\")\n",
    "        model.load_state_dict(best_weights)\n",
    "\n",
    "    total_time = time.perf_counter() - t0\n",
    "    print(f\"Tiempo total de entrenamiento: {total_time:.4f} [s]\")\n",
    "\n",
    "    if save_model:\n",
    "        os.makedirs(\"saved_models\", exist_ok=True)\n",
    "        if save_path is None:\n",
    "            save_path = f\"saved_models/{model.rnn_type}_epochs{epochs}.pt\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Modelo guardado en: {save_path}\")\n",
    "    model.cpu()\n",
    "    return curves, total_time\n",
    "\n",
    "def show_curves(all_curves, suptitle=''):\n",
    "    final_curve_means = {k: np.mean([c[k] for c in all_curves], axis=0) for k in all_curves[0].keys()}\n",
    "    final_curve_stds = {k: np.std([c[k] for c in all_curves], axis=0) for k in all_curves[0].keys()}\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    fig.set_facecolor('white')\n",
    "\n",
    "    epochs = np.arange(len(final_curve_means[\"val_loss\"])) + 1\n",
    "\n",
    "    # ==== Plot de pérdidas ====\n",
    "    ax[0].plot(epochs, final_curve_means['val_loss'], label='validation')\n",
    "    ax[0].plot(epochs, final_curve_means['train_loss'], label='training')\n",
    "    ax[0].fill_between(epochs, \n",
    "                       y1=final_curve_means[\"val_loss\"] - final_curve_stds[\"val_loss\"], \n",
    "                       y2=final_curve_means[\"val_loss\"] + final_curve_stds[\"val_loss\"], alpha=.5)\n",
    "    ax[0].fill_between(epochs, \n",
    "                       y1=final_curve_means[\"train_loss\"] - final_curve_stds[\"train_loss\"], \n",
    "                       y2=final_curve_means[\"train_loss\"] + final_curve_stds[\"train_loss\"], alpha=.5)\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Loss evolution during training')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # ==== Plot de precisión ====\n",
    "    ax[1].plot(epochs, final_curve_means['val_acc'], label='validation')\n",
    "    ax[1].plot(epochs, final_curve_means['train_acc'], label='training')\n",
    "    ax[1].fill_between(epochs, \n",
    "                       y1=final_curve_means[\"val_acc\"] - final_curve_stds[\"val_acc\"], \n",
    "                       y2=final_curve_means[\"val_acc\"] + final_curve_stds[\"val_acc\"], alpha=.5)\n",
    "    ax[1].fill_between(epochs, \n",
    "                       y1=final_curve_means[\"train_acc\"] - final_curve_stds[\"train_acc\"], \n",
    "                       y2=final_curve_means[\"train_acc\"] + final_curve_stds[\"train_acc\"], alpha=.5)\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_title('Accuracy evolution during training')\n",
    "    ax[1].legend()\n",
    "\n",
    "    fig.suptitle(suptitle, fontsize=16, weight=\"bold\")\n",
    "\n",
    "    # ==== Guardar y cerrar ====\n",
    "    filepath = os.path.join('img', f'{suptitle}.pdf')\n",
    "    plt.savefig(filepath, bbox_inches='tight', format='pdf')\n",
    "    plt.close(fig)  \n",
    "\n",
    "def get_metrics_and_confusion_matrix(models, dataset, name=''):\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=min(32, len(dataset)))\n",
    "\n",
    "    y_true = torch.cat([y for _, y in dataloader])\n",
    "    total_classes = len(torch.unique(y_true))\n",
    "\n",
    "    if hasattr(dataset, \"idx_to_label\"):\n",
    "        class_names = [dataset.idx_to_label[i] for i in range(total_classes)]\n",
    "    elif hasattr(dataset, \"labels\"):\n",
    "        class_names = dataset.labels\n",
    "    else:\n",
    "        class_names = [str(i) for i in range(total_classes)]\n",
    "\n",
    "    counts = torch.bincount(y_true, minlength=total_classes)\n",
    "    top10 = torch.argsort(counts, descending=True)[:10].tolist()\n",
    "\n",
    "    # Mapear: top-10 → se quedan igual, otras → class_id = 10 (\"others\")\n",
    "    top10_set = set(top10)\n",
    "\n",
    "    y_true_A = y_true.clone()\n",
    "    for cls in range(total_classes):\n",
    "        if cls not in top10_set:\n",
    "            y_true_A[y_true_A == cls] = 10  # others = class 10\n",
    "\n",
    "    labels_A = top10 + [\"others\"]\n",
    "    nA = 11\n",
    "\n",
    "    mask_B = torch.tensor([c not in top10_set for c in y_true], dtype=torch.bool)\n",
    "    y_true_B = y_true[mask_B]\n",
    "    labels_B = sorted(torch.unique(y_true_B).tolist())\n",
    "\n",
    "    def map_groupA(pred):\n",
    "        predA = pred.clone()\n",
    "        for cls in range(total_classes):\n",
    "            if cls not in top10_set:\n",
    "                predA[predA == cls] = 10  # others\n",
    "        return predA\n",
    "\n",
    "    def compute_group(models, dataloader, y_true_group, labels, mask=None, map_func=None):\n",
    "        cms, accs = [], []\n",
    "\n",
    "        for model in models:\n",
    "            model.cpu()\n",
    "            model.eval()\n",
    "\n",
    "            preds = []\n",
    "            for x, _ in dataloader:\n",
    "                p = model(x).argmax(dim=1)\n",
    "                if map_func:\n",
    "                    p = map_func(p)\n",
    "                preds.append(p)\n",
    "\n",
    "            preds = torch.cat(preds)\n",
    "\n",
    "            if mask is not None:\n",
    "                preds = preds[mask]\n",
    "\n",
    "            cm = confusion_matrix(\n",
    "                y_true_group,\n",
    "                preds,\n",
    "                labels=list(range(len(labels))),\n",
    "                normalize=\"true\",\n",
    "            )\n",
    "            cms.append(cm)\n",
    "            accs.append(accuracy_score(y_true_group, preds))\n",
    "\n",
    "        return (\n",
    "            np.mean(cms, axis=0),\n",
    "            np.std(cms, axis=0),\n",
    "            np.mean(accs) * 100,\n",
    "            np.std(accs) * 100,\n",
    "        )\n",
    "\n",
    "    label_names_A = [class_names[cls] for cls in top10] + [\"others\"]\n",
    "\n",
    "    cmA_mean, cmA_std, accA_mean, accA_std = compute_group(\n",
    "        models, dataloader, y_true_A, labels_A, map_func=map_groupA\n",
    "    )\n",
    "\n",
    "    label_names_B = [class_names[c] for c in labels_B]\n",
    "\n",
    "    cmB_mean, cmB_std, accB_mean, accB_std = compute_group(\n",
    "        models, dataloader, y_true_B, labels_B, mask=mask_B\n",
    "    )\n",
    "\n",
    "    os.makedirs(\"img\", exist_ok=True)\n",
    "\n",
    "    def plot_cm(mean, std, classes, title, filename, fontsize, rotation = 45):\n",
    "        fig, ax = plt.subplots(figsize=(9, 8))\n",
    "        im = ax.imshow(mean, cmap=plt.cm.Blues)\n",
    "\n",
    "        ax.set_xticks(np.arange(len(classes)))\n",
    "        ax.set_yticks(np.arange(len(classes)))\n",
    "        ax.set_xticklabels(classes, rotation=90, ha=\"center\")\n",
    "        ax.set_yticklabels(classes)\n",
    "\n",
    "        # Texto en cada celda\n",
    "        for i in range(len(classes)):\n",
    "            for j in range(len(classes)):\n",
    "                # Color del fondo de esta celda según el colormap\n",
    "                rgba = im.cmap(im.norm(mean[i, j]))\n",
    "                r, g, b, _ = rgba\n",
    "                \n",
    "                # Luminancia percibida\n",
    "                luminance = 0.299*r + 0.587*g + 0.114*b\n",
    "                \n",
    "                # Elegir color del texto: blanco si el fondo es oscuro\n",
    "                text_color = \"white\" if luminance < 0.5 else \"black\"\n",
    "\n",
    "                ax.text(\n",
    "                    j, i,\n",
    "                    f\"{std[i,j]:.2f}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=fontsize,\n",
    "                    rotation=rotation,\n",
    "                    rotation_mode=\"anchor\",\n",
    "                    color=text_color\n",
    "            )\n",
    "        ax.set_title(title)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    plot_cm(\n",
    "        cmA_mean, cmA_std,\n",
    "        label_names_A,\n",
    "        f\"Top-10 + others\\nacc={accA_mean:.2f} ± {accA_std:.2f}%\",\n",
    "        f\"img/conf_mat_groupA_{name}.pdf\",\n",
    "        8,\n",
    "        0\n",
    "    )\n",
    "\n",
    "    print(f\"[OK] Saved: img/conf_mat_groupA_{name}.pdf\")\n",
    "\n",
    "    plot_cm(\n",
    "        cmB_mean, cmB_std,\n",
    "        label_names_B,\n",
    "        f\"Remaining classes\\nacc={accB_mean:.2f} ± {accB_std:.2f}%\",\n",
    "        f\"img/conf_mat_groupB_{name}.pdf\",\n",
    "        5\n",
    "    )\n",
    "\n",
    "    print(f\"[OK] Saved: img/conf_mat_groupB_{name}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c360d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b30a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(wf, sample_rate, label=\"\", figname=None):\n",
    "    \"\"\"\n",
    "    Muestra el waveform (izquierda) y los MFCCs (derecha) de una señal de audio.\n",
    "\n",
    "    Parámetros:\n",
    "        wf (Tensor): señal de audio [1, N] o [N]\n",
    "        sample_rate (int): frecuencia de muestreo (Hz)\n",
    "        label (str): etiqueta opcional para el título\n",
    "        figname (str): ruta para guardar la figura (si es None, solo muestra)\n",
    "    \"\"\"\n",
    "    if isinstance(wf, torch.Tensor):\n",
    "        wf = wf.squeeze().cpu()\n",
    "\n",
    "    # === Transformación MFCC ===\n",
    "    mfcc_transform = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=13,\n",
    "        melkwargs={\"n_fft\": 320, \"hop_length\": 160, \"n_mels\": 23},\n",
    "        log_mels=True\n",
    "    )\n",
    "    mfcc = mfcc_transform(wf.unsqueeze(0)).squeeze().cpu().numpy()  # [n_mfcc, time]\n",
    "\n",
    "    # === Crear figura con 2 subplots ===\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # --- Waveform ---\n",
    "    time = torch.arange(0, len(wf)) / sample_rate\n",
    "    axes[0].plot(time, wf.numpy(), color=\"steelblue\", linewidth=1.0)\n",
    "    axes[0].set_title(\"Waveform\", fontsize=12)\n",
    "    axes[0].set_xlabel(\"Tiempo [s]\")\n",
    "    axes[0].set_ylabel(\"Amplitud\")\n",
    "\n",
    "    # --- MFCC ---\n",
    "    sns.heatmap(mfcc, ax=axes[1], cmap=\"viridis\", cbar=True)\n",
    "    axes[1].set_title(\"MFCCs\", fontsize=12)\n",
    "    axes[1].set_xlabel(\"Tiempo (frames)\")\n",
    "    axes[1].set_ylabel(\"Coeficiente MFCC\")\n",
    "\n",
    "    fig.suptitle(f\"Audio: {label}\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === Guardar o mostrar ===\n",
    "    if figname:\n",
    "        name = os.path.join('img', f'{figname}.pdf')\n",
    "        plt.savefig(name, bbox_inches=\"tight\")\n",
    "        print(f\"Figura guardada en {name}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89760d",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47cce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipeipe/Documents/Proyecto-EL4106/proyecto/lib/python3.12/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/features/train_mfcc.pt ya existe, saltando...\n",
      "data/features/train_mfcc_delta.pt ya existe, saltando...\n",
      "data/features/train_mfcc_delta_delta.pt ya existe, saltando...\n",
      "data/features/train_wav2vec2.pt ya existe, saltando...\n",
      "data/features/val_mfcc.pt ya existe, saltando...\n",
      "data/features/val_mfcc_delta.pt ya existe, saltando...\n",
      "data/features/val_mfcc_delta_delta.pt ya existe, saltando...\n",
      "data/features/val_wav2vec2.pt ya existe, saltando...\n",
      "data/features/test_mfcc.pt ya existe, saltando...\n",
      "data/features/test_mfcc_delta.pt ya existe, saltando...\n",
      "data/features/test_mfcc_delta_delta.pt ya existe, saltando...\n",
      "data/features/test_wav2vec2.pt ya existe, saltando...\n",
      "\n",
      "Extracción de features completada.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuración base ---\n",
    "ROOT_DIR = \"data\"\n",
    "SAVE_DIR = os.path.join(ROOT_DIR, \"features\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Parámetros comunes ---\n",
    "mfcc = torchaudio.transforms.MFCC(\n",
    "    sample_rate=16000,\n",
    "    n_mfcc=13,\n",
    "    melkwargs={\"n_fft\": 320, \"hop_length\": 160, \"n_mels\": 23}\n",
    ")\n",
    "\n",
    "# --- Inicializar Wav2Vec2 una sola vez ---\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
    "\n",
    "# --- Configuración de modos y extractores ---\n",
    "modes = {\n",
    "    \"mfcc\": mfcc,\n",
    "    \"mfcc_delta\": mfcc,\n",
    "    \"mfcc_delta_delta\": mfcc,\n",
    "    \"wav2vec2\": wav2vec2,\n",
    "}\n",
    "\n",
    "# --- Procesar para train y val ---\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    list_path = os.path.join(ROOT_DIR, f\"{split}_list.txt\")\n",
    "\n",
    "    for mode, extractor in modes.items():\n",
    "        save_path = os.path.join(SAVE_DIR, f\"{split}_{mode}.pt\")\n",
    "\n",
    "        if os.path.isfile(save_path):\n",
    "            print(f\"{save_path} ya existe, saltando...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nExtrayendo {mode} para {split}...\")\n",
    "\n",
    "        dataset = CustomSpeechCommands(ROOT_DIR, list_path, mode=mode)\n",
    "        if mode == \"wav2vec2\":\n",
    "            dataset.save_features(\n",
    "                feature_extractor=extractor,\n",
    "                processor=processor,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "        else:\n",
    "            dataset.save_features(\n",
    "                feature_extractor=extractor,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "\n",
    "print(\"\\nExtracción de features completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac49807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/petes/train_320_mfcc.pt ya existe, saltando...\n",
      "data/petes/train_160_mfcc.pt ya existe, saltando...\n",
      "data/petes/train_54_mfcc.pt ya existe, saltando...\n",
      "data/petes/train_32_mfcc.pt ya existe, saltando...\n",
      "data/petes/train_16_mfcc.pt ya existe, saltando...\n",
      "data/petes/val_320_mfcc.pt ya existe, saltando...\n",
      "data/petes/val_160_mfcc.pt ya existe, saltando...\n",
      "data/petes/val_54_mfcc.pt ya existe, saltando...\n",
      "data/petes/val_32_mfcc.pt ya existe, saltando...\n",
      "data/petes/val_16_mfcc.pt ya existe, saltando...\n",
      "data/petes/test_320_mfcc.pt ya existe, saltando...\n",
      "data/petes/test_160_mfcc.pt ya existe, saltando...\n",
      "data/petes/test_54_mfcc.pt ya existe, saltando...\n",
      "data/petes/test_32_mfcc.pt ya existe, saltando...\n",
      "data/petes/test_16_mfcc.pt ya existe, saltando...\n",
      "\n",
      "Extracción de features completada.\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = os.path.join(ROOT_DIR, 'petes')\n",
    "# --- Procesar para train y val ---\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    list_path = os.path.join(ROOT_DIR, f\"{split}_list.txt\")\n",
    "        \n",
    "    for hl in [320, 160, 54, 32, 16]:\n",
    "        mode = torchaudio.transforms.MFCC(\n",
    "            sample_rate=16000,\n",
    "            n_mfcc=13,\n",
    "            log_mels = True,\n",
    "            melkwargs={\"n_fft\": 320, \"hop_length\": hl, \"n_mels\": 23}\n",
    "        )\n",
    "        save_path = os.path.join(SAVE_DIR, f\"{split}_{hl}_mfcc.pt\")\n",
    "        if os.path.isfile(save_path):\n",
    "            print(f\"{save_path} ya existe, saltando...\")\n",
    "            continue\n",
    "        print(f\"\\nExtrayendo {mode} para {split}...\")\n",
    "\n",
    "        dataset = CustomSpeechCommands(ROOT_DIR, list_path, mode='mfcc')\n",
    "        if mode == \"wav2vec2\":\n",
    "            dataset.save_features(\n",
    "                feature_extractor=extractor,\n",
    "                processor=processor,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "        else:\n",
    "            dataset.save_features(\n",
    "                feature_extractor=mode,\n",
    "                device=device,\n",
    "                save_path=save_path,\n",
    "            )\n",
    "\n",
    "print(\"\\nExtracción de features completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab517a3b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce6895b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.join(\"data\",\"petes\")\n",
    "SAVE_DIR = ROOT_DIR\n",
    "device = \"cuda\"\n",
    "\n",
    "# lr = 5e-4\n",
    "# batch_size = 32\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# n_trains = 2\n",
    "\n",
    "# epochs = 3\n",
    "# use_gpu = True\n",
    "\n",
    "# pattern = re.compile(r\"train_(\\d+)_mfcc.pt\")\n",
    "# hop_lengths = sorted(\n",
    "#     [int(pattern.search(f).group(1)) for f in os.listdir(SAVE_DIR) if pattern.search(f)]\n",
    "# )\n",
    "\n",
    "# f1_scores = []\n",
    "# f1_stds = []\n",
    "# seq_lengths = []\n",
    "\n",
    "# for hop_length in hop_lengths:\n",
    "#     seq_len = 1 + 16000 // hop_length\n",
    "#     seq_lengths.append(seq_len)\n",
    "\n",
    "#     print(f\"\\n--- Hop length {hop_length} -> Secuencia {seq_len} frames ---\")\n",
    "\n",
    "#     train_dataset = FeaturesDataset(os.path.join(SAVE_DIR, f\"train_{hop_length}_mfcc.pt\"))\n",
    "#     val_dataset   = FeaturesDataset(os.path.join(SAVE_DIR, f\"val_{hop_length}_mfcc.pt\"))\n",
    "#     test_dataset  = FeaturesDataset(os.path.join(SAVE_DIR, f\"test_{hop_length}_mfcc.pt\"))\n",
    "\n",
    "#     models = []\n",
    "#     curves = []\n",
    "\n",
    "#     for k in range(n_trains):\n",
    "#         print(f\"Entrenamiento {k+1}/{n_trains}\")\n",
    "\n",
    "#         # Crear modelo T-CNN\n",
    "#         model = TCNNModel(n_input_channels=13, hidd_size=64, out_features=35)\n",
    "\n",
    "#         curve, _ = train_model(\n",
    "#             model,\n",
    "#             train_dataset,\n",
    "#             val_dataset,\n",
    "#             epochs,\n",
    "#             criterion,\n",
    "#             batch_size,\n",
    "#             lr,\n",
    "#             n_evaluations_per_epoch=3,\n",
    "#             use_gpu=use_gpu,\n",
    "#         )\n",
    "\n",
    "#         curves.append(curve)\n",
    "#         models.append(model)\n",
    "\n",
    "#     show_curves(curves, suptitle=f\"TCNN_seq{seq_len}\")\n",
    "\n",
    "#     test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     metrics_mean, metrics_std, _ = evaluate_models_metrics(models, test_loader, criterion, use_gpu=use_gpu)\n",
    "\n",
    "#     f1_scores.append(metrics_mean[\"f1\"])\n",
    "#     f1_stds.append(metrics_std[\"f1\"])\n",
    "\n",
    "#     print(f\"F1 TCNN_seq{seq_len}: {metrics_mean['f1']:.3f} ± {metrics_std['f1']:.3f}\")\n",
    "\n",
    "#     get_metrics_and_confusion_matrix(models, test_dataset, name=f\"TCNN_seq{seq_len}\")\n",
    "\n",
    "# # --- Visualización de F1 vs longitud de secuencia ---\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.errorbar(seq_lengths, f1_scores, yerr=f1_stds, label=\"TCNN\", marker=\"o\", capsize=4)\n",
    "# plt.xlabel(\"Cantidad de frames en la secuencia (MFCC)\")\n",
    "# plt.ylabel(\"F1-score promedio (± std)\")\n",
    "# plt.title(\"F1-score TCNN según longitud de secuencia\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"img/f1_vs_length_tcnn.pdf\", bbox_inches=\"tight\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado desde data/petes/train_16_mfcc.pt\n",
      " - 32453 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/val_16_mfcc.pt\n",
      " - 3875 ejemplos\n",
      " - 35 clases\n",
      "Dataset cargado desde data/petes/test_16_mfcc.pt\n",
      " - 4381 ejemplos\n",
      " - 35 clases\n",
      "Entrenando Modelo Transformer con d_model=128\n",
      "Entrenando modelo 1/2\n",
      "1015\n",
      "Epoch 1/5\n",
      "Iteration 338 - Batch 338/1015 - Train loss: 1.6941541232893953, Train acc: 0.49557522123893805\n",
      "Iteration 676 - Batch 676/1015 - Train loss: 1.331261065545075, Train acc: 0.6063977104874446\n",
      "Iteration 1014 - Batch 1014/1015 - Train loss: 1.1365347909247074, Train acc: 0.6643145471913229\n",
      "Val loss: 0.6124032637996059, Val acc: 0.8136774193548387\n",
      "Epoch 2/5\n",
      "Iteration 1353 - Batch 338/1015 - Train loss: 0.6150485655639024, Train acc: 0.8187684365781711\n",
      "Iteration 1691 - Batch 676/1015 - Train loss: 0.5875045518711364, Train acc: 0.8263478581979321\n",
      "Iteration 2029 - Batch 1014/1015 - Train loss: 0.5712779103587894, Train acc: 0.8308014667365112\n",
      "Val loss: 0.5322634831705401, Val acc: 0.8423225806451613\n",
      "Epoch 3/5\n",
      "Iteration 2368 - Batch 338/1015 - Train loss: 0.4772277848031317, Train acc: 0.859882005899705\n",
      "Iteration 2706 - Batch 676/1015 - Train loss: 0.46256048964168683, Train acc: 0.8609675036927622\n",
      "Iteration 3044 - Batch 1014/1015 - Train loss: 0.46057113726745236, Train acc: 0.8605675900533079\n",
      "Val loss: 0.46109009548925584, Val acc: 0.8652903225806452\n",
      "Epoch 4/5\n",
      "Iteration 3383 - Batch 338/1015 - Train loss: 0.4074509576138845, Train acc: 0.8730641592920354\n",
      "Iteration 3721 - Batch 676/1015 - Train loss: 0.39223202671222884, Train acc: 0.8810468980797637\n",
      "Iteration 4059 - Batch 1014/1015 - Train loss: 0.39030549503844264, Train acc: 0.8809046929405602\n",
      "Val loss: 0.4112190855010863, Val acc: 0.8748387096774194\n",
      "Epoch 5/5\n",
      "Iteration 4398 - Batch 338/1015 - Train loss: 0.3624566873002545, Train acc: 0.8891961651917404\n",
      "Iteration 4736 - Batch 676/1015 - Train loss: 0.35908967883847487, Train acc: 0.8900941654357459\n",
      "Iteration 5074 - Batch 1014/1015 - Train loss: 0.35102281420417397, Train acc: 0.8929220719193911\n",
      "Val loss: 0.3594813030746675, Val acc: 0.8934193548387097\n",
      "🔄 Restaurando mejores pesos del modelo…\n",
      "Tiempo total de entrenamiento: 2593.1343 [s]\n",
      "Entrenando modelo 2/2\n",
      "1015\n",
      "Epoch 1/5\n",
      "Iteration 338 - Batch 338/1015 - Train loss: 1.7338483001278564, Train acc: 0.4811946902654867\n",
      "Iteration 676 - Batch 676/1015 - Train loss: 1.3477437437196955, Train acc: 0.6040435745937962\n",
      "Iteration 1014 - Batch 1014/1015 - Train loss: 1.1486166505050583, Train acc: 0.6641604782300558\n",
      "Val loss: 0.7038802230588851, Val acc: 0.7958709677419354\n",
      "Epoch 2/5\n",
      "Iteration 1353 - Batch 338/1015 - Train loss: 0.6320618699170144, Train acc: 0.8136061946902655\n",
      "Iteration 1691 - Batch 676/1015 - Train loss: 0.598155367673764, Train acc: 0.8233474889217134\n",
      "Iteration 2029 - Batch 1014/1015 - Train loss: 0.5777459189257199, Train acc: 0.828243921979478\n",
      "Val loss: 0.4894766795558314, Val acc: 0.8534193548387097\n",
      "Epoch 3/5\n",
      "Iteration 2368 - Batch 338/1015 - Train loss: 0.4519197322362292, Train acc: 0.8611725663716814\n",
      "Iteration 2706 - Batch 676/1015 - Train loss: 0.448181075130643, Train acc: 0.8638755539143279\n"
     ]
    }
   ],
   "source": [
    "# Preliminary testing\n",
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_trains = 2 # Número de repeticiones para obtener media y std\n",
    "epochs = 5   # Aumenta las épocas, los Transformers suelen necesitar más\n",
    "\n",
    "\n",
    "train_dataset = FeaturesDataset(os.path.join(SAVE_DIR, f\"train_16_mfcc.pt\"))\n",
    "val_dataset = FeaturesDataset(os.path.join(SAVE_DIR, f\"val_16_mfcc.pt\"))\n",
    "test_dataset = FeaturesDataset(os.path.join(SAVE_DIR, f\"test_16_mfcc.pt\"))\n",
    "\n",
    "# Usa las dimensiones de tu dataset (MFCCs y número de clases)\n",
    "N_INPUT_FEATURES = train_dataset.features.shape[2]  # 13 MFCCs\n",
    "N_OUTPUT_CLASSES = len(train_dataset.label_to_idx)  # 35 clases\n",
    "\n",
    "# --- Configuración del Transformer ---\n",
    "TRANSFORMER_ARCH_PARAMS = {\n",
    "    \"n_input_features\": N_INPUT_FEATURES,\n",
    "    \"n_output_classes\": N_OUTPUT_CLASSES,\n",
    "    \"d_model\": 128,\n",
    "    \"nhead\": 8,\n",
    "    \"n_layers\": 4, # Puedes empezar con 4-6 capas\n",
    "    \"d_hid\": 512,  # Debe ser mayor que d_model, e.g., 4 * d_model\n",
    "}\n",
    "# -----------------------------------\n",
    "\n",
    "ARCH = 'Transformer'\n",
    "print(f'Entrenando Modelo {ARCH} con d_model={TRANSFORMER_ARCH_PARAMS[\"d_model\"]}')\n",
    "\n",
    "times_of_training = []\n",
    "models = []\n",
    "curves = []\n",
    "\n",
    "for k in range(n_trains):\n",
    "    print(f'Entrenando modelo {k+1}/{n_trains}')\n",
    "    \n",
    "    model = TransformerModel(**TRANSFORMER_ARCH_PARAMS) \n",
    "    \n",
    "    # Entrenar\n",
    "    all_curves, times = train_model(\n",
    "        model, \n",
    "        train_dataset, \n",
    "        val_dataset, \n",
    "        epochs, \n",
    "        criterion, \n",
    "        batch_size, \n",
    "        lr, \n",
    "        n_evaluations_per_epoch=3, \n",
    "        use_gpu=True\n",
    "    )\n",
    "    curves.append(all_curves)\n",
    "    times_of_training.append(times)\n",
    "    models.append(model)\n",
    "    \n",
    "show_curves(curves, ARCH)\n",
    "get_metrics_and_confusion_matrix(models, test_dataset, ARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2af0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"data\"\n",
    "SAVE_DIR = ROOT_DIR\n",
    "device = \"cuda\"\n",
    "\n",
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_trains = 5\n",
    "epochs = 20\n",
    "use_gpu = True\n",
    "\n",
    "neurons_on_hidd_layer = [256, 128, 64, 32, 16, 8]\n",
    "\n",
    "train_dataset = FeaturesDataset(os.path.join(SAVE_DIR, \"train.pt\"))\n",
    "val_dataset   = FeaturesDataset(os.path.join(SAVE_DIR, \"val.pt\"))\n",
    "test_dataset  = FeaturesDataset(os.path.join(SAVE_DIR, \"test.pt\"))\n",
    "\n",
    "# Diccionarios para guardar resultados\n",
    "f1_scores = {arch: [] for arch in [\"GRU\", \"LSTM\"]}\n",
    "f1_stds   = {arch: [] for arch in [\"GRU\", \"LSTM\"]}\n",
    "\n",
    "for arch in [\"GRU\", \"LSTM\", 'RNN']:\n",
    "    print(f\"\\n======= Entrenando modelos tipo {arch} =======\")\n",
    "\n",
    "    for hidd_size in neurons_on_hidd_layer:\n",
    "        print(f\"\\n--- Modelo con hidd_size = {hidd_size} ---\")\n",
    "\n",
    "        models = []\n",
    "        curves = []\n",
    "\n",
    "        for k in range(n_trains):\n",
    "            print(f\"Entrenamiento {k+1}/{n_trains}\")\n",
    "            model = RNNModel(rnn_type=arch, n_input_channels=13, hidd_size=hidd_size)\n",
    "\n",
    "            curve, _ = train_model(\n",
    "                model,\n",
    "                train_dataset,\n",
    "                val_dataset,\n",
    "                epochs,\n",
    "                criterion,\n",
    "                batch_size,\n",
    "                lr,\n",
    "                n_evaluations_per_epoch=3,\n",
    "                use_gpu=use_gpu,\n",
    "            )\n",
    "\n",
    "            curves.append(curve)\n",
    "            models.append(model)\n",
    "\n",
    "        # Mostrar curvas promedio\n",
    "        show_curves(curves, suptitle=f\"{arch}_h{hidd_size}\")\n",
    "\n",
    "        # Evaluar métricas\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        metrics_mean, metrics_std, _ = evaluate_models_metrics(models, test_loader, criterion, use_gpu=use_gpu)\n",
    "\n",
    "        f1_scores[arch].append(metrics_mean[\"f1\"])\n",
    "        f1_stds[arch].append(metrics_std[\"f1\"])\n",
    "\n",
    "        print(f\"F1 {arch}_h{hidd_size}: {metrics_mean['f1']:.3f} ± {metrics_std['f1']:.3f}\")\n",
    "\n",
    "        # Guardar matriz de confusión promedio\n",
    "        get_metrics_and_confusion_matrix(models, test_dataset, name=f\"{arch}_h{hidd_size}\")\n",
    "\n",
    "# --- Gráfico F1 vs número de neuronas ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "for arch in [\"GRU\", \"LSTM\"]:\n",
    "    plt.errorbar(neurons_on_hidd_layer, f1_scores[arch], yerr=f1_stds[arch], \n",
    "                 label=arch, marker=\"o\", capsize=4)\n",
    "\n",
    "plt.xlabel(\"Número de neuronas ocultas (hidd_size)\")\n",
    "plt.ylabel(\"F1-score promedio (± std)\")\n",
    "plt.title(\"F1-score según tamaño de capa oculta\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/f1_vs_hidd_size.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"data/petes\"\n",
    "SAVE_DIR = ROOT_DIR\n",
    "device = \"cuda\"\n",
    "\n",
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_trains = 5\n",
    "epochs = 20\n",
    "use_gpu = True\n",
    "\n",
    "pattern = re.compile(r\"train_(\\d+)_mfcc.pt\")\n",
    "hop_lengths = sorted(\n",
    "    [int(pattern.search(f).group(1)) for f in os.listdir(SAVE_DIR) if pattern.search(f)]\n",
    ")\n",
    "\n",
    "# Diccionarios para guardar resultados\n",
    "f1_scores = {\"GRU\": [], \"LSTM\": [], \"TCNN\": [], \"RNN\": []}\n",
    "f1_stds   = {\"GRU\": [], \"LSTM\": [], \"TCNN\": [], \"RNN\": []}\n",
    "seq_lengths = []\n",
    "\n",
    "for hop_length in hop_lengths:\n",
    "    seq_len = 1 + 16000 // hop_length\n",
    "    seq_lengths.append(seq_len)\n",
    "\n",
    "    print(f\"\\n======= Hop length {hop_length} -> Secuencia {seq_len} frames =======\")\n",
    "\n",
    "    # Cargar datasets\n",
    "    train_dataset = FeaturesDataset(os.path.join(SAVE_DIR, f\"train_{hop_length}_mfcc.pt\"))\n",
    "    val_dataset   = FeaturesDataset(os.path.join(SAVE_DIR, f\"val_{hop_length}_mfcc.pt\"))\n",
    "    test_dataset  = FeaturesDataset(os.path.join(SAVE_DIR, f\"test_{hop_length}_mfcc.pt\"))\n",
    "\n",
    "    # Entrenar cada tipo de modelo\n",
    "    for arch in [\"GRU\", \"LSTM\", \"TCNN\", \"RNN\"]:\n",
    "        print(f\"\\n--- Entrenando modelo tipo {arch} ---\")\n",
    "\n",
    "        models = []\n",
    "        curves = []\n",
    "\n",
    "        for k in range(n_trains):\n",
    "            print(f\"Entrenamiento {k+1}/{n_trains}\")\n",
    "\n",
    "            # Crear modelo según tipo\n",
    "            if arch in [\"GRU\", \"LSTM\", \"RNN\"]:\n",
    "                model = RNNModel(\n",
    "                    rnn_type=arch,\n",
    "                    n_input_channels=13,\n",
    "                    hidd_size=128,\n",
    "                )\n",
    "            elif arch == \"TCNN\":\n",
    "                model = TCNNModel(\n",
    "                    n_input_channels=13,\n",
    "                    hidd_size=128,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Modelo no reconocido\")\n",
    "\n",
    "            # Entrenamiento\n",
    "            curve, _ = train_model(\n",
    "                model,\n",
    "                train_dataset,\n",
    "                val_dataset,\n",
    "                epochs,\n",
    "                criterion,\n",
    "                batch_size,\n",
    "                lr,\n",
    "                n_evaluations_per_epoch=3,\n",
    "                use_gpu=use_gpu,\n",
    "            )\n",
    "            curves.append(curve)\n",
    "            models.append(model)\n",
    "\n",
    "        # Curvas de entrenamiento\n",
    "        show_curves(curves, suptitle=f\"{arch}_seq{seq_len}\")\n",
    "\n",
    "        # Evaluación\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        metrics_mean, metrics_std, _ = evaluate_models_metrics(models, test_loader, criterion, use_gpu=use_gpu)\n",
    "\n",
    "        f1_scores[arch].append(metrics_mean[\"f1\"])\n",
    "        f1_stds[arch].append(metrics_std[\"f1\"])\n",
    "\n",
    "        print(f\"F1 {arch}_seq{seq_len}: {metrics_mean['f1']:.3f} ± {metrics_std['f1']:.3f}\")\n",
    "\n",
    "        get_metrics_and_confusion_matrix(models, test_dataset, name=f\"{arch}_seq{seq_len}\")\n",
    "\n",
    "# --- Visualización F1 vs longitud de secuencia ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "for arch in [\"GRU\", \"LSTM\", \"TCNN\"]:\n",
    "    plt.errorbar(seq_lengths, f1_scores[arch], yerr=f1_stds[arch], label=arch, marker=\"o\", capsize=4)\n",
    "\n",
    "plt.xlabel(\"Cantidad de frames en la secuencia (MFCC)\")\n",
    "plt.ylabel(\"F1-score promedio (± std)\")\n",
    "plt.title(\"Comparación de F1-score según longitud de secuencia\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/f1_vs_length_all.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"data/petes\"\n",
    "SAVE_DIR = ROOT_DIR\n",
    "device = \"cuda\"\n",
    "\n",
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_trains = 5\n",
    "epochs = 20\n",
    "use_gpu = True\n",
    "\n",
    "hop_length = 32\n",
    "seq_len_input = 1 + 16000 // hop_length  # 501 frames de entrada\n",
    "print(f\"Largo de secuencia de entrada: {seq_len_input}\")\n",
    "\n",
    "# Cargar datasets generados con hop_length=32\n",
    "train_dataset = FeaturesDataset(os.path.join(SAVE_DIR, f\"train_{hop_length}_mfcc.pt\"))\n",
    "val_dataset   = FeaturesDataset(os.path.join(SAVE_DIR, f\"val_{hop_length}_mfcc.pt\"))\n",
    "test_dataset  = FeaturesDataset(os.path.join(SAVE_DIR, f\"test_{hop_length}_mfcc.pt\"))\n",
    "\n",
    "# Configuraciones a probar\n",
    "N_conv_blocks_list = range(4)\n",
    "\n",
    "# Resultados\n",
    "f1_scores = []\n",
    "f1_stds = []\n",
    "seq_lengths_seen = []\n",
    "\n",
    "for N_conv_blocks in N_conv_blocks_list:\n",
    "    # Largo de secuencia que verá la RNN\n",
    "    seq_len_rnn = seq_len_input // (2 ** N_conv_blocks)\n",
    "    seq_lengths_seen.append(seq_len_rnn)\n",
    "\n",
    "    print(f\"\\n=== Entrenando modelo con {N_conv_blocks} bloques conv \"\n",
    "          f\"(seq_len RNN ≈ {seq_len_rnn}) ===\")\n",
    "\n",
    "    models = []\n",
    "    curves = []\n",
    "\n",
    "    for k in range(n_trains):\n",
    "        print(f\"Entrenamiento {k+1}/{n_trains}\")\n",
    "\n",
    "        model = MejorCNN1DModel(\n",
    "            N_conv_blocks=N_conv_blocks,\n",
    "            hidd_size=128,        # puedes ajustar si quieres\n",
    "            in_channels=13,\n",
    "            out_channels=64\n",
    "        )\n",
    "\n",
    "        curve, _ = train_model(\n",
    "            model,\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            epochs,\n",
    "            criterion,\n",
    "            batch_size,\n",
    "            lr,\n",
    "            n_evaluations_per_epoch=3,\n",
    "            use_gpu=use_gpu,\n",
    "        )\n",
    "\n",
    "        curves.append(curve)\n",
    "        models.append(model)\n",
    "\n",
    "    show_curves(curves, suptitle=f\"CNN1D_{N_conv_blocks}blocks\")\n",
    "\n",
    "    # Evaluar métricas\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    metrics_mean, metrics_std, _ = evaluate_models_metrics(models, test_loader, criterion, use_gpu=use_gpu)\n",
    "\n",
    "    f1_scores.append(metrics_mean[\"f1\"])\n",
    "    f1_stds.append(metrics_std[\"f1\"])\n",
    "\n",
    "    print(f\"F1 CNN1D_{N_conv_blocks}blocks: {metrics_mean['f1']:.3f} ± {metrics_std['f1']:.3f}\")\n",
    "\n",
    "    get_metrics_and_confusion_matrix(models, test_dataset, name=f\"CNN1D_{N_conv_blocks}blocks\")\n",
    "\n",
    "# --- Gráfico ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.errorbar(seq_lengths_seen, f1_scores, yerr=f1_stds, marker=\"o\", capsize=4, label=\"CNN+RNN\")\n",
    "plt.xlabel(\"Largo de secuencia que entra a la RNN (frames)\")\n",
    "plt.ylabel(\"F1-score promedio (± std)\")\n",
    "plt.title(\"F1-score vs Largo de secuencia visto por la RNN\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/f1_vs_seq_len_rnn.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
